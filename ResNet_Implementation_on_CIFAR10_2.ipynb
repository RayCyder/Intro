{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RayCyder/Intro/blob/main/ResNet_Implementation_on_CIFAR10_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6ikW2JiYwp0"
      },
      "source": [
        "## model from paper\n",
        "The Resnet Research paper can be accessed from here https://arxiv.org/pdf/1512.03385v1.pdf\n",
        "\n",
        "**Defining the Network Architecture**\n",
        "In this section the entire Research Paper is implemented to define the Residual Network approach taken by the researchers\n",
        "\n",
        "NOTE:\n",
        "\n",
        "Output volume for a convolutional layer\n",
        "To compute the output size of a given convolutional layer we can perform the following calculation (taken from Stanford's cs231n course):\n",
        "\n",
        "We can compute the spatial size of the output volume as a function of the input volume size (W), the kernel/filter size (F), the stride with which they are applied (S), and the amount of zero padding used (P) on the border. The correct formula for calculating how many neurons define the output_W is given by (W−F+2P)/S+1.\n",
        "\n",
        "For example for a 7x7 input and a 3x3 filter with stride 1 and pad 0 we would get a 5x5 output. With stride 2 we would get a 3x3 output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pe9AqkGDIcm3"
      },
      "outputs": [],
      "source": [
        "'''ResNet in PyTorch.\n",
        "\n",
        "For Pre-activation ResNet, see 'preact_resnet.py'.\n",
        "\n",
        "Reference:\n",
        "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
        "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
        "\n",
        "\n",
        "def test():\n",
        "    net = ResNet18()\n",
        "    y = net(torch.randn(1, 3, 32, 32))\n",
        "    print(y.size())\n",
        "\n",
        "# test()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Random Seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEa-4QGLIcm3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "seed_seed =888 #42 #777\n",
        "random.seed(seed_seed)\n",
        "torch.manual_seed(seed_seed)\n",
        "torch.cuda.manual_seed_all(seed_seed)\n",
        "np.random.seed(seed_seed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mT2iu-rBIcm4",
        "outputId": "1d873526-da23-41a2-c9fe-33865f21991b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "import copy\n",
        "class TrainConfig(object):\n",
        "  epochs = 100\n",
        "\n",
        "# choose device\n",
        "device = (\n",
        "    \"cuda\" if torch.cuda.is_available()\n",
        "    else \"mps\" if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using device: {device}\")\n",
        "if device == \"cuda\":\n",
        "    import torch.backends.cudnn as cudnn\n",
        "    cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "# method for initialize model\n",
        "def initialize_model(device):\n",
        "    # resnet18 = models.resnet18(weights=None, num_classes=10)   # train from scratch\n",
        "    resnet18 = ResNet18()\n",
        "    # resnet18.load_state_dict(torch.load('baseline.pt'))\n",
        "    resnet18 = resnet18.to(device)\n",
        "    if device == \"cuda\" and torch.cuda.device_count() > 1:\n",
        "        resnet18 = torch.nn.DataParallel(resnet18)\n",
        "    return resnet18\n",
        "\n",
        "base_model = initialize_model(device)\n",
        "base_init_state = copy.deepcopy(base_model.state_dict())\n",
        "def get_base_model():\n",
        "    model = initialize_model(device)\n",
        "    model.load_state_dict(copy.deepcopy(base_init_state))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lf-2lnBKIcm4"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "# base class for optimizer with scheduler\n",
        "class BaseOptimizer(object):\n",
        "  def __init__(self,model, criterion, epochs,learning_rate):\n",
        "    self.epochs = epochs\n",
        "    self.criterion = criterion\n",
        "    self.model = model\n",
        "    self.optimizer,self.scheduler = None, None\n",
        "    self.learning_rate = learning_rate\n",
        "  def initialize_optimzer(self):\n",
        "    raise Exception(\"non implemetation\")\n",
        "  def zero_grad(self,inputs=None,labels=None):\n",
        "    self.optimizer.zero_grad()\n",
        "  def step(self):\n",
        "    self.optimizer.step()\n",
        "  def epoch_step(self):\n",
        "    if self.scheduler:\n",
        "      self.scheduler.step()\n",
        "  \n",
        "  def set_scheduler(self, optimizer, learning_rate, rate=0.1):\n",
        "     scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer,\n",
        "        T_max=self.epochs,   # period (usually = total epochs)\n",
        "        eta_min=rate*learning_rate      # minimum LR (here 0.1× of 0.01)\n",
        "    )\n",
        "     self.scheduler = scheduler\n",
        "     return scheduler\n",
        "\n",
        "class TrainModel(object):\n",
        "  def __init__(self,train_loader=None, val_loader=None, classes=None,epochs=100):\n",
        "    self.criterion = torch.nn.CrossEntropyLoss()\n",
        "    self.val_loader = val_loader\n",
        "    self.train_loader = train_loader\n",
        "    self.classes = classes\n",
        "    self.epochs = epochs\n",
        "  def initialize_model(self):\n",
        "    model = get_base_model()\n",
        "    self.model = model\n",
        "    return model\n",
        "  def train(self, optimizer, need_save=False):\n",
        "    epochs = self.epochs\n",
        "    model, criterion = self.model, self.criterion\n",
        "    train_loader, val_loader, classes = self.train_loader, self.val_loader, self.classes\n",
        "    #start train\n",
        "    losses = [] #train loss\n",
        "    val_losses = [] #val loss\n",
        "    accs =[] #train accuracy\n",
        "    elapsed_all = [] #time cost\n",
        "    val_accs = [] #val accurary\n",
        "    best = 0.0 #best accuraray\n",
        "    #optimizer = MuonMVR2Optimizer(model,criterion,epochs,learning_rate)\n",
        "    #optimizer.initialize_optimizer()\n",
        "    start = time.time()\n",
        "    for epoch in range(epochs):\n",
        "      model.train()\n",
        "      running_loss = 0.0\n",
        "      correct = 0\n",
        "      for i, (inputs, labels) in enumerate(train_loader):\n",
        "          inputs, labels = inputs.to(device), labels.to(device)\n",
        "          optimizer.zero_grad(inputs, labels)\n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          running_loss+=loss.item()\n",
        "          correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
        "      accuracy = 100. * correct / len(train_loader.dataset)\n",
        "      accs.append(accuracy)\n",
        "      # step the scheduler once per epoch\n",
        "      optimizer.epoch_step()\n",
        "      losses.append(running_loss/max(1, len(train_loader)))\n",
        "      if optimizer.scheduler:\n",
        "        current_lr = optimizer.scheduler.get_last_lr()[0]  # single param group\n",
        "      else:\n",
        "        current_lr = optimizer.learning_rate\n",
        "      #val_loss = val_once(model,criterion,val_loader)\n",
        "      val_loss, val_acc = test_model(model, criterion, val_loader, classes)\n",
        "      if val_acc > best:\n",
        "        if need_save:\n",
        "          state = {'model': model.state_dict(),\n",
        "                  'accuracy': val_acc,\n",
        "                  'epoch': epoch,}\n",
        "          if not os.path.isdir('checkpoints'):\n",
        "              os.mkdir('checkpoints')\n",
        "          def _get_name(opt):\n",
        "            return opt.__name__ if hasattr(opt, \"__name__\") else opt.__class__.__name__\n",
        "          torch.save(state, f'checkpoints/best_model_{_get_name(optimizer)}.pth')\n",
        "        best = val_acc\n",
        "\n",
        "      val_losses.append(val_loss)\n",
        "      val_accs.append(val_acc)\n",
        "      elapsed = time.time() - start\n",
        "      elapsed_all.append(elapsed)\n",
        "      print(f\"[{epoch+1}] time: {elapsed} loss: {losses[-1]:.3f} val_loss:{val_loss} current_lr={current_lr:.6f}\")\n",
        "\n",
        "    print(f\"Training finished!\")\n",
        "    return losses, accs, val_losses, val_accs, elapsed_all\n",
        "\n",
        "@torch.no_grad()\n",
        "def test_model(ResNet18, criterion, test_loader, classes):\n",
        "  test_loss = 0.0\n",
        "  class_correct = [0 for _ in range(10)]\n",
        "  class_total   = [0 for _ in range(10)]\n",
        "\n",
        "  ResNet18.eval()\n",
        "  # iterate over test data\n",
        "  for batch_idx, (data, target) in enumerate(test_loader):\n",
        "    # move tensors to GPU if CUDA is available\n",
        "    data, target = data.to(device), target.to(device)\n",
        "    count = data.size(0)\n",
        "    # forward pass: compute predicted outputs by passing inputs to the model\n",
        "    output = ResNet18(data)\n",
        "    # calculate the batch loss\n",
        "    loss = criterion(output, target)\n",
        "    # update test loss\n",
        "    test_loss += loss.item()\n",
        "    # convert output probabilities to predicted class\n",
        "    _, pred = torch.max(output, 1)\n",
        "    # compare predictions to true label\n",
        "    correct_tensor = pred.eq(target)\n",
        "    # correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    # calculate test accuracy for each object class\n",
        "    for i in range(count):\n",
        "      label = target.data[i]\n",
        "      class_correct[label] += int(correct_tensor[i].item())\n",
        "      class_total[label] += 1\n",
        "\n",
        "  # average test loss\n",
        "  total_samples = sum(class_total)\n",
        "  test_loss = test_loss/max(1,  len(test_loader))\n",
        "  print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "  for i in range(10):\n",
        "    if class_total[i] > 0:\n",
        "      acc_i = 100 * class_correct[i] / class_total[i]\n",
        "      print('Test Accuracy of %5s: %2d%% (%d/%d)' % (\n",
        "          classes[i],acc_i ,\n",
        "          class_correct[i], class_total[i]))\n",
        "    else:\n",
        "      print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "  acc = 100. * np.sum(class_correct) / total_samples\n",
        "  print('\\nTest Accuracy (Overall): %2d%% (%d/%d)' % (\n",
        "      acc,\n",
        "      sum(class_correct), total_samples))\n",
        "  return test_loss, acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVwz1WydnN_7"
      },
      "source": [
        "## **CIFAR10**\n",
        "The CIFAR10 and CIFAR-100 are labeled subsets of the 80 million tiny images dataset. They were collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.\n",
        "\n",
        "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
        "\n",
        "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\n",
        "\n",
        "Here are the classes in the dataset:\n",
        "1. airplane\n",
        "2. automobile\n",
        "3. bird\n",
        "4. cat\n",
        "5. deer\n",
        "6. dog\n",
        "7. frog\n",
        "8. horse\n",
        "9. ship\n",
        "10. truck\n",
        "\n",
        "The classes are completely mutually exclusive. There is no overlap between automobiles and trucks. \"Automobile\" includes sedans, SUVs, things of that sort. \"Truck\" includes only big trucks. Neither includes pickup trucks.\n",
        "\n",
        "More can be read from their page at https://www.cs.toronto.edu/~kriz/cifar.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Download And Augmentation\n",
        "#**Downloading the CIFAR10 datset and loading the data in Normalized form as torch.FloatTensor datatype and generating a validation set by dividing the training set in 80-20 ratio**\n",
        "\n",
        "#**download dataset to local data directory**\n",
        "#**Image Augmentation**\n",
        "In this cell, we perform some simple data augmentation by randomly flipping and cropping the given image data. We do this by defining a torchvision transform, and you can learn about all the transforms that are used to pre-process and augment data from the [PyTorch documentation](https://pytorch.org/docs/stable/torchvision/transforms.html)\n",
        "\n",
        "\n",
        "we split train data to two part one is about 0.1 part of train data used as val data ,the last 0.9 part is used as train data\n",
        "\n",
        "test data only used once to check model at last\n",
        "\n",
        "so we choose 1000 for each class from 50,000 train data set  and split it to train data(0.9) and val data(0.1)\n",
        "choose 100 for each class from 10,000 test data as test data;\n",
        "\n",
        "we use val data to choose best learning rate,and use test data for accuracy and loss validation to avoid data leaking caused  validation loss overfit(val loss become much higher than lowest val loss at the end of training and training loss is very low)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKi4TfnSIcm4",
        "outputId": "0cf0110a-20a6-4ca3-a7e0-d2aa2f87e65a"
      },
      "outputs": [],
      "source": [
        "import torchvision, torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = transforms.ToTensor()   # 只先转成 [0,1] tensor，不做 Normalize\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "loader = DataLoader(trainset, batch_size=100, shuffle=False, num_workers=4)\n",
        "\n",
        "mean = 0.0\n",
        "var = 0.0\n",
        "total = 0\n",
        "\n",
        "for imgs, _ in loader:\n",
        "    # imgs shape: [B, 3, 32, 32], 值在 [0,1]\n",
        "    batch_samples = imgs.size(0)\n",
        "    imgs = imgs.view(batch_samples, 3, -1)  # [B, 3, 32*32]\n",
        "    mean += imgs.mean(dim=(0, 2)) * batch_samples\n",
        "    var  += imgs.var(dim=(0, 2), unbiased=False) * batch_samples\n",
        "    total += batch_samples\n",
        "\n",
        "mean /= total\n",
        "var  /= total\n",
        "std = var.sqrt()\n",
        "\n",
        "print(mean, std)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5WmV1je_1kr",
        "outputId": "36adadf4-44b8-4210-aafc-6167f00cd0bc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import Subset, random_split, DataLoader\n",
        "def sub_dataset(trainset, each_class_len,need_random=False):\n",
        "  subset_indices = []\n",
        "  targets = np.array(trainset.targets)\n",
        "  for c in np.unique(targets):\n",
        "    class_idx = np.where(targets == c)[0]\n",
        "    # print(class_idx.shape,class_idx[:10])\n",
        "    if need_random:\n",
        "      np.random.shuffle(class_idx)\n",
        "    subset_indices.extend(class_idx[:each_class_len])\n",
        "  # Convert subset_indices to a numpy array to allow advanced indexing\n",
        "  subset_indices = np.array(subset_indices)\n",
        "  return subset_indices\n",
        "\n",
        "def sub_sub_indices(subset_indices, val_ratio=0.1,need_random=True):\n",
        "  # train_set,val_set = random_split(train_subset,[n_train,n_val])\n",
        "  train_indices = []\n",
        "  val_indices = []\n",
        "  val_ratio = 0.1\n",
        "  subset_labels = targets[subset_indices]\n",
        "  for c in np.unique(subset_labels):\n",
        "    class_idx = np.where(subset_labels == c)[0]\n",
        "    if need_random:\n",
        "      np.random.shuffle(class_idx)\n",
        "    n_val = int(len(class_idx)*val_ratio)\n",
        "    val_indices.extend(subset_indices[class_idx[:n_val]])\n",
        "    train_indices.extend(subset_indices[class_idx[n_val:]])\n",
        "  return train_indices, val_indices\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    #transforms.Resize(40),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2467, 0.2432, 0.2611))\n",
        "])\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2467, 0.2432, 0.2611))\n",
        "])\n",
        "batch_size = 128\n",
        "# --- Load full CIFAR-10 train set ---\n",
        "trainset_full = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "targets = np.array(trainset_full.targets)\n",
        "# --- Class names ---\n",
        "classes = trainset_full.classes\n",
        "# classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "#            'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "print(classes)\n",
        "\n",
        "# --- (Optional) use only a subset for speed\n",
        "subset_indices = sub_dataset(trainset_full, 110, need_random=False)\n",
        "#train_subset = Subset(trainset_full, subset_indices)\n",
        "\n",
        "# --- Split subset_indices into train/validation ---\n",
        "train_indices, val_indices = sub_sub_indices(subset_indices, val_ratio=0.1, need_random=False)\n",
        "train_set = Subset(trainset_full,train_indices)\n",
        "val_set  = Subset(trainset_full, val_indices)\n",
        "train_labels = [classes[x] for x in targets[train_indices]]\n",
        "val_labels = [classes[x] for x in targets[val_indices]]\n",
        "# use all\n",
        "# train_set = trainset_full\n",
        "# train_labels = [classes[x] for x in targets]\n",
        "# --- DataLoaders ---\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True,num_workers=4)\n",
        "val_loader = DataLoader(val_set,batch_size = batch_size,shuffle=False,num_workers=4)\n",
        "# --- Test Set ---\n",
        "test_set = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform_test)\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False,num_workers=4)\n",
        "test_targets = np.array(test_set.targets)\n",
        "test_labels = [classes[x] for x in test_targets]\n",
        "# use part of test\n",
        "test_indices = sub_dataset(test_set, 100, need_random=False)\n",
        "test_set_sub =  Subset(test_set, test_indices)\n",
        "test_labels = [classes[x] for x in test_targets[test_indices]]\n",
        "test_loader =  DataLoader(test_set_sub,batch_size = batch_size, shuffle=False)\n",
        "#use test set as val set\n",
        "# val_set = test_set\n",
        "# val_loader = test_loader\n",
        "# val_labels = test_labels\n",
        "\n",
        "print(f\"Batch size:{batch_size}, \\\n",
        "      Train batches:{len(train_loader)}, \\\n",
        "      Val batches:{len(val_loader)},\\\n",
        "      Test batches:{len(test_loader)}\")\n",
        "# check train and val set is balanced or not\n",
        "from collections import Counter\n",
        "print(\"Train class counts:\",Counter(train_labels))\n",
        "print(\"Val class counts:\", Counter(val_labels))\n",
        "print(\"Test class counts:\", Counter(test_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdIarqe4tnBs"
      },
      "source": [
        "### **Visualizing the Data**\n",
        "Obtaining a batch of training data and plot the same with its lables using matplotlib library. You can also see how the transformations which you applied in the previous step show up in these visualizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 637
        },
        "id": "xhhpI2ntAB8f",
        "outputId": "b5b37f16-3398-4977-87da-19b1cf330928"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "print(classes)\n",
        "%matplotlib inline\n",
        "\n",
        "# helper function to un-normalize and display an image\n",
        "def imshow(img):\n",
        "  img = img / 2 + 0.5  # unnormalize\n",
        "  plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image\n",
        "\n",
        "# obtain one batch of training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "images = images.numpy() # convert images to numpy for display\n",
        "print(images[0].shape)\n",
        "# plot the images in the batch, along with the corresponding labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "# display 20 images\n",
        "for idx in np.arange(20):\n",
        "  ax = fig.add_subplot(2, 20//2, idx+1, xticks=[], yticks=[])\n",
        "  imshow(images[idx])\n",
        "  ax.set_title(classes[labels[idx]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQBRSenSvD7P"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDAlhqzyyYaG"
      },
      "source": [
        "## AdaOrth Optimizer\n",
        "**Specifying the Loss Function and Optimizer**\n",
        "We use CrossEntropyLoss as Loss function and\n",
        "\n",
        "[Stochastic Gradient Descent](https://leon.bottou.org/publications/pdf/compstat-2010.pdf)\n",
        "Adam\n",
        "MuonMVR2\n",
        "AdaOrth\n",
        " as Optimizer with momentum and weight decay specified by the research paper of **ON THE CONVERGENCE OF MUON AND BEYOND**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXwhO0FpIcm5"
      },
      "source": [
        "#**AdaOrthL method:** \n",
        "\n",
        "\n",
        "![alt text](https://github.com/RayCyder/Intro/blob/main/adaorth.jpg?raw=1)\n",
        "\n",
        "#**AdaOrthL+ method:** similar like MuonMVR2 use two random batches to decrease noise\n",
        "\n",
        "![alt text](https://github.com/RayCyder/Intro/blob/main/adaorth+.jpeg?raw=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AluijoKUIcm5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# ---------- compile helper (MPS uses aot_eager; CUDA/CPU uses inductor) ----------\n",
        "def _compile_for_device(fn):\n",
        "    if torch.backends.mps.is_available():\n",
        "        return torch.compile(fn, backend=\"aot_eager\", dynamic=True)\n",
        "    return torch.compile(fn, backend=\"inductor\", dynamic=True)\n",
        "\n",
        "\n",
        "@_compile_for_device\n",
        "def zeropower_via_newtonschulz5(\n",
        "    G: torch.Tensor,\n",
        "    steps: int = 3,\n",
        "    eps: float = 1e-7,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Approximate Orth(G) via a Newton–Schulz-style iteration (polar-factor-like).\n",
        "\n",
        "    - Uses bf16 for matmul speed; returns to original dtype.\n",
        "    - Normalizes by Frobenius norm as a cheap stabilizer (keeps scale small).\n",
        "    - If m>n, works on G^T to reduce cost of A = X X^T.\n",
        "    \"\"\"\n",
        "    assert G.ndim == 2\n",
        "    a, b, c = 3.4445, -4.7750, 2.0315\n",
        "\n",
        "    out_dtype = G.dtype\n",
        "    bf16 = torch.bfloat16\n",
        "\n",
        "    # Work in bf16 for speed; keep on same device.\n",
        "    X = G.to(dtype=bf16)\n",
        "\n",
        "    # Stabilize scaling: avoid division by zero and keep values bounded.\n",
        "    # (Frobenius norm is cheap; if you prefer spectral norm, it's too expensive.)\n",
        "    X = X / (X.norm() + eps)\n",
        "\n",
        "    m, n = X.shape\n",
        "    transposed = m > n\n",
        "    if transposed:\n",
        "        X = X.transpose(0, 1)  # .T is fine too; transpose is explicit\n",
        "\n",
        "    # Newton–Schulz iterations\n",
        "    for _ in range(steps):\n",
        "        A = X @ X.transpose(0, 1)      # A = X X^T\n",
        "        A2 = A @ A                    # reuse A^2\n",
        "        B = b * A + c * A2\n",
        "        X = a * X + (B @ X)\n",
        "\n",
        "    if transposed:\n",
        "        X = X.transpose(0, 1)\n",
        "\n",
        "    return X.to(dtype=out_dtype)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Base optimizer\n",
        "# =========================\n",
        "class AdaOrthBasic(torch.optim.Optimizer):\n",
        "    \"\"\"\n",
        "    Base class:\n",
        "      - 2D params (matrices): delegate to step_detail(...) implemented by subclasses\n",
        "      - non-2D params: use a simple AdamW-like update with a corrected input c_t\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        params,\n",
        "        lr: float = 1e-3,\n",
        "        weight_decay: float = 0.0,\n",
        "        adamw_betas=(0.95, 0.99),\n",
        "        gamma: float = 0.025,\n",
        "        eps: float = 1e-8,\n",
        "    ):\n",
        "        defaults = dict(lr=lr, weight_decay=weight_decay, adamw_betas=adamw_betas, gamma=gamma, eps=eps)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update_last_grad(self):\n",
        "        \"\"\"Cache current p.grad into state['last_grad'] for all params.\"\"\"\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                state = self.state[p]\n",
        "                if \"last_grad\" not in state:\n",
        "                    state[\"last_grad\"] = torch.zeros_like(p)\n",
        "                state[\"last_grad\"].copy_(p.grad)\n",
        "\n",
        "    # ---- hooks for subclasses ----\n",
        "    def step_detail(self, grad: torch.Tensor, state: dict, *, eps: float, lr: float) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Return (eta_t, update_matrix) for 2D parameters. Must be overridden.\"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # ---- internal helpers ----\n",
        "    @staticmethod\n",
        "    def _init_scalar_state_like(p: torch.Tensor, grad: torch.Tensor, value: float) -> torch.Tensor:\n",
        "        \"\"\"Create a 0-dim tensor on the same device/dtype as grad.\"\"\"\n",
        "        return torch.tensor(value, device=p.device, dtype=grad.dtype)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _init_state(self, p: torch.Tensor, grad: torch.Tensor):\n",
        "        state = self.state[p]\n",
        "        if len(state) != 0:\n",
        "            return\n",
        "\n",
        "        state[\"step\"] = 0\n",
        "        state[\"last_grad\"] = torch.zeros_like(p)\n",
        "\n",
        "        if p.ndim == 2:\n",
        "            # 2D (matrix) states (subclasses may use a subset, but it's fine to pre-create)\n",
        "            state[\"m_t\"] = torch.zeros_like(p)\n",
        "\n",
        "            # scalar accumulators as 0-dim tensors (device-safe)\n",
        "            state[\"g_sum\"] = self._init_scalar_state_like(p, grad, 0.0)   # Σ g_i^2\n",
        "            state[\"g_max\"] = self._init_scalar_state_like(p, grad, 0.0)   # max g_i^2\n",
        "\n",
        "            # common accumulator for Σ ||M_i||^2 / α_i\n",
        "            state[\"H_sum\"] = self._init_scalar_state_like(p, grad, 0.0)\n",
        "\n",
        "            # for AdaOrth-L+ prefix-min alpha\n",
        "            state[\"alpha\"] = self._init_scalar_state_like(p, grad, 1.0)   # alpha_0 = 1\n",
        "        else:\n",
        "            # non-2D: AdamW-like moments\n",
        "            state[\"exp_avg\"] = torch.zeros_like(p)\n",
        "            state[\"exp_avg_sq\"] = torch.zeros_like(p)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _step_non2d(self, p: torch.Tensor, grad: torch.Tensor, state: dict, *, lr: float, eps: float,\n",
        "                    weight_decay: float, beta1: float, beta2: float, gamma: float):\n",
        "        \"\"\"Non-2D parameters: corrected-input AdamW-like update (engineering choice).\"\"\"\n",
        "        step = state[\"step\"]\n",
        "        last_grad = state[\"last_grad\"]\n",
        "\n",
        "        # c_t = g_t + gamma * (beta1/(1-beta1)) * (g_t - g_{t-1})\n",
        "        c_t = (grad - last_grad).mul(gamma * (beta1 / (1.0 - beta1))).add(grad)\n",
        "\n",
        "        # unit-norm clipping to avoid rare spikes\n",
        "        c_norm = torch.norm(c_t)\n",
        "        if c_norm > 1.0:\n",
        "            c_t = c_t / c_norm\n",
        "\n",
        "        exp_avg = state[\"exp_avg\"]\n",
        "        exp_avg_sq = state[\"exp_avg_sq\"]\n",
        "\n",
        "        # EMA updates\n",
        "        exp_avg.lerp_(c_t, 1.0 - beta1)               # m_t\n",
        "        exp_avg_sq.lerp_(c_t.square(), 1.0 - beta2)   # v_t\n",
        "\n",
        "\n",
        "        g_hat = exp_avg / (eps + exp_avg_sq.sqrt())\n",
        "\n",
        "        # bias correction\n",
        "        bias_correction1 = 1.0 - beta1 ** step\n",
        "        bias_correction2 = 1.0 - beta2 ** step\n",
        "        scale = bias_correction1 / (bias_correction2 ** 0.5)\n",
        "\n",
        "        # decoupled weight decay + update\n",
        "        p.data.mul_(1.0 - lr * weight_decay)\n",
        "        p.data.add_(g_hat, alpha=-lr / scale)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self):\n",
        "        \"\"\"\n",
        "        One optimizer step. n.   \n",
        "\n",
        "        For 2D params: delegates to step_detail(...) implemented in subclasses (AdaOrth-L / AdaOrth-L+).\n",
        "        For non-2D params: uses a simple AdamW-like rule with corrected input c_t.\n",
        "\n",
        "        IMPORTANT (AdaOrth-L+):\n",
        "          state[\"last_grad\"] must store ∇f(X_{t-1}; ξ_t) computed on the SAME ξ_t as current grad ∇f(X_t; ξ_t).\n",
        "          This must be ensured by the training loop via update_last_grad().\n",
        "        \"\"\"\n",
        "        for group in self.param_groups:\n",
        "            lr = group[\"lr\"]\n",
        "            beta1, beta2 = group[\"adamw_betas\"]\n",
        "            eps = group[\"eps\"]\n",
        "            weight_decay = group[\"weight_decay\"]\n",
        "            gamma = group[\"gamma\"]\n",
        "\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                grad = p.grad\n",
        "                self._init_state(p, grad)\n",
        "\n",
        "                state = self.state[p]\n",
        "                state[\"step\"] += 1\n",
        "\n",
        "                if p.ndim == 2:\n",
        "                    eta_t, update = self.step_detail(grad, state, eps=eps, lr=lr)\n",
        "                    # engineering choice: keep decoupled weight decay for matrices as well\n",
        "                    p.data.mul_(1.0 - lr * weight_decay)\n",
        "                    p.data.add_(update, alpha=-eta_t)\n",
        "                else:\n",
        "                    self._step_non2d(\n",
        "                        p, grad, state,\n",
        "                        lr=lr, eps=eps, weight_decay=weight_decay,\n",
        "                        beta1=beta1, beta2=beta2, gamma=gamma\n",
        "                    )\n",
        "\n",
        "\n",
        "# =========================\n",
        "# AdaOrth-L (Algorithm 1)\n",
        "# =========================\n",
        "class AdaOrthL(AdaOrthBasic):\n",
        "    @torch.no_grad()\n",
        "    def step_detail(self, grad: torch.Tensor, state: dict, *, eps: float, lr: float):\n",
        "        # Paper mapping:\n",
        "        #   G_t=grad,  g_t^2=||G_t||_F^2,  α_t=(1+Σ g_i^2)^(-1/2),\n",
        "        #   M_t=(1-α_t)M_{t-1}+α_t G_t,\n",
        "        #   H_t=Σ ||M_i||^2/α_i,\n",
        "        #   γ_t=min( α_t/(eps+max g^2), (eps+H_t)^(-1/2) ),\n",
        "        #   η_t=θ γ_t ||M_t||_F (θ=lr),\n",
        "        #   update=Orth(M_t)\n",
        "        g_sq = grad.norm().pow(2)\n",
        "        state[\"g_sum\"] = state[\"g_sum\"] + g_sq\n",
        "        state[\"g_max\"] = torch.maximum(state[\"g_max\"], g_sq)\n",
        "\n",
        "        alpha = (1.0 + state[\"g_sum\"]).pow(-0.5)\n",
        "\n",
        "        M = state[\"m_t\"]\n",
        "        M.mul_(1.0 - alpha).add_(grad, alpha=alpha)\n",
        "        m_norm = M.norm()\n",
        "\n",
        "        H = state[\"H_sum\"] + (m_norm.pow(2) / (alpha + eps))\n",
        "        state[\"H_sum\"] = H\n",
        "\n",
        "        gamma1 = alpha / (eps + state[\"g_max\"])\n",
        "        gamma2 = (eps + H).pow(-0.5)\n",
        "        gamma_t = torch.minimum(gamma1, gamma2)\n",
        "\n",
        "        eta_t = lr * gamma_t * m_norm\n",
        "        update = zeropower_via_newtonschulz5(M)\n",
        "        return eta_t, update\n",
        "\n",
        "\n",
        "# =========================\n",
        "# AdaOrth-L+ (Algorithm 2)\n",
        "# =========================\n",
        "class AdaOrthLPlus(AdaOrthBasic):\n",
        "    @torch.no_grad()\n",
        "    def step_detail(self, grad: torch.Tensor, state: dict, *, eps: float, lr: float):\n",
        "        # Paper mapping:\n",
        "        #   G_t=grad,  G_{t-1}(same ξ_t)=state[\"last_grad\"],\n",
        "        #   α_t = min_{k<=t} ((eps+max g_i^2)/(eps+Σ g_i^2))^(2/3) (prefix-min recursion),\n",
        "        #   M_t=(1-α_t)(M_{t-1}-G_{t-1})+G_t,\n",
        "        #   H_t=Σ ||M_i||^2/α_i,\n",
        "        #   γ_t=min( sqrt(α_t)/(eps+max g^2), (eps+H_t)^(-1/2) ),\n",
        "        #   η_t=θ γ_t ||M_t||_F (θ=lr),\n",
        "        #   update=Orth(M_t)\n",
        "        g_sq = grad.norm().pow(2)\n",
        "        state[\"g_sum\"] = state[\"g_sum\"] + g_sq\n",
        "        state[\"g_max\"] = torch.maximum(state[\"g_max\"], g_sq)\n",
        "\n",
        "        r_t = ((eps + state[\"g_max\"]) / (eps + state[\"g_sum\"])).pow(2.0 / 3.0)\n",
        "        alpha = torch.minimum(state[\"alpha\"], r_t)   # prefix-min\n",
        "        state[\"alpha\"] = alpha\n",
        "\n",
        "        M = state[\"m_t\"]\n",
        "        M.sub_(state[\"last_grad\"]).mul_(1.0 - alpha).add_(grad)\n",
        "        m_norm = M.norm()\n",
        "\n",
        "        H = state[\"H_sum\"] + (m_norm.pow(2) / (alpha + eps))\n",
        "        state[\"H_sum\"] = H\n",
        "\n",
        "        gamma1 = alpha.sqrt() / (eps + state[\"g_max\"])\n",
        "        gamma2 = (eps + H).pow(-0.5)\n",
        "        gamma_t = torch.minimum(gamma1, gamma2)\n",
        "\n",
        "        eta_t = lr * gamma_t * m_norm\n",
        "        update = zeropower_via_newtonschulz5(M)\n",
        "        return eta_t, update"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lw1tuJ5UIcm5"
      },
      "source": [
        "## Muon-MVR2 Optimzier\n",
        "**Muon-MVR2 method from ON THE CONVERGENCE OF MUON AND BEYOND**\n",
        "\n",
        "fixed some error for mac M1\n",
        "\n",
        "mvr2 only works with 2 dim weights, other dims use method like adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEAqv4_vfqlD"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Muon-MVR For LLM Pretraining.\n",
        "'''\n",
        "import torch\n",
        "import math\n",
        "\n",
        "#fix for mps\n",
        "def _compile_for_device(fn):\n",
        "    # Inductor doesn't support MPS; use AOTAutograd there.\n",
        "    if torch.backends.mps.is_available():\n",
        "        return torch.compile(fn, backend=\"aot_eager\", dynamic=True)\n",
        "    # Inductor is fine on CUDA/CPU\n",
        "    return torch.compile(fn, backend=\"inductor\", dynamic=True)\n",
        "\n",
        "\n",
        "@_compile_for_device\n",
        "def zeropower_via_newtonschulz5(G, steps=3, eps=1e-7):\n",
        "    \"\"\"\n",
        "    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G.\n",
        "    \"\"\"\n",
        "    assert len(G.shape) == 2\n",
        "    a, b, c = (3.4445, -4.7750,  2.0315)\n",
        "    d_type = G.dtype\n",
        "    X = G.bfloat16()\n",
        "    #x_flat = X.reshape(-1)      # 把矩阵展平成一个 1D 向量\n",
        "    X /= (X.norm() + eps) # ensure top singular value <= 1\n",
        "    if G.size(0) > G.size(1):\n",
        "        X = X.T\n",
        "    for _ in range(steps):\n",
        "        A = X @ X.T\n",
        "        B = b * A + c * A @ A\n",
        "        X = a * X + B @ X\n",
        "    if G.size(0) > G.size(1):\n",
        "        X = X.T\n",
        "    return X.to(d_type)\n",
        "\n",
        "class MuonMVR(torch.optim.Optimizer):\n",
        "    r'''\n",
        "    Standard MVR:\\\\(nabla f(X_t;\\xi_t) - \\\\nabla f(X_{t-1};\\xi_{t})\n",
        "    Approximate MVR:\n",
        "        1.\\\\(nabla f(X_t;\\xi_t) - \\\\nabla f(X_{t-1};\\xi_{t-1})\n",
        "        2.\\\\(nabla f(X_t;\\xi_t) - \\\\nabla f(X_{t};\\xi_{t-1}), It has low computational complexity\n",
        "         and is more convenient in practice\n",
        "    '''\n",
        "    def __init__(self, params, lr=3e-3, momentum = 0.95 ,adamw_betas=(0.95, 0.99), eps=1e-8,\n",
        "                 weight_decay=0.0, gamma=0.025, is_approx=False):\n",
        "        if lr < 0.0:\n",
        "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
        "        if not 0.0 <= momentum < 1.0:\n",
        "            raise ValueError(f\"Invalid momentum parameter: {momentum}\")\n",
        "        if not 0.0 <= adamw_betas[0] < 1.0:\n",
        "            raise ValueError(f\"Invalid beta1 parameter: {adamw_betas[0]}\")\n",
        "        if not 0.0 <= adamw_betas[1] < 1.0:\n",
        "            raise ValueError(f\"Invalid beta2 parameter: {adamw_betas[1]}\")\n",
        "\n",
        "        defaults = dict(lr=lr, momentum=momentum,adamw_betas=adamw_betas, eps=eps,\n",
        "                       weight_decay=weight_decay, gamma=gamma)\n",
        "        super().__init__(params, defaults)\n",
        "        self.is_approx = is_approx\n",
        "\n",
        "    def adjust_lr_for_muon(self, lr, param_shape):\n",
        "        A, B = param_shape[:2]\n",
        "        # We adjust the learning rate and weight decay based on the size of the parameter matrix\n",
        "        # as describted in the paper\n",
        "        adjusted_ratio = 0.2 * math.sqrt(max(A, B))\n",
        "        # adjusted_ratio = math.sqrt(A*B)\n",
        "        adjusted_lr = lr * adjusted_ratio\n",
        "        return adjusted_lr\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update_last_grad(self):\n",
        "        if not self.is_approx:\n",
        "            for group in self.param_groups:\n",
        "                for p in group['params']:\n",
        "                    state = self.state[p]\n",
        "                    if \"last_grad\" not in state:\n",
        "                        state[\"last_grad\"] = torch.zeros_like(p)\n",
        "                    state[\"last_grad\"].zero_().add_(p.grad, alpha=1.0)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self):\n",
        "        for group in self.param_groups:\n",
        "            lr = group['lr']\n",
        "            beta1, beta2 = group['adamw_betas']\n",
        "            eps = group['eps']\n",
        "            weight_decay = group['weight_decay']\n",
        "            momentum = group['momentum']\n",
        "            gamma = group['gamma']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                grad = p.grad\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p)\n",
        "                    state['last_grad'] = torch.zeros_like(p)\n",
        "                    # state['previous_grad'] = torch.zeros_like(p)\n",
        "                    if len(p.shape) != 2:  # Only for 2D tensors\n",
        "                        state['exp_avg_sq'] = torch.zeros_like(p)\n",
        "\n",
        "                state['step'] += 1\n",
        "                last_grad = state['last_grad']\n",
        "                if len(p.shape) == 2:\n",
        "                    exp_avg = state['exp_avg']\n",
        "\n",
        "                    # Compute momentum-like term with correction\n",
        "                    c_t = (grad - last_grad).mul(gamma * (momentum / (1. - momentum))).add(grad)\n",
        "                    c_t_norm = torch.norm(c_t)\n",
        "                    if c_t_norm > 1.:\n",
        "                        c_t = c_t / c_t_norm\n",
        "                    # Update moving averages\n",
        "                    exp_avg.mul_(momentum).add_(c_t, alpha=1 - momentum)\n",
        "                    update = zeropower_via_newtonschulz5(exp_avg.mul(1./(1.- momentum))) # whiten the update\n",
        "                    adjusted_lr = self.adjust_lr_for_muon(lr, p.shape)\n",
        "                    p.data.mul_(1 - lr * weight_decay)\n",
        "                    p.data.add_(update, alpha=-adjusted_lr)\n",
        "                else:\n",
        "                    # For bias vectors - use simple update\n",
        "                    step = state['step']\n",
        "                    # Compute momentum-like term with correction\n",
        "                    c_t = (grad - last_grad).mul(gamma * (beta1 / (1. - beta1))).add(grad)\n",
        "                    c_t_norm = torch.norm(c_t)\n",
        "                    # avoid inductor lowering bug: compute norm explicitly and detach\n",
        "                    # c_t_norm = torch.sqrt(torch.sum((c_t.detach() * c_t.detach()), dim=None))\n",
        "                    if c_t_norm > 1.:\n",
        "                        c_t = c_t / c_t_norm\n",
        "                    exp_avg = state['exp_avg']\n",
        "                    exp_avg_sq = state['exp_avg_sq']\n",
        "                    exp_avg.lerp_(c_t, 1 - beta1)\n",
        "                    exp_avg_sq.lerp_(c_t.square(), 1 - beta2)\n",
        "                    g = exp_avg / (eps + exp_avg_sq.sqrt())\n",
        "                    bias_correction1 = 1 - beta1**step\n",
        "                    bias_correction2 = 1 - beta2**step\n",
        "                    scale = bias_correction1 / bias_correction2**0.5\n",
        "                    p.data.mul_(1 - lr * weight_decay)\n",
        "                    p.data.add_(g, alpha=-lr / scale)\n",
        "\n",
        "                if self.is_approx:\n",
        "                    state['last_grad'].copy_(grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dMWiYJf1Ou-"
      },
      "source": [
        "#**MuonMVR Training Loop**\n",
        "Here we train the architecture on training data and check its validation loss by using the validation set and saving the model only if there is an improvement ie decrease in the validation loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bFAzI_3Vwhj"
      },
      "source": [
        "### MVR2 Usage\n",
        "3.  **Optimizer Modes**\n",
        "    MuonMVR can be initialized in different modes to trade off between precision and computational cost.\n",
        "\n",
        "    **Exact Variance Reduction (`is_approx=False`)**\n",
        "    To achieve the most precise variance reduction, you must manually manage the model state. Before calculating the gradient for the previous batch, you need to load the model state from the previous iteration. This ensures that the gradient is computed with the correct model weights.\n",
        "    ```python\n",
        "    optimizer = MuonMVR(model.parameters(), lr=1e-3, is_approx=False)\n",
        "    old_state_dict = {}\n",
        "    \n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        # Store the current model state\n",
        "        cur_state_dict = {k: v.data.clone() for k, v in net.state_dict().items()}\n",
        "    \n",
        "        if old_state_dict:\n",
        "            # Load the previous model state to compute the old gradient\n",
        "            net.load_state_dict(old_state_dict)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.update_last_grad()\n",
        "    \n",
        "        # Restore the current model state to compute the new gradient\n",
        "        net.load_state_dict(cur_state_dict)\n",
        "        old_state_dict = {k: v.data.clone() for k, v in cur_state_dict.items()}\n",
        "        \n",
        "        # Standard forward/backward pass and step\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimizer with Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiicKSbbint0"
      },
      "outputs": [],
      "source": [
        "class MuonMVR2Optimizer(BaseOptimizer):\n",
        "  def __init__(self,model,criterion, epochs, learning_rate):\n",
        "    super().__init__(model,criterion, epochs, learning_rate)\n",
        "    self.old_state_dict = {}\n",
        "  def initialize_optimizer(self):\n",
        "    learning_rate = self.learning_rate\n",
        "    optimizer = MuonMVR(self.model.parameters(), lr=learning_rate,weight_decay=0.01,gamma=0.1,is_approx=False)\n",
        "    # Cosine annealing: lr → eta_min by the end\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer,\n",
        "        T_max=self.epochs,   # period (usually = total epochs)\n",
        "        eta_min=0.1*learning_rate      # minimum LR (here 0.1× of 0.01)\n",
        "    )\n",
        "    self.optimizer,self.scheduler =  optimizer, scheduler\n",
        "\n",
        "  def zero_grad(self,inputs=None,labels=None):\n",
        "    model, old_state_dict = self.model, self.old_state_dict\n",
        "    optimizer = self.optimizer\n",
        "    # Store the current model state\n",
        "    cur_state_dict = {k: v.data.clone() for k, v in model.state_dict().items()}\n",
        "    if old_state_dict:\n",
        "        # Load the previous model state to compute the old gradient\n",
        "        model.load_state_dict(old_state_dict)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = self.criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.update_last_grad()\n",
        "    #restore state\n",
        "    model.load_state_dict(cur_state_dict)\n",
        "    self.old_state_dict = {k: v.data.clone() for k, v in cur_state_dict.items()}\n",
        "    self.optimizer.zero_grad()\n",
        "\n",
        "\n",
        "class SGDOptimizer(BaseOptimizer):\n",
        "  def __init__(self,model,criterion,epochs,learning_rate):\n",
        "    super().__init__(model,criterion,epochs, learning_rate)\n",
        "  def initialize_optimizer(self):\n",
        "    learning_rate = self.learning_rate\n",
        "    optimizer = optim.SGD(self.model.parameters(), lr=learning_rate, weight_decay=0.01,momentum=0.9)\n",
        "    # Cosine annealing: lr → eta_min by the end\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer,\n",
        "        T_max=self.epochs,   # period (usually = total epochs)\n",
        "        eta_min=0.1*learning_rate      # minimum LR (here 0.1× of 0.01)\n",
        "    )\n",
        "    self.optimizer,self.scheduler =  optimizer, scheduler\n",
        "\n",
        "\n",
        "class AdamOptimizer(BaseOptimizer):\n",
        "  def __init__(self,model,criterion,epochs,learning_rate):\n",
        "    super().__init__(model,criterion,epochs, learning_rate)\n",
        "  def initialize_optimizer(self):\n",
        "    learning_rate = self.learning_rate\n",
        "    optimizer = optim.Adam(self.model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "    # Cosine annealing: lr → eta_min by the end\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer,\n",
        "        T_max=self.epochs,   # period (usually = total epochs)\n",
        "        eta_min=0.1*learning_rate      # minimum LR (here 0.1× of 0.01)\n",
        "    )\n",
        "    self.optimizer,self.scheduler =  optimizer, scheduler\n",
        "\n",
        "\n",
        "class AdaOrthLOptimizer(BaseOptimizer):\n",
        "  def __init__(self,model,criterion,epochs,learning_rate):\n",
        "    super().__init__(model,criterion,epochs, learning_rate)\n",
        "    self.old_state_dict = {}\n",
        "  def initialize_optimizer(self):\n",
        "    learning_rate = self.learning_rate\n",
        "    optimizer = AdaOrthL(self.model.parameters(), lr=learning_rate, weight_decay=0.01,gamma=0.1)\n",
        "    # Cosine annealing: lr → eta_min by the end\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer,\n",
        "        T_max=self.epochs,   # period (usually = total epochs)\n",
        "        eta_min=0.1*learning_rate      # minimum LR (here 0.1× of 0.01)\n",
        "    )\n",
        "    self.optimizer,self.scheduler =  optimizer, scheduler\n",
        "  def zero_grad(self,inputs=None,labels=None):\n",
        "    model, old_state_dict = self.model, self.old_state_dict\n",
        "    optimizer = self.optimizer\n",
        "    # Store the current model state\n",
        "    cur_state_dict = {k: v.data.clone() for k, v in model.state_dict().items()}\n",
        "    if self.old_state_dict:\n",
        "        # Load the previous model state to compute the old gradient\n",
        "        model.load_state_dict(old_state_dict)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = self.criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.update_last_grad()\n",
        "    #restore state\n",
        "    model.load_state_dict(cur_state_dict)\n",
        "    self.old_state_dict = {k: v.data.clone() for k, v in cur_state_dict.items()}\n",
        "    self.optimizer.zero_grad()\n",
        "\n",
        "\n",
        "class AdaOrthLPLusOptimizer(BaseOptimizer):\n",
        "  def __init__(self,model,criterion,epochs,learning_rate):\n",
        "    super().__init__(model,criterion,epochs, learning_rate)\n",
        "  def initialize_optimizer(self):\n",
        "    #weight_decay=0.01,gamma=0.1\n",
        "    learning_rate = self.learning_rate\n",
        "    optimizer = AdaOrthLPlus(self.model.parameters(), lr=learning_rate, weight_decay=0.01,gamma=0.1)\n",
        "    # Cosine annealing: lr → eta_min by the end\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer,\n",
        "        T_max=self.epochs,   # period (usually = total epochs)\n",
        "        eta_min=0.1*learning_rate      # minimum LR (here 0.1× of 0.01)\n",
        "    )\n",
        "    self.optimizer,self.scheduler =  optimizer, scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvhesdndign7"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGej2WdOVHEs"
      },
      "source": [
        "## **grid search learning rate**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HvRH5HvVE_X"
      },
      "outputs": [],
      "source": [
        "# learning_rates = [1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2, 1e-1]\n",
        "# def grids_search_val(get_optimizer):\n",
        "#   result = []\n",
        "#   for ii, learning_rate in enumerate(learning_rates):\n",
        "#     print(f\".........Learning rate {ii+1}/{len(learning_rates)}: {learning_rate}............\")\n",
        "#     model = initialize_model(device)\n",
        "#     optimizer = get_optimizer(model, learning_rate)\n",
        "#     train_losses, train_accs, val_losses, val_accs, time_cost = train_once(model,criterion,optimizer,train_loader,val_loader,learning_rate,epochs)\n",
        "#     result.append(min(val_losses))\n",
        "#   return result\n",
        "\n",
        "# import copy\n",
        "# import numpy as np\n",
        "import copy\n",
        "def grid_search_val_no_earlystop(optimizer_class, learning_rates, train_object_val):\n",
        "    # 固定初始模型权重，保证不同lr可比（不动seed也行）\n",
        "    results = []  # 每个lr的 final val loss\n",
        "    for ii, lr in enumerate(learning_rates):\n",
        "        print(f\".........Learning rate {ii+1}/{len(learning_rates)}: {lr}............\")\n",
        "        train_object_val.initialize_model()\n",
        "        optimizer = optimizer_class(train_object_val.model,train_object_val.criterion,train_object_val.epochs, lr)\n",
        "        optimizer.initialize_optimizer()\n",
        "        optimizer.scheduler = None #search using no scheduler\n",
        "        train_losses, train_accs, val_losses, val_accs, time_cost  = train_object_val.train(optimizer, need_save=False)\n",
        "        #final_val_loss = float(val_losses[-1])   # ✅ 不用early stop\n",
        "        best_val_loss = min(val_losses)\n",
        "        results.append(best_val_loss)\n",
        "\n",
        "    best_i = int(np.argmin(results))\n",
        "    best_lr = learning_rates[best_i]\n",
        "    print(f\"\\n✅ Best lr by final val loss: {best_lr}, final val loss={results[best_i]:.6f}\")\n",
        "\n",
        "    return best_lr, results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "def _to_float_list(xs):\n",
        "    return [float(x) for x in xs]\n",
        "\n",
        "def save_grid_result(path: Path, payload: dict):\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with path.open(\"w\") as f:\n",
        "        json.dump(payload, f, indent=2, sort_keys=True)\n",
        "\n",
        "def load_grid_result(path: Path):\n",
        "    with path.open(\"r\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def grid_cache_path(cache_dir: str, optimizer_name: str, random_seed: int, tag: str = \"\") -> Path:\n",
        "    # tag can include model/dataset info, e.g., \"resnet18_cifar10\"\n",
        "    tag_part = f\"_{tag}\" if tag else \"\"\n",
        "    return Path(cache_dir) / f\"grid_{optimizer_name}{tag_part}_seed{random_seed}.json\"\n",
        "\n",
        "def get_or_run_grid_search(\n",
        "    optimizer_name: str,\n",
        "    opt_fn,\n",
        "    random_seed: int,\n",
        "    learning_rates,\n",
        "    train_object: TrainModel,\n",
        "    epochs: int,\n",
        "    cache_dir: str = \"cache_grid\",\n",
        "    tag: str = \"resnet18_cifar10\",\n",
        "    force_rerun: bool = False,\n",
        "):\n",
        "    \"\"\"\n",
        "    Returns: best_lr (float), losses (list[float])\n",
        "    Assumes grid_search_val_no_earlystop(opt_fn) returns (best_lr, losses).\n",
        "    \"\"\"\n",
        "    path = grid_cache_path(cache_dir, optimizer_name, random_seed, tag)\n",
        "\n",
        "    # metadata to ensure \"same run\"\n",
        "    meta = {\n",
        "        \"optimizer\": optimizer_name,\n",
        "        \"random_seed\": int(random_seed),\n",
        "        \"learning_rates\": _to_float_list(learning_rates),\n",
        "        \"epochs\": int(epochs),\n",
        "        \"tag\": tag,\n",
        "    }\n",
        "\n",
        "    if (not force_rerun) and path.exists():\n",
        "        cached = load_grid_result(path)\n",
        "\n",
        "        # basic compatibility check\n",
        "        if cached.get(\"meta\") == meta:\n",
        "            print(f\"[CACHE HIT] {optimizer_name} seed={random_seed} -> {path}\")\n",
        "            return float(cached[\"best_lr\"]), _to_float_list(cached[\"losses\"])\n",
        "        else:\n",
        "            print(f\"[CACHE MISMATCH] {optimizer_name} seed={random_seed} -> rerun\\n\"\n",
        "                  f\"  cached meta: {cached.get('meta')}\\n\"\n",
        "                  f\"  current meta: {meta}\")\n",
        "\n",
        "    # ---- run grid search ----\n",
        "    print(f\"[CACHE MISS] running grid search for {optimizer_name}, seed={random_seed}\")\n",
        "    # IMPORTANT: you must set seeds BEFORE calling grid_search_val_no_earlystop\n",
        "    # set_seed(random_seed)  # <- your seed function here\n",
        "\n",
        "    best_lr, losses = grid_search_val_no_earlystop(opt_fn,learning_rates,train_object)\n",
        "\n",
        "    payload = {\n",
        "        \"meta\": meta,\n",
        "        \"best_lr\": float(best_lr),\n",
        "        \"losses\": _to_float_list(losses),\n",
        "        \"saved_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
        "    }\n",
        "    save_grid_result(path, payload)\n",
        "    print(f\"[SAVED] {optimizer_name} seed={random_seed} -> {path}\")\n",
        "\n",
        "    return float(best_lr), _to_float_list(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "optimizers = {\n",
        "    \"Muon-MVR2\": MuonMVR2Optimizer,\n",
        "    \"SGD\": SGDOptimizer,\n",
        "    \"ADAM\": AdamOptimizer,\n",
        "    \"AdaOrthL\": AdaOrthLOptimizer,\n",
        "    \"AdaOrthL+\": AdaOrthLPLusOptimizer,\n",
        "}\n",
        "\n",
        "# 运行 grid search，保存结果\n",
        "results = {}\n",
        "best = {}\n",
        "train_object_val = TrainModel(train_loader, val_loader, classes, epochs=10)\n",
        "learning_rates = [1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2, 1e-1]\n",
        "\n",
        "for name, opt_fn in optimizers.items():\n",
        "    best_lr, losses = get_or_run_grid_search(\n",
        "    optimizer_name=name,\n",
        "    opt_fn=opt_fn,\n",
        "    random_seed=seed_seed,\n",
        "    learning_rates=learning_rates,\n",
        "    epochs=10,\n",
        "    cache_dir=\"cache_grid\",\n",
        "    tag=\"resnet18_cifar10\",\n",
        "    train_object = train_object_val\n",
        ")\n",
        "    results[name] = losses\n",
        "    best[name] = (best_lr, float(np.min(losses)))\n",
        "    print(f\"[{name}] best lr = {best_lr}, min val loss = {best[name][1]:.6f}\")\n",
        "\n",
        "# 画一张总图\n",
        "plt.figure(figsize=(7, 4.5))\n",
        "plt.xscale(\"log\")\n",
        "plt.xlabel(\"Learning rate\")\n",
        "plt.ylabel(\"Min Validation Loss\")\n",
        "plt.title(\"Learning Rate Grid Search (All Optimizers)\")\n",
        "plt.xticks(learning_rates, labels=[str(lr) for lr in learning_rates], rotation=45)\n",
        "\n",
        "for name, losses in results.items():\n",
        "    plt.plot(learning_rates, losses, marker=\"o\", linewidth=2, label=name)\n",
        "\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vMhznmaIcm6"
      },
      "source": [
        "## **training Model  using best learning rate**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRLLuJ7xIcm6"
      },
      "outputs": [],
      "source": [
        "train_object = TrainModel(train_loader, test_loader, classes, epochs=100)\n",
        "run_results = {}\n",
        "# for name, (best_lr, _) in best.items():\n",
        "for name in optimizers.keys():\n",
        "    best_lr = best[name][0]\n",
        "    print(f\"Training with best lr for {name}: {best_lr}\")\n",
        "    model = train_object.initialize_model()\n",
        "    optimizer = optimizers[name](model,train_object.criterion,train_object.epochs, best_lr)\n",
        "    optimizer.initialize_optimizer()\n",
        "    train_losses, train_accs, test_losses, test_accs, time_costs = train_object.train(optimizer, need_save=True)\n",
        "    run_results[name] = {\n",
        "        \"train_losses\": train_losses,\n",
        "        \"train_accs\": train_accs,\n",
        "        \"test_losses\": test_losses,\n",
        "        \"test_accs\": test_accs,\n",
        "        \"time_curve\": time_costs,\n",
        "    }\n",
        "    print(f\"{name} Test Accuracy: {test_accs[-1]:.2f}%, Time Cost: {time_costs[-1]:.2f}s\")\n",
        "# optimizer = muonmvr2_optimizer(model_mvr2, learning_rate)\n",
        "# losses_mvr2, mvr2_acc, losses_mvr2_val, mvr2_acc_val, mvr2_time_cost = train_once(model_mvr2,criterion,optimizer,train_loader,test_loader,learning_rate,epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4j1G8AgDIcnE"
      },
      "source": [
        "#**Plot Training Loss and Val Loss**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "def to_jsonable(x):\n",
        "    \"\"\"Convert common ML objects to JSON-serializable Python types.\"\"\"\n",
        "    try:\n",
        "        import numpy as np\n",
        "        if isinstance(x, np.ndarray):\n",
        "            return x.astype(float).tolist()\n",
        "        if isinstance(x, (np.floating,)):\n",
        "            return float(x)\n",
        "        if isinstance(x, (np.integer,)):\n",
        "            return int(x)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        import torch\n",
        "        if torch.is_tensor(x):\n",
        "            x = x.detach().cpu()\n",
        "            return x.tolist() if x.ndim > 0 else float(x.item())\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    if isinstance(x, (float, int, str, bool)) or x is None:\n",
        "        return x\n",
        "    if isinstance(x, (list, tuple)):\n",
        "        return [to_jsonable(v) for v in x]\n",
        "    if isinstance(x, dict):\n",
        "        return {str(k): to_jsonable(v) for k, v in x.items()}\n",
        "\n",
        "    # last resort: stringify\n",
        "    return str(x)\n",
        "\n",
        "def save_run_results(run_results: dict, filepath: str, meta: dict | None = None):\n",
        "    payload = {\n",
        "        \"meta\": to_jsonable(meta or {}),\n",
        "        \"run_results\": to_jsonable(run_results),\n",
        "    }\n",
        "    path = Path(filepath)\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(payload, f, indent=2, sort_keys=True)\n",
        "    print(f\"[SAVED] {path}\")\n",
        "\n",
        "def load_run_results(filepath: str):\n",
        "    path = Path(filepath)\n",
        "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        payload = json.load(f)\n",
        "    return payload[\"run_results\"], payload.get(\"meta\", {})\n",
        "meta = {\n",
        "    \"dataset\": \"CIFAR10\",\n",
        "    \"model\": \"ResNet18\",\n",
        "    \"epochs\": train_object.epochs,\n",
        "    \"learning_rates\": learning_rates,\n",
        "    \"best_lrs\": {k: v[0] for k, v in best.items()},  # if best[name] = (best_lr, best_loss)\n",
        "    \"random_seed\": seed_seed,\n",
        "}\n",
        "\n",
        "save_run_results(run_results, f\"run_results_seed{seed_seed}.json\", meta=meta)\n",
        "# run_results2, meta2 = load_run_results(f\"run_results_seed{seed_seed}.json\")\n",
        "# print(meta2[\"best_lrs\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# optimizers = {\n",
        "#     \"Muon-MVR2\": MuonMVR2Optimizer,\n",
        "#     \"SGD\": SGDOptimizer,\n",
        "#     \"Adam\": AdamOptimizer,\n",
        "#     \"AdaOrth\": AdaOrthOptimizer,\n",
        "#     \"AdaOrthLMinus\": AdaOrthLMinusOptimizer,\n",
        "# }\n",
        "colors = {\n",
        "    \"Muon-MVR2\": \"#d62728\",\n",
        "    \"SGD\": \"#9467bd\",\n",
        "    \"ADAM\": \"#e377c2\",\n",
        "    \"AdaOrthL\": \"#bcbd22\",\n",
        "    \"AdaOrthL+\": \"#1f77b4\",\n",
        "}\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_loss_curves(run_results, epochs, colors, save_path=\"loss.png\"):\n",
        "    epochs_range = range(1, epochs + 1)\n",
        "    plt.figure(figsize=(7, 4.5))\n",
        "\n",
        "    for name, res in run_results.items():\n",
        "        c = colors.get(name, None)\n",
        "        plt.plot(epochs_range, res[\"train_losses\"], label=f\"{name} (Train)\", linewidth=2, color=c)\n",
        "        plt.plot(epochs_range, res[\"test_losses\"],  label=f\"{name} (Val/Test)\", linewidth=2, linestyle=\"--\", color=c)\n",
        "\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training/Validation Loss Comparison\")\n",
        "    plt.legend(fontsize=10, frameon=True, ncol=2)\n",
        "    plt.xlim(1, epochs)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300)\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def plot_acc_curves(run_results, epochs, colors, save_path=f\"acc.png\"):\n",
        "    epochs_range = range(1, epochs + 1)\n",
        "    plt.figure(figsize=(7, 4.5))\n",
        "\n",
        "    for name, res in run_results.items():\n",
        "        c = colors.get(name, None)\n",
        "        plt.plot(epochs_range, res[\"train_accs\"], label=f\"{name} (Train)\", linewidth=2, color=c)\n",
        "        plt.plot(epochs_range, res[\"test_accs\"],  label=f\"{name} (Val/Test)\", linewidth=2, linestyle=\"--\", color=c)\n",
        "\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy (%)\")\n",
        "    plt.title(\"Training/Validation Accuracy Comparison\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend(fontsize=10, frameon=True, ncol=2)\n",
        "    plt.xlim(1, epochs)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300)\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def plot_acc_vs_time(run_results, colors, save_path=\"acc_time.png\"):\n",
        "    \"\"\"\n",
        "    Requires res[\"time_curve\"] as list[float] length=epochs.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(7, 4.5))\n",
        "\n",
        "    for name, res in run_results.items():\n",
        "        c = colors.get(name, None)\n",
        "        if isinstance(res.get(\"time_curve\", None), (list, tuple)) and len(res[\"time_curve\"]) == len(res[\"test_accs\"]):\n",
        "            plt.plot(res[\"time_curve\"], res[\"test_accs\"], label=name, linewidth=2, color=c)\n",
        "        else:\n",
        "            # fallback: total time -> single point (final acc)\n",
        "            t = float(res[\"time_cost\"])\n",
        "            a = float(res[\"test_accs\"][-1])\n",
        "            plt.scatter([t], [a], label=name, color=c)\n",
        "\n",
        "    plt.xlabel(\"Wall Clock Time (s)\")\n",
        "    plt.ylabel(\"Validation/Test Accuracy (%)\")\n",
        "    plt.title(\"Validation Accuracy vs Time\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300)\n",
        "    plt.show()\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_loss_curves(run_results, epochs=train_object.epochs, colors=colors, save_path=f\"loss_{seed_seed}.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_acc_curves(run_results, epochs=train_object.epochs, colors=colors, save_path=f\"acc_{seed_seed}.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Accuracy with Time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_acc_vs_time(run_results, colors=colors, save_path=f\"acc_time_{seed_seed}.png\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "name": "ResNet Implementation on CIFAR10.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "312",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
