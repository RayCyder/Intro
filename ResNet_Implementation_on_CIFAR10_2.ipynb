{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RayCyder/Intro/blob/main/ResNet_Implementation_on_CIFAR10_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6ikW2JiYwp0"
      },
      "source": [
        "##The Resnet Research paper can be accessed from here https://arxiv.org/pdf/1512.03385v1.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Axosx_BqhOzn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.manual_seed(4242)\n",
        "torch.cuda.manual_seed_all(4242)\n",
        "import numpy as np\n",
        "np.random.seed(4242)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVwz1WydnN_7"
      },
      "source": [
        "#**Downloading the CIFAR10 datset and loading the data in Normalized form as torch.FloatTensor datatype and generating a validation set by dividing the training set in 80-20 ratio**\n",
        "#**CIFAR10**\n",
        "The CIFAR10 and CIFAR-100 are labeled subsets of the 80 million tiny images dataset. They were collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.\n",
        "\n",
        "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
        "\n",
        "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\n",
        "\n",
        "Here are the classes in the dataset:\n",
        "1. airplane\n",
        "2. automobile\n",
        "3. bird\n",
        "4. cat\n",
        "5. deer\n",
        "6. dog\n",
        "7. frog\n",
        "8. horse\n",
        "9. ship\n",
        "10. truck\n",
        "\n",
        "The classes are completely mutually exclusive. There is no overlap between automobiles and trucks. \"Automobile\" includes sedans, SUVs, things of that sort. \"Truck\" includes only big trucks. Neither includes pickup trucks.\n",
        "\n",
        "More can be read from their page at https://www.cs.toronto.edu/~kriz/cifar.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U2Vn2qhvrIi"
      },
      "source": [
        "#**Image Augmentation**\n",
        "In this cell, we perform some simple data augmentation by randomly flipping and cropping the given image data. We do this by defining a torchvision transform, and you can learn about all the transforms that are used to pre-process and augment data from the [PyTorch documentation](https://pytorch.org/docs/stable/torchvision/transforms.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5WmV1je_1kr",
        "outputId": "13ac6e71-a56f-448b-c20b-025e9e07720a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import Subset, random_split, DataLoader\n",
        "transform = transforms.Compose([\n",
        "    # transforms.Resize(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
        "])\n",
        "batch_size = 128\n",
        "# --- Load full CIFAR-10 train set ---\n",
        "trainset_full = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "\n",
        "# --- (Optional) use only a subset for speed\n",
        "subset_indices = []\n",
        "targets = np.array(trainset_full.targets)\n",
        "for c in np.unique(targets):\n",
        "  class_idx = np.where(targets == c)[0]\n",
        "  np.random.shuffle(class_idx)\n",
        "  subset_indices.extend(class_idx[:500])\n",
        "\n",
        "# Convert subset_indices to a numpy array to allow advanced indexing\n",
        "subset_indices = np.array(subset_indices)\n",
        "\n",
        "#train_subset = Subset(trainset_full, subset_indices)\n",
        "\n",
        "# --- Split subsert into train/validation ---\n",
        "train_indices = []\n",
        "val_indices = []\n",
        "val_ratio = 0.1\n",
        "subset_labels = targets[subset_indices]\n",
        "for c in np.unique(subset_labels):\n",
        "  class_idx = np.where(subset_labels == c)[0]\n",
        "  np.random.shuffle(class_idx)\n",
        "  n_val = int(len(class_idx)*val_ratio)\n",
        "  val_indices.extend(subset_indices[class_idx[:n_val]])\n",
        "  train_indices.extend(subset_indices[class_idx[n_val:]])\n",
        "#train_set,val_set = random_split(train_subset,[n_train,n_val])\n",
        "train_set = Subset(trainset_full,train_indices)\n",
        "val_set  = Subset(trainset_full, val_indices)\n",
        "# --- DataLoaders ---\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_set,batch_size = batch_size,shuffle=False)\n",
        "# --- Test Set ---\n",
        "testset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
        "# --- Class names ---\n",
        "classes = trainset_full.classes\n",
        "# classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "#            'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "print(classes)\n",
        "print(f\"Batch size:{batch_size}, \\\n",
        "      Train batches:{len(train_loader)}, \\\n",
        "      Val batches:{len(val_loader)},\\\n",
        "      Test batches:{len(test_loader)}\")\n",
        "# check train and val set is balanced or not\n",
        "from collections import Counter\n",
        "train_labels = [classes[x] for x in targets[train_indices]]\n",
        "val_labels = [classes[x] for x in targets[val_indices]]\n",
        "print(\"Train class counts:\",Counter(train_labels))\n",
        "print(\"Val class counts:\", Counter(val_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdIarqe4tnBs"
      },
      "source": [
        "#**Visualizing the Data**\n",
        "Obtaining a batch of training data and plot the same with its lables using matplotlib library. You can also see how the transformations which you applied in the previous step show up in these visualizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "id": "xhhpI2ntAB8f",
        "outputId": "d187e479-f455-46ae-bdac-b0af3c212f91"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "print(classes)\n",
        "%matplotlib inline\n",
        "\n",
        "# helper function to un-normalize and display an image\n",
        "def imshow(img):\n",
        "  img = img / 2 + 0.5  # unnormalize\n",
        "  plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image\n",
        "\n",
        "# obtain one batch of training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "images = images.numpy() # convert images to numpy for display\n",
        "print(images[0].shape)\n",
        "# plot the images in the batch, along with the corresponding labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "# display 20 images\n",
        "for idx in np.arange(20):\n",
        "  ax = fig.add_subplot(2, 20//2, idx+1, xticks=[], yticks=[])\n",
        "  imshow(images[idx])\n",
        "  ax.set_title(classes[labels[idx]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQBRSenSvD7P"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkqQ7fsUxA0r"
      },
      "source": [
        "#**Defining the Network Architecture**\n",
        "In this section the entire Research Paper is implemented to define the Residual Network approach taken by the researchers\n",
        "\n",
        "NOTE:\n",
        "\n",
        "Output volume for a convolutional layer\n",
        "To compute the output size of a given convolutional layer we can perform the following calculation (taken from Stanford's cs231n course):\n",
        "\n",
        "We can compute the spatial size of the output volume as a function of the input volume size (W), the kernel/filter size (F), the stride with which they are applied (S), and the amount of zero padding used (P) on the border. The correct formula for calculating how many neurons define the output_W is given by (W−F+2P)/S+1.\n",
        "\n",
        "For example for a 7x7 input and a 3x3 filter with stride 1 and pad 0 we would get a 5x5 output. With stride 2 we would get a 3x3 output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEc93C7xAXEE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "# choose device\n",
        "device = (\n",
        "    \"cuda\" if torch.cuda.is_available()\n",
        "    else \"mps\" if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using device: {device}\")\n",
        "def initialize_model(device):\n",
        "  # Load a pre-defined ResNet18\n",
        "  ResNet18 = models.resnet18(weights=None)   # train from scratch\n",
        "  # or use pretrained weights:\n",
        "  # model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "\n",
        "  # Adjust the final layer for your dataset (e.g. 10 classes for CIFAR-10)\n",
        "  num_ftrs = ResNet18.fc.in_features\n",
        "  ResNet18.fc = nn.Linear(num_ftrs, 10)\n",
        "  ResNet18 = ResNet18.to(device)\n",
        "  if device == \"cuda\":\n",
        "    import torch.backends.cudnn as cudnn\n",
        "    ResNet18 = torch.nn.DataParallel(ResNet18)\n",
        "    cudnn.benchmark = True\n",
        "  return ResNet18\n",
        "resnet18 = initialize_model(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDAlhqzyyYaG"
      },
      "source": [
        "#**Specifying the Loss Function and Optimizer**\n",
        "We use CrossEntropyLoss as Loss function and\n",
        "\n",
        "[Stochastic Gradient Descent](https://leon.bottou.org/publications/pdf/compstat-2010.pdf) as Optimizer with momentum and weight decay specified by the research paper of ResNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlkXUNc-aVV9"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "# specify loss function (categorical cross-entropy)\n",
        "# specify optimizer\n",
        "# optimizer = optim.SGD(ResNet18.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0001)\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEAqv4_vfqlD"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Muon-MVR For LLM Pretraining.\n",
        "'''\n",
        "import torch\n",
        "import math\n",
        "\n",
        "#fix for mps\n",
        "def _compile_for_device(fn):\n",
        "    # Inductor doesn't support MPS; use AOTAutograd there.\n",
        "    if torch.backends.mps.is_available():\n",
        "        return torch.compile(fn, backend=\"aot_eager\", dynamic=True)\n",
        "    # Inductor is fine on CUDA/CPU\n",
        "    return torch.compile(fn, backend=\"inductor\", dynamic=True)\n",
        "\n",
        "\n",
        "@_compile_for_device\n",
        "def zeropower_via_newtonschulz5(G, steps=3, eps=1e-7):\n",
        "    \"\"\"\n",
        "    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G.\n",
        "    \"\"\"\n",
        "    assert len(G.shape) == 2\n",
        "    a, b, c = (3.4445, -4.7750,  2.0315)\n",
        "    X = G.bfloat16()\n",
        "    #x_flat = X.reshape(-1)      # 把矩阵展平成一个 1D 向量\n",
        "    X /= (X.norm() + eps) # ensure top singular value <= 1\n",
        "    if G.size(0) > G.size(1):\n",
        "        X = X.T\n",
        "    for _ in range(steps):\n",
        "        A = X @ X.T\n",
        "        B = b * A + c * A @ A\n",
        "        X = a * X + B @ X\n",
        "    if G.size(0) > G.size(1):\n",
        "        X = X.T\n",
        "    return X\n",
        "\n",
        "class MuonMVR(torch.optim.Optimizer):\n",
        "    r'''\n",
        "    Standard MVR:\\\\(nabla f(X_t;\\xi_t) - \\\\nabla f(X_{t-1};\\xi_{t})\n",
        "    Approximate MVR:\n",
        "        1.\\\\(nabla f(X_t;\\xi_t) - \\\\nabla f(X_{t-1};\\xi_{t-1})\n",
        "        2.\\\\(nabla f(X_t;\\xi_t) - \\\\nabla f(X_{t};\\xi_{t-1}), It has low computational complexity\n",
        "         and is more convenient in practice\n",
        "    '''\n",
        "    def __init__(self, params, lr=3e-3, momentum = 0.95 ,adamw_betas=(0.95, 0.99), eps=1e-8,\n",
        "                 weight_decay=0.0, gamma=0.025, is_approx=False):\n",
        "        if lr < 0.0:\n",
        "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
        "        if not 0.0 <= momentum < 1.0:\n",
        "            raise ValueError(f\"Invalid momentum parameter: {momentum}\")\n",
        "        if not 0.0 <= adamw_betas[0] < 1.0:\n",
        "            raise ValueError(f\"Invalid beta1 parameter: {adamw_betas[0]}\")\n",
        "        if not 0.0 <= adamw_betas[1] < 1.0:\n",
        "            raise ValueError(f\"Invalid beta2 parameter: {adamw_betas[1]}\")\n",
        "\n",
        "        defaults = dict(lr=lr, momentum=momentum,adamw_betas=adamw_betas, eps=eps,\n",
        "                       weight_decay=weight_decay, gamma=gamma)\n",
        "        super().__init__(params, defaults)\n",
        "        self.is_approx = is_approx\n",
        "\n",
        "    def adjust_lr_for_muon(self, lr, param_shape):\n",
        "        A, B = param_shape[:2]\n",
        "        # We adjust the learning rate and weight decay based on the size of the parameter matrix\n",
        "        # as describted in the paper\n",
        "        adjusted_ratio = 0.2 * math.sqrt(max(A, B))\n",
        "        # adjusted_ratio = math.sqrt(A*B)\n",
        "        adjusted_lr = lr * adjusted_ratio\n",
        "        return adjusted_lr\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update_last_grad(self):\n",
        "        if not self.is_approx:\n",
        "            for group in self.param_groups:\n",
        "                for p in group['params']:\n",
        "                    state = self.state[p]\n",
        "                    if \"last_grad\" not in state:\n",
        "                        state[\"last_grad\"] = torch.zeros_like(p)\n",
        "                    state[\"last_grad\"].zero_().add_(p.grad, alpha=1.0)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self):\n",
        "        for group in self.param_groups:\n",
        "            lr = group['lr']\n",
        "            beta1, beta2 = group['adamw_betas']\n",
        "            eps = group['eps']\n",
        "            weight_decay = group['weight_decay']\n",
        "            momentum = group['momentum']\n",
        "            gamma = group['gamma']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                grad = p.grad\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p)\n",
        "                    state['last_grad'] = torch.zeros_like(p)\n",
        "                    # state['previous_grad'] = torch.zeros_like(p)\n",
        "                    if len(p.shape) != 2:  # Only for 2D tensors\n",
        "                        state['exp_avg_sq'] = torch.zeros_like(p)\n",
        "\n",
        "                state['step'] += 1\n",
        "                last_grad = state['last_grad']\n",
        "                if len(p.shape) == 2:\n",
        "                    exp_avg = state['exp_avg']\n",
        "\n",
        "                    # Compute momentum-like term with correction\n",
        "                    c_t = (grad - last_grad).mul(gamma * (momentum / (1. - momentum))).add(grad)\n",
        "                    c_t_norm = torch.norm(c_t)\n",
        "                    if c_t_norm > 1.:\n",
        "                        c_t = c_t / c_t_norm\n",
        "                    # Update moving averages\n",
        "                    exp_avg.mul_(momentum).add_(c_t, alpha=1 - momentum)\n",
        "                    update = zeropower_via_newtonschulz5(exp_avg.mul(1./(1.- momentum))) # whiten the update\n",
        "                    adjusted_lr = self.adjust_lr_for_muon(lr, p.shape)\n",
        "                    p.data.mul_(1 - lr * weight_decay)\n",
        "                    p.data.add_(update, alpha=-adjusted_lr)\n",
        "                else:\n",
        "                    # For bias vectors - use simple update\n",
        "                    step = state['step']\n",
        "                    # Compute momentum-like term with correction\n",
        "                    c_t = (grad - last_grad).mul(gamma * (beta1 / (1. - beta1))).add(grad)\n",
        "                    c_t_norm = torch.norm(c_t)\n",
        "                    # avoid inductor lowering bug: compute norm explicitly and detach\n",
        "                    # c_t_norm = torch.sqrt(torch.sum((c_t.detach() * c_t.detach()), dim=None))\n",
        "                    if c_t_norm > 1.:\n",
        "                        c_t = c_t / c_t_norm\n",
        "                    exp_avg = state['exp_avg']\n",
        "                    exp_avg_sq = state['exp_avg_sq']\n",
        "                    exp_avg.lerp_(c_t, 1 - beta1)\n",
        "                    exp_avg_sq.lerp_(c_t.square(), 1 - beta2)\n",
        "                    g = exp_avg / (eps + exp_avg_sq.sqrt())\n",
        "                    bias_correction1 = 1 - beta1**step\n",
        "                    bias_correction2 = 1 - beta2**step\n",
        "                    scale = bias_correction1 / bias_correction2**0.5\n",
        "                    p.data.mul_(1 - lr * weight_decay)\n",
        "                    p.data.add_(g, alpha=-lr / scale)\n",
        "\n",
        "                if self.is_approx:\n",
        "                    state['last_grad'].copy_(grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dMWiYJf1Ou-"
      },
      "source": [
        "#**MuonMVR Training Loop**\n",
        "Here we train the architecture on training data and check its validation loss by using the validation set and saving the model only if there is an improvement ie decrease in the validation loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bFAzI_3Vwhj"
      },
      "source": [
        "## MVR2 Usage\n",
        "3.  **Optimizer Modes**\n",
        "    MuonMVR can be initialized in different modes to trade off between precision and computational cost.\n",
        "\n",
        "    **Exact Variance Reduction (`is_approx=False`)**\n",
        "    To achieve the most precise variance reduction, you must manually manage the model state. Before calculating the gradient for the previous batch, you need to load the model state from the previous iteration. This ensures that the gradient is computed with the correct model weights.\n",
        "    ```python\n",
        "    optimizer = MuonMVR(model.parameters(), lr=1e-3, is_approx=False)\n",
        "    old_state_dict = {}\n",
        "    \n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        # Store the current model state\n",
        "        cur_state_dict = {k: v.data.clone() for k, v in net.state_dict().items()}\n",
        "    \n",
        "        if old_state_dict:\n",
        "            # Load the previous model state to compute the old gradient\n",
        "            net.load_state_dict(old_state_dict)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.update_last_grad()\n",
        "    \n",
        "        # Restore the current model state to compute the new gradient\n",
        "        net.load_state_dict(cur_state_dict)\n",
        "        old_state_dict = {k: v.data.clone() for k, v in cur_state_dict.items()}\n",
        "        \n",
        "        # Standard forward/backward pass and step\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiicKSbbint0"
      },
      "outputs": [],
      "source": [
        "class BaseOptimizer(object):\n",
        "  def __init__(self,model,criterion,epochs):\n",
        "    self.model = model\n",
        "    self.epochs = epochs\n",
        "    self.criterion = criterion\n",
        "    self.optimizer,self.scheduler = None, None\n",
        "  def initialize_optimzer(self):\n",
        "    raise Exception(\"non implemetation\")\n",
        "  def zero_grad(self,inputs=None,labels=None):\n",
        "    self.optimizer.zero_grad()\n",
        "  def step(self):\n",
        "    self.optimizer.step()\n",
        "  def epoch_step(self):\n",
        "    if self.scheduler:\n",
        "      self.scheduler.step()\n",
        "\n",
        "class MuonMVR2Optimizer(BaseOptimizer):\n",
        "  def __init__(self,model,criterion,epochs):\n",
        "    super().__init__(model,criterion,epochs)\n",
        "    self.old_state_dict = {}\n",
        "  def initialize_optimizer(self, learning_rate):\n",
        "    optimizer = MuonMVR(self.model.parameters(), lr=learning_rate,weight_decay=0.01,gamma=0.1,is_approx=False)\n",
        "    # Cosine annealing: lr → eta_min by the end\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer,\n",
        "        T_max=self.epochs,   # period (usually = total epochs)\n",
        "        eta_min=0.1*learning_rate      # minimum LR (here 0.1× of 0.01)\n",
        "    )\n",
        "    self.optimizer,self.scheduler =  optimizer, scheduler\n",
        "  def zero_grad(self,inputs=None,labels=None):\n",
        "    model, old_state_dict = self.model, self.old_state_dict\n",
        "    optimizer = self.optimizer\n",
        "    # Store the current model state\n",
        "    cur_state_dict = {k: v.data.clone() for k, v in model.state_dict().items()}\n",
        "    if self.old_state_dict:\n",
        "        # Load the previous model state to compute the old gradient\n",
        "        model.load_state_dict(old_state_dict)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.update_last_grad()\n",
        "    #restore state\n",
        "    model.load_state_dict(cur_state_dict)\n",
        "    self.old_state_dict = {k: v.data.clone() for k, v in cur_state_dict.items()}\n",
        "    self.optimizer.zero_grad()\n",
        "def val_once(model, criterion, val_loader):\n",
        "  model.eval()\n",
        "  val_loss = 0.0\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in val_loader:\n",
        "      inputs, labels = inputs.to(device),labels.to(device)\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs,labels)\n",
        "      val_loss += loss.item()\n",
        "  val_loss = val_loss/max(1,len(val_loader))\n",
        "  return val_loss\n",
        "def train_once(model,criterion,optimizer,train_loader,val_loader,learning_rate,epochs=100):\n",
        "  losses = []\n",
        "  val_losses = []\n",
        "  #optimizer = MuonMVR2Optimizer(model,criterion,epochs)\n",
        "  #optimizer.initialize_optimizer(learning_rate)\n",
        "  for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad(inputs, labels)\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss+=loss.item()\n",
        "    # step the scheduler once per epoch\n",
        "    optimizer.epoch_step()\n",
        "    losses.append(running_loss/max(1, len(train_loader)))\n",
        "    current_lr = optimizer.scheduler.get_last_lr()[0]  # single param group\n",
        "    val_loss = val_once(model,criterion,val_loader)\n",
        "    val_losses.append(val_loss)\n",
        "    print(f\"[{epoch+1}] loss: {losses[-1]:.3f} val_loss:{val_loss} current_lr={current_lr:.6f}\")\n",
        "\n",
        "  print(\"Training finished!\")\n",
        "  return losses, val_losses\n",
        "\n",
        "\n",
        "def train_with_mvr2_once(model,criterion,train_loader,val_loader,learning_rate,epochs):\n",
        "  optimizer = MuonMVR2Optimizer(model,criterion,epochs)\n",
        "  optimizer.initialize_optimizer(learning_rate)\n",
        "  return train_once(model,criterion,optimizer,train_loader,val_loader,learning_rate,epochs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvhesdndign7"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGej2WdOVHEs"
      },
      "source": [
        "##**grid search**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HvRH5HvVE_X",
        "outputId": "4cec4db1-e77b-4c91-c2f9-44a9ad5358dd"
      },
      "outputs": [],
      "source": [
        "learning_rates = [1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2, 1e-1]\n",
        "result = [1.662683129310608, 1.5963122844696045, 1.4677045345306396,\n",
        "           1.3441717624664307, 1.3620641231536865, 1.5036698579788208, 1.4877097606658936]\n",
        "epochs = 30\n",
        "def grids_search_val(get_optimizer):\n",
        "  result = []\n",
        "  for learning_rate in learning_rates:\n",
        "    model = initialize_model(device)\n",
        "    optimizer = get_optimizer(model, learning_rate)\n",
        "    train_losses, val_losses = train_once(model,criterion,optimizer,train_loader,val_loader,learning_rate,epochs)\n",
        "    result.append(min(val_losses))\n",
        "  return result\n",
        "def muonmvr2_optimizer(model,learning_rate):\n",
        "  optimizer = MuonMVR2Optimizer(model,criterion,epochs)\n",
        "  optimizer.initialize_optimizer(learning_rate)\n",
        "  return optimizer\n",
        "if not result:\n",
        "  result = grids_search_val(muonmvr2_optimizer)\n",
        "  result = [x.cpu() for x in result]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "encBhZ6_2tN4",
        "outputId": "a68916c2-74f2-476e-c4cd-08966809e1ac"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "plt.xscale('log')   # good practice for learning rates\n",
        "plt.xlabel(\"Learning rate\")\n",
        "plt.ylabel(\"Min Validation Loss\")\n",
        "print(result)\n",
        "plt.title(\"Learning Rate Grid Search\")\n",
        "plt.plot(learning_rates, result, label=\"Grid Search(Validataion Loss)\", linewidth=2, marker='o')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1M4p3Is2RMg"
      },
      "source": [
        "#**Clean Model**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "min_index = result.index(min(result))\n",
        "learning_rate = learning_rates[min_index]\n",
        "print(f\"Best learning rate: {learning_rate}, Min val loss: {result[min_index]}\")\n",
        "print([float(x) for x in result])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import torch\n",
        "# torch._dynamo.config.verbose = True\n",
        "# import os\n",
        "# os.environ[\"TORCH_LOGS\"] = \"recompiles,graph_breaks,guards\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = initialize_model(device)\n",
        "losses_mvr2,losses_mvr2_val = train_with_mvr2_once(model,criterion,train_loader,val_loader,learning_rate,epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBC_rC7Lj5L3"
      },
      "source": [
        "#**SGD Training Loop**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7HXCXm0kJ_3"
      },
      "outputs": [],
      "source": [
        "#optimizer = optim.SGD(ResNet18.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "class SGDOptimizer(BaseOptimizer):\n",
        "  def __init__(self,model,criterion,epochs):\n",
        "    super().__init__(model,criterion,epochs)\n",
        "  def initialize_optimizer(self, learning_rate):\n",
        "    optimizer = optim.SGD(self.model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "    # Cosine annealing: lr → eta_min by the end\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer,\n",
        "        T_max=self.epochs,   # period (usually = total epochs)\n",
        "        eta_min=0.1*learning_rate      # minimum LR (here 0.1× of 0.01)\n",
        "    )\n",
        "    self.optimizer,self.scheduler =  optimizer, scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_sgd(model, learning_rate):\n",
        "    optimizer = SGDOptimizer(model, criterion, epochs)\n",
        "    optimizer.initialize_optimizer(learning_rate)\n",
        "    return optimizer\n",
        "result_sgd = [2.1374309062957764, 1.7963900566101074, 1.6781493425369263, \n",
        "              1.566157341003418, 1.650704264640808, 1.8818279504776, 1.6078990697860718]\n",
        "if not result_sgd:\n",
        "    result_sgd = grids_search_val(get_sgd)\n",
        "    result_sgd = [float(x.cpu()) for x in result_sgd]\n",
        "print(result_sgd)\n",
        "min_index = result_sgd.index(min(result_sgd))\n",
        "learning_rate = learning_rates[min_index]\n",
        "print(f\"Best learning rate: {learning_rate}, Min val loss: {result_sgd[min_index]}\")\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.xscale('log')   # good practice for learning rates\n",
        "plt.xlabel(\"Learning rate\")\n",
        "plt.ylabel(\"Min Validation Loss\")\n",
        "plt.title(\"Learning Rate Grid Search\")\n",
        "plt.plot(learning_rates, result_sgd, label=\"Grid Search(Validataion Loss)\", linewidth=2, marker='o')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLI6pPDhg1VO",
        "outputId": "9e6f45d6-f63e-4d4b-e0bf-2fa1bcd452b1"
      },
      "outputs": [],
      "source": [
        "ResNet18 = initialize_model(device)\n",
        "optimizer = SGDOptimizer(ResNet18,criterion,epochs)\n",
        "optimizer.initialize_optimizer(learning_rate)\n",
        "losses_sgd,losses_sgd_val = train_once(ResNet18,criterion,optimizer,train_loader,val_loader,learning_rate,epochs)\n",
        "print(\"Training finished!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AdamWOptimizer(BaseOptimizer):\n",
        "  def __init__(self,model,criterion,epochs):\n",
        "    super().__init__(model,criterion,epochs)\n",
        "  def initialize_optimizer(self, learning_rate):\n",
        "    optimizer = optim.AdamW(self.model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "    # Cosine annealing: lr → eta_min by the end\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer,\n",
        "        T_max=self.epochs,   # period (usually = total epochs)\n",
        "        eta_min=0.1*learning_rate      # minimum LR (here 0.1× of 0.01)\n",
        "    )\n",
        "    self.optimizer,self.scheduler =  optimizer, scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_adam(model, learning_rate):\n",
        "    optimizer = SGDOptimizer(model, criterion, epochs)\n",
        "    optimizer.initialize_optimizer(learning_rate)\n",
        "    return optimizer\n",
        "result_adam = [2.1293160915374756, 1.8071961402893066, 1.66524338722229, \n",
        "               1.6158260107040405, 1.6555125713348389, 1.9691908359527588, 1.6998178958892822]\n",
        "if not result_adam:\n",
        "    result_adam = grids_search_val(get_adam)\n",
        "    result_adam = [float(x.cpu()) for x in result_adam]\n",
        "print(result_adam)\n",
        "min_index = result_adam.index(min(result_adam))\n",
        "learning_rate = learning_rates[min_index]\n",
        "print(f\"Best learning rate: {learning_rate}, Min val loss: {result_adam[min_index]}\")\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.xscale('log')   # good practice for learning rates\n",
        "plt.xlabel(\"Learning rate\")\n",
        "plt.ylabel(\"Min Validation Loss\")\n",
        "plt.title(\"Learning Rate Grid Search\")\n",
        "plt.plot(learning_rates, result_adam, label=\"Grid Search(Validataion Loss)\", linewidth=2, marker='o')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ResNet18_adamw = initialize_model(device)\n",
        "optimizer = AdamWOptimizer(ResNet18_adamw,criterion,epochs)\n",
        "optimizer.initialize_optimizer(learning_rate)\n",
        "losses_adamw,losses_adamw_val = train_once(ResNet18_adamw,criterion,optimizer,train_loader,val_loader,learning_rate,epochs)\n",
        "print(\"Training finished!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(losses_mvr2, label=\"Muon-MVR2\", linewidth=2)\n",
        "plt.plot(losses_sgd, label=\"SGD\", linewidth=2)\n",
        "plt.plot(losses_adamw, label=\"AdamW\", linewidth=2)\n",
        "plt.plot(losses_mvr2_val, label=\"Muon-MVR2(Val)\", linewidth=2, linestyle='--')\n",
        "plt.plot(losses_sgd_val, label=\"SGD(Val)\", linewidth=2, linestyle='--')\n",
        "plt.plot(losses_adamw_val, label=\"AdamW(Val)\", linewidth=2, linestyle='--')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Training Loss\")\n",
        "plt.title(\"Training Loss Comparison\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWr3orBMiXrv"
      },
      "source": [
        "#**show losses for two optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "angPoGSTeykj",
        "outputId": "6fb5128f-3db6-43d7-9a0f-0f1f3f095d60"
      },
      "outputs": [],
      "source": [
        "# epochs_list = range(1,epochs+1)\n",
        "# plt.figure(figsize=(8, 5))  # larger, readable figure\n",
        "\n",
        "# # plot both losses\n",
        "# plt.plot(epochs_list, losses_mvr2, label=\"MuonMVR Loss\", linewidth=2)\n",
        "# plt.plot(epochs_list, losses_sgd, label=\"SGD Loss\", linewidth=2, linestyle=\"--\")\n",
        "\n",
        "# # formatting\n",
        "# plt.xlabel(\"Epoch\", fontsize=12)\n",
        "# plt.ylabel(\"Loss\", fontsize=12)\n",
        "# plt.title(\"Training Loss Comparison with CIFAR10\", fontsize=14)\n",
        "# plt.legend()\n",
        "# plt.grid(True, alpha=0.3)\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_lEna0p2Wp5"
      },
      "source": [
        "#**Testing Loop**\n",
        "The real test of the model architecture how well does the model recognizes the image and what is the accuracy on the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsqLF8SVfsnn"
      },
      "outputs": [],
      "source": [
        "# ResNet18.eval()\n",
        "# correct, total = 0, 0\n",
        "# with torch.no_grad():\n",
        "#     for inputs, labels in test_loader:\n",
        "#         inputs, labels = inputs.to(device), labels.to(device)\n",
        "#         outputs = ResNet18(inputs)\n",
        "#         _, predicted = torch.max(outputs.data, 1)\n",
        "#         total += labels.size(0)\n",
        "#         correct += (predicted == labels).sum().item()\n",
        "\n",
        "# print(f\"Accuracy: {100 * correct / total:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# track test loss\n",
        "test_loss = 0.0\n",
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "ResNet18.eval()\n",
        "# iterate over test data\n",
        "for batch_idx, (data, target) in enumerate(test_loader):\n",
        "  # move tensors to GPU if CUDA is available\n",
        "  if train_on_gpu:\n",
        "    data, target = data.cuda(), target.cuda()\n",
        "  # forward pass: compute predicted outputs by passing inputs to the model\n",
        "  output = ResNet18(data)\n",
        "  # calculate the batch loss\n",
        "  loss = criterion(output, target)\n",
        "  # update test loss\n",
        "  test_loss += loss.item()*data.size(0)\n",
        "  # convert output probabilities to predicted class\n",
        "  _, pred = torch.max(output, 1)\n",
        "  # compare predictions to true label\n",
        "  correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "  correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "  # calculate test accuracy for each object class\n",
        "  for i in range(batch_size):\n",
        "    label = target.data[i]\n",
        "    class_correct[label] += correct[i].item()\n",
        "    class_total[label] += 1\n",
        "\n",
        "# average test loss\n",
        "test_loss = test_loss/len(test_loader.dataset)\n",
        "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "for i in range(10):\n",
        "  if class_total[i] > 0:\n",
        "    print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "        classes[i], 100 * class_correct[i] / class_total[i],\n",
        "        np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "  else:\n",
        "    print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "\n",
        "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\n",
        "    np.sum(class_correct), np.sum(class_total)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "name": "ResNet Implementation on CIFAR10.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
