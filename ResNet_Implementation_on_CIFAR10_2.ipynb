{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RayCyder/Intro/blob/main/ResNet_Implementation_on_CIFAR10_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6ikW2JiYwp0"
      },
      "source": [
        "##The Resnet Research paper can be accessed from here https://arxiv.org/pdf/1512.03385v1.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''ResNet in PyTorch.\n",
        "\n",
        "For Pre-activation ResNet, see 'preact_resnet.py'.\n",
        "\n",
        "Reference:\n",
        "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
        "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
        "\n",
        "\n",
        "def test():\n",
        "    net = ResNet18()\n",
        "    y = net(torch.randn(1, 3, 32, 32))\n",
        "    print(y.size())\n",
        "\n",
        "# test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "seed_seed =777 #555 #4242 42\n",
        "random.seed(seed_seed)\n",
        "torch.manual_seed(seed_seed)\n",
        "torch.cuda.manual_seed_all(seed_seed)\n",
        "np.random.seed(seed_seed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "# choose device\n",
        "device = (\n",
        "    \"cuda\" if torch.cuda.is_available()\n",
        "    else \"mps\" if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using device: {device}\")\n",
        "if device == \"cuda\":\n",
        "    import torch.backends.cudnn as cudnn\n",
        "    cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision import datasets\n",
        "trainset_full = datasets.CIFAR10(root=\"./data\", train=True, download=True)\n",
        "classes = trainset_full.classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "class BaseOptimizer(object):\n",
        "  def __init__(self,model,criterion,epochs):\n",
        "    self.model = model\n",
        "    self.epochs = epochs\n",
        "    self.criterion = criterion\n",
        "    self.optimizer,self.scheduler = None, None\n",
        "  def initialize_optimzer(self):\n",
        "    raise Exception(\"non implemetation\")\n",
        "  def zero_grad(self,inputs=None,labels=None):\n",
        "    self.optimizer.zero_grad()\n",
        "  def step(self):\n",
        "    self.optimizer.step()\n",
        "  def epoch_step(self):\n",
        "    if self.scheduler:\n",
        "      self.scheduler.step()\n",
        "@torch.no_grad()\n",
        "def val_once(model, criterion, val_loader):\n",
        "  model.eval()\n",
        "  val_loss = 0.0\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in val_loader:\n",
        "      inputs, labels = inputs.to(device),labels.to(device)\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs,labels)\n",
        "      val_loss += loss.item()\n",
        "  val_loss = val_loss/max(1,len(val_loader))\n",
        "  return val_loss\n",
        "def train_once(model,criterion,optimizer,train_loader,val_loader,learning_rate,epochs=100):\n",
        "  losses = []\n",
        "  val_losses = []\n",
        "  accs =[]\n",
        "  elapsed_all = []\n",
        "  #optimizer = MuonMVR2Optimizer(model,criterion,epochs)\n",
        "  #optimizer.initialize_optimizer(learning_rate)\n",
        "  start = time.time()\n",
        "  for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad(inputs, labels)\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss+=loss.item()\n",
        "    # step the scheduler once per epoch\n",
        "    optimizer.epoch_step()\n",
        "    losses.append(running_loss/max(1, len(train_loader)))\n",
        "    current_lr = optimizer.scheduler.get_last_lr()[0]  # single param group\n",
        "    #val_loss = val_once(model,criterion,val_loader)\n",
        "    val_loss, acc = test_model(model, criterion, val_loader)\n",
        "    val_losses.append(val_loss)\n",
        "    accs.append(acc)\n",
        "    elapsed = time.time() - start\n",
        "    elapsed_all.append(elapsed)\n",
        "    print(f\"[{epoch+1}] time: {elapsed} loss: {losses[-1]:.3f} val_loss:{val_loss} current_lr={current_lr:.6f}\")\n",
        "\n",
        "  print(f\"Training finished!\")\n",
        "  return losses, val_losses, accs, elapsed_all\n",
        "\n",
        "  # track test loss\n",
        "@torch.no_grad()\n",
        "def test_model(ResNet18, criterion, test_loader):\n",
        "  test_loss = 0.0\n",
        "  class_correct = [0 for _ in range(10)]\n",
        "  class_total   = [0 for _ in range(10)]\n",
        "\n",
        "  ResNet18.eval()\n",
        "  # iterate over test data\n",
        "  for batch_idx, (data, target) in enumerate(test_loader):\n",
        "    # move tensors to GPU if CUDA is available\n",
        "    data, target = data.to(device), target.to(device)\n",
        "    count = data.size(0)\n",
        "    # forward pass: compute predicted outputs by passing inputs to the model\n",
        "    output = ResNet18(data)\n",
        "    # calculate the batch loss\n",
        "    loss = criterion(output, target)\n",
        "    # update test loss\n",
        "    test_loss += loss.item()\n",
        "    # convert output probabilities to predicted class\n",
        "    _, pred = torch.max(output, 1)\n",
        "    # compare predictions to true label\n",
        "    correct_tensor = pred.eq(target)\n",
        "    # correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    # calculate test accuracy for each object class\n",
        "    for i in range(count):\n",
        "      label = target.data[i]\n",
        "      class_correct[label] += int(correct_tensor[i].item())\n",
        "      class_total[label] += 1\n",
        "\n",
        "  # average test loss\n",
        "  total_samples = sum(class_total)\n",
        "  test_loss = test_loss/max(1,  len(test_loader))\n",
        "  print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "  for i in range(10):\n",
        "    if class_total[i] > 0:\n",
        "      acc_i = 100 * class_correct[i] / class_total[i]\n",
        "      print('Test Accuracy of %5s: %2d%% (%d/%d)' % (\n",
        "          classes[i],acc_i ,\n",
        "          class_correct[i], class_total[i]))\n",
        "    else:\n",
        "      print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "  acc = 100. * np.sum(class_correct) / total_samples\n",
        "  print('\\nTest Accuracy (Overall): %2d%% (%d/%d)' % (\n",
        "      acc,\n",
        "      sum(class_correct), total_samples))\n",
        "  return test_loss, acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVwz1WydnN_7"
      },
      "source": [
        "#**Downloading the CIFAR10 datset and loading the data in Normalized form as torch.FloatTensor datatype and generating a validation set by dividing the training set in 80-20 ratio**\n",
        "# **CIFAR10**\n",
        "The CIFAR10 and CIFAR-100 are labeled subsets of the 80 million tiny images dataset. They were collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.\n",
        "\n",
        "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
        "\n",
        "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\n",
        "\n",
        "Here are the classes in the dataset:\n",
        "1. airplane\n",
        "2. automobile\n",
        "3. bird\n",
        "4. cat\n",
        "5. deer\n",
        "6. dog\n",
        "7. frog\n",
        "8. horse\n",
        "9. ship\n",
        "10. truck\n",
        "\n",
        "The classes are completely mutually exclusive. There is no overlap between automobiles and trucks. \"Automobile\" includes sedans, SUVs, things of that sort. \"Truck\" includes only big trucks. Neither includes pickup trucks.\n",
        "\n",
        "More can be read from their page at https://www.cs.toronto.edu/~kriz/cifar.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U2Vn2qhvrIi"
      },
      "source": [
        "#**download dataset to local data directory**\n",
        "\n",
        "#**Image Augmentation**\n",
        "In this cell, we perform some simple data augmentation by randomly flipping and cropping the given image data. We do this by defining a torchvision transform, and you can learn about all the transforms that are used to pre-process and augment data from the [PyTorch documentation](https://pytorch.org/docs/stable/torchvision/transforms.html)\n",
        "\n",
        "\n",
        "we split train data to two part one is about 0.1 part of train data used as val data ,the last 0.9 part is used as train data \n",
        "\n",
        "test data only used once to check model at last\n",
        "\n",
        "so we choose 1000 for each class from 50,000 train data set  and split it to train data(0.9) and val data(0.1)\n",
        "choose 100 for each class from 10,000 test data as test data; \n",
        "\n",
        "we use val data to choose best learning rate,and use test data for accuracy and loss validation to avoid data leaking caused  validation loss overfit(val loss become much higher than lowest val loss at the end of training and training loss is very low)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torchvision, torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = transforms.ToTensor()   # 只先转成 [0,1] tensor，不做 Normalize\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "loader = DataLoader(trainset, batch_size=100, shuffle=False, num_workers=4)\n",
        "\n",
        "mean = 0.0\n",
        "var = 0.0\n",
        "total = 0\n",
        "\n",
        "for imgs, _ in loader:\n",
        "    # imgs shape: [B, 3, 32, 32], 值在 [0,1]\n",
        "    batch_samples = imgs.size(0)\n",
        "    imgs = imgs.view(batch_samples, 3, -1)  # [B, 3, 32*32]\n",
        "    mean += imgs.mean(dim=(0, 2)) * batch_samples\n",
        "    var  += imgs.var(dim=(0, 2), unbiased=False) * batch_samples\n",
        "    total += batch_samples\n",
        "\n",
        "mean /= total\n",
        "var  /= total\n",
        "std = var.sqrt()\n",
        "\n",
        "print(mean, std)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5WmV1je_1kr",
        "outputId": "13ac6e71-a56f-448b-c20b-025e9e07720a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import Subset, random_split, DataLoader\n",
        "def sub_dataset(trainset, each_class_len,need_random=False):\n",
        "  subset_indices = []\n",
        "  targets = np.array(trainset.targets)\n",
        "  for c in np.unique(targets):\n",
        "    class_idx = np.where(targets == c)[0]\n",
        "    # print(class_idx.shape,class_idx[:10])\n",
        "    if need_random:\n",
        "      np.random.shuffle(class_idx)\n",
        "    subset_indices.extend(class_idx[:each_class_len])\n",
        "  # Convert subset_indices to a numpy array to allow advanced indexing\n",
        "  subset_indices = np.array(subset_indices)\n",
        "  return subset_indices\n",
        "\n",
        "def sub_sub_indices(subset_indices, val_ratio=0.1,need_random=True):\n",
        "  # train_set,val_set = random_split(train_subset,[n_train,n_val])\n",
        "  train_indices = []\n",
        "  val_indices = []\n",
        "  val_ratio = 0.1\n",
        "  subset_labels = targets[subset_indices]\n",
        "  for c in np.unique(subset_labels):\n",
        "    class_idx = np.where(subset_labels == c)[0]\n",
        "    if need_random:\n",
        "      np.random.shuffle(class_idx)\n",
        "    n_val = int(len(class_idx)*val_ratio)\n",
        "    val_indices.extend(subset_indices[class_idx[:n_val]])\n",
        "    train_indices.extend(subset_indices[class_idx[n_val:]])\n",
        "  return train_indices, val_indices\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    #transforms.Resize(40),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2467, 0.2432, 0.2611))\n",
        "])\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2467, 0.2432, 0.2611))\n",
        "])\n",
        "batch_size = 128*2\n",
        "# --- Load full CIFAR-10 train set ---\n",
        "trainset_full = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "targets = np.array(trainset_full.targets)\n",
        "# --- Class names ---\n",
        "classes = trainset_full.classes\n",
        "# classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "#            'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "print(classes)\n",
        "\n",
        "# --- (Optional) use only a subset for speed\n",
        "subset_indices = sub_dataset(trainset_full, 1000, need_random=False)\n",
        "#train_subset = Subset(trainset_full, subset_indices)\n",
        "\n",
        "# --- Split subset_indices into train/validation ---\n",
        "train_indices, val_indices = sub_sub_indices(subset_indices, val_ratio=0.1, need_random=True)\n",
        "train_set = Subset(trainset_full,train_indices)\n",
        "val_set  = Subset(trainset_full, val_indices)\n",
        "train_labels = [classes[x] for x in targets[train_indices]]\n",
        "val_labels = [classes[x] for x in targets[val_indices]]\n",
        "# use all \n",
        "train_set = trainset_full\n",
        "train_labels = [classes[x] for x in targets]\n",
        "# --- DataLoaders ---\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True,num_workers=8)\n",
        "val_loader = DataLoader(val_set,batch_size = batch_size,shuffle=False,num_workers=8)\n",
        "# --- Test Set ---\n",
        "test_set = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform_test)\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False,num_workers=8)\n",
        "test_targets = np.array(test_set.targets)\n",
        "test_labels = [classes[x] for x in test_targets]\n",
        "# use part of test \n",
        "# test_indices = sub_dataset(test_set, 100, need_random=False)\n",
        "# test_set_sub =  Subset(test_set, test_indices)\n",
        "# test_labels = [classes[x] for x in test_targets[test_indices]]\n",
        "# test_loader =  DataLoader(test_set_sub,batch_size = batch_size, shuffle=False)\n",
        "# use test set as val set\n",
        "val_set = test_set\n",
        "val_loader = test_loader\n",
        "val_labels = test_labels\n",
        "\n",
        "print(f\"Batch size:{batch_size}, \\\n",
        "      Train batches:{len(train_loader)}, \\\n",
        "      Val batches:{len(val_loader)},\\\n",
        "      Test batches:{len(test_loader)}\")\n",
        "# check train and val set is balanced or not\n",
        "from collections import Counter\n",
        "print(\"Train class counts:\",Counter(train_labels))\n",
        "print(\"Val class counts:\", Counter(val_labels))\n",
        "print(\"Test class counts:\", Counter(test_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdIarqe4tnBs"
      },
      "source": [
        "#**Visualizing the Data**\n",
        "Obtaining a batch of training data and plot the same with its lables using matplotlib library. You can also see how the transformations which you applied in the previous step show up in these visualizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "id": "xhhpI2ntAB8f",
        "outputId": "d187e479-f455-46ae-bdac-b0af3c212f91"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "print(classes)\n",
        "%matplotlib inline\n",
        "\n",
        "# helper function to un-normalize and display an image\n",
        "def imshow(img):\n",
        "  img = img / 2 + 0.5  # unnormalize\n",
        "  plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image\n",
        "\n",
        "# obtain one batch of training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "images = images.numpy() # convert images to numpy for display\n",
        "print(images[0].shape)\n",
        "# plot the images in the batch, along with the corresponding labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "# display 20 images\n",
        "for idx in np.arange(20):\n",
        "  ax = fig.add_subplot(2, 20//2, idx+1, xticks=[], yticks=[])\n",
        "  imshow(images[idx])\n",
        "  ax.set_title(classes[labels[idx]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQBRSenSvD7P"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkqQ7fsUxA0r"
      },
      "source": [
        "#**Defining the Network Architecture**\n",
        "In this section the entire Research Paper is implemented to define the Residual Network approach taken by the researchers\n",
        "\n",
        "NOTE:\n",
        "\n",
        "Output volume for a convolutional layer\n",
        "To compute the output size of a given convolutional layer we can perform the following calculation (taken from Stanford's cs231n course):\n",
        "\n",
        "We can compute the spatial size of the output volume as a function of the input volume size (W), the kernel/filter size (F), the stride with which they are applied (S), and the amount of zero padding used (P) on the border. The correct formula for calculating how many neurons define the output_W is given by (W−F+2P)/S+1.\n",
        "\n",
        "For example for a 7x7 input and a 3x3 filter with stride 1 and pad 0 we would get a 5x5 output. With stride 2 we would get a 3x3 output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#optimizer = optim.SGD(ResNet18.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "class SGDBaseOptimizer(BaseOptimizer):\n",
        "  def __init__(self,model,criterion,epochs):\n",
        "    super().__init__(model,criterion,epochs)\n",
        "  def initialize_optimizer(self, learning_rate):\n",
        "    optimizer = optim.SGD(self.model.parameters(), lr=learning_rate, weight_decay=0.0001,momentum=0.9)\n",
        "    # Cosine annealing: lr → eta_min by the end\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer,\n",
        "        T_max=self.epochs,   # period (usually = total epochs)\n",
        "        eta_min=0.1*learning_rate      # minimum LR (here 0.1× of 0.01)\n",
        "    )\n",
        "    self.optimizer,self.scheduler =  optimizer, scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import os\n",
        "# base_model = \"model/baseline.pt\"\n",
        "# if os.path.exists(base_model):\n",
        "#     print(\"Load pre-trained baseline model\")\n",
        "# else:\n",
        "#     # resnet18 = models.resnet18(weights=None, num_classes=10)   # train from scratch\n",
        "#     resnet18 = ResNet18()\n",
        "#     resnet18 = resnet18.to(device)\n",
        "#     epochs = 100\n",
        "#     criterion = nn.CrossEntropyLoss(reduction='mean')\n",
        "#     learning_rate = 0.1 #from paper\n",
        "#     optimizer = SGDBaseOptimizer(resnet18,criterion,epochs)\n",
        "#     optimizer.initialize_optimizer(learning_rate)\n",
        "#     losses_base,losses_base_val,base_acc, base_time_cost = train_once(resnet18,criterion,optimizer,train_loader,test_loader,learning_rate,epochs)\n",
        "#     print(\"Training finished!\")\n",
        "#     torch.save(resnet18.state_dict(), 'model/baseline.pt')\n",
        "def initialize_model(device):\n",
        "    # resnet18 = models.resnet18(weights=None, num_classes=10)   # train from scratch\n",
        "    resnet18 = ResNet18()\n",
        "    # resnet18.load_state_dict(torch.load('baseline.pt'))\n",
        "    resnet18 = resnet18.to(device)\n",
        "    if device == \"cuda\" and torch.cuda.device_count() > 1:\n",
        "        resnet18 = torch.nn.DataParallel(resnet18)\n",
        "    return resnet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDAlhqzyyYaG"
      },
      "source": [
        "#**Specifying the Loss Function and Optimizer**\n",
        "We use CrossEntropyLoss as Loss function and\n",
        "\n",
        "[Stochastic Gradient Descent](https://leon.bottou.org/publications/pdf/compstat-2010.pdf)\n",
        "Adam\n",
        "MuonMVR2\n",
        "AdaOrth\n",
        " as Optimizer with momentum and weight decay specified by the research paper of **ON THE CONVERGENCE OF MUON AND BEYOND**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlkXUNc-aVV9"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "# specify loss function (categorical cross-entropy)\n",
        "# specify optimizer\n",
        "# optimizer = optim.SGD(ResNet18.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0001)\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#**AdaOrthL+ method:** similar like MuonMVR2 use two random batches to decrease noise\n",
        "\n",
        "\n",
        "![alt text](image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "#fix for mps\n",
        "def _compile_for_device(fn):\n",
        "    # Inductor doesn't support MPS; use AOTAutograd there.\n",
        "    if torch.backends.mps.is_available():\n",
        "        return torch.compile(fn, backend=\"aot_eager\", dynamic=True)\n",
        "    # Inductor is fine on CUDA/CPU\n",
        "    return torch.compile(fn, backend=\"inductor\", dynamic=True)\n",
        "\n",
        "@_compile_for_device\n",
        "def zeropower_via_newtonschulz5(G, steps=3, eps=1e-7):\n",
        "    \"\"\"\n",
        "    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G.\n",
        "    \"\"\"\n",
        "    assert len(G.shape) == 2\n",
        "    a, b, c = (3.4445, -4.7750,  2.0315)\n",
        "    d_type = G.dtype\n",
        "    X = G.bfloat16()\n",
        "    #x_flat = X.reshape(-1)      # 把矩阵展平成一个 1D 向量\n",
        "    X /= (X.norm() + eps) # ensure top singular value <= 1\n",
        "    if G.size(0) > G.size(1):\n",
        "        X = X.T\n",
        "    for _ in range(steps):\n",
        "        A = X @ X.T\n",
        "        B = b * A + c * A @ A\n",
        "        X = a * X + B @ X\n",
        "    if G.size(0) > G.size(1):\n",
        "        X = X.T\n",
        "    return X.to(d_type)\n",
        "class AdaOrth(torch.optim.Optimizer):\n",
        "    def __init__(self, params, nu=1, lr=1e-3, weight_decay=0,adamw_betas=(0.95, 0.99),gamma=0.025,eps=1e-8):\n",
        "        r\"\"\"\n",
        "    AdaOrth: Adaptive Orthogonalized Gradient Method (Algorithm 1)\n",
        "\n",
        "    变体：\n",
        "      - nu=0  (AdaOrth-L↓):   M_t = α_t g_t + (1-α_t) M_{t-1}, g_t=∇f(X_t​;ξ_t​)\n",
        "      - nu=1  (AdaOrth-L↑):   M_t​=(1−α_t​)(M_{t−1}​−∇f(X_{t-1}​;ξ_t))+∇f(X_t​;ξ_t​)\n",
        "\n",
        "    其它：\n",
        "      - α_t = ( Σ_{i=1..t} g_i^2 / max_{1..t} g_i^2 )^{-p}\n",
        "      - γ_t = ( Σ_{i=1..t} m_i^2 / α_i )^{-p}\n",
        "      - η_t = θ γ_t ||M_t||_F\n",
        "      - X_{t+1} = X_t - η_t Orth(M_t)\n",
        "\n",
        "    仅对二维参数（权重矩阵）使用正交化步；其它参数退化为普通 SGD。\n",
        "    \"\"\"\n",
        "        defaults = dict(lr=lr, weight_decay=weight_decay, adamw_betas=adamw_betas, gamma=gamma, eps=eps)\n",
        "        super(AdaOrth, self).__init__(params, defaults)\n",
        "        self.nu = nu \n",
        "        if nu == 0:\n",
        "            self.p = 0.5\n",
        "        else:\n",
        "            self.p = 2.0/3.0\n",
        "    @torch.no_grad()\n",
        "    def update_last_grad(self):\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                state = self.state[p]\n",
        "                if \"last_grad\" not in state:\n",
        "                    state[\"last_grad\"] = torch.zeros_like(p)\n",
        "                state[\"last_grad\"].zero_().add_(p.grad, alpha=1.0)\n",
        "    @torch.no_grad()\n",
        "    def step(self):\n",
        "        for group in self.param_groups:\n",
        "            lr = group['lr']\n",
        "            beta1, beta2 = group['adamw_betas']\n",
        "            eps = group['eps']\n",
        "            weight_decay = group['weight_decay']\n",
        "            # momentum = group['momentum']\n",
        "            gamma = group['gamma']\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad\n",
        "                state = self.state[p]\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['last_grad'] = torch.zeros_like(p)\n",
        "                    if len(p.shape) != 2:\n",
        "                        # For bias vectors - AdamW style moments\n",
        "                        state['exp_avg'] = torch.zeros_like(p)\n",
        "                        state['exp_avg_sq'] = torch.zeros_like(p)\n",
        "                    else:\n",
        "                        state['m_t'] = torch.zeros_like(p)\n",
        "                        state['m_t_norm_square_div_alpha_sum'] = 0.0\n",
        "                        state['g_sum'] = 0.0\n",
        "                        state['g_max'] = 0.0\n",
        "\n",
        "                last_grad = state['last_grad']\n",
        "                state['step'] += 1\n",
        "\n",
        "                if len(p.shape) == 2:\n",
        "                    g_square = grad.norm().pow(2)\n",
        "                    last_grad = state['last_grad']\n",
        "                    state['g_sum'] += g_square\n",
        "                    state['g_max'] = max(state['g_max'], g_square)\n",
        "                    alpha = (state['g_sum']/(state['g_max']+eps)).pow(-self.p)\n",
        "                    m_t = state['m_t']\n",
        "                    if self.nu == 0:\n",
        "                        m_t = (1-alpha)*m_t + alpha*grad\n",
        "                    else:\n",
        "                        m_t = (1-alpha)*(m_t - last_grad) + grad\n",
        "                    state['m_t'] = m_t\n",
        "                    m_t_norm = m_t.norm()\n",
        "                    state['m_t_norm_square_div_alpha_sum'] = state['m_t_norm_square_div_alpha_sum'] + m_t_norm.pow(2)/(alpha+eps)\n",
        "                    update =  zeropower_via_newtonschulz5(m_t)\n",
        "                    adjusted_lr = lr*state['m_t_norm_square_div_alpha_sum'].pow(-self.p)*m_t_norm\n",
        "                    p.data.mul_(1 - lr * weight_decay)\n",
        "                    p.data.add_(update, alpha=-adjusted_lr)\n",
        "                    if self.nu == 0:\n",
        "                        state['last_grad'] = p.grad\n",
        "                else:\n",
        "                    # For bias vectors - use simple update\n",
        "                    step = state['step']\n",
        "                    # Compute momentum-like term with correction\n",
        "                    c_t = (grad - last_grad).mul(gamma * (beta1 / (1. - beta1))).add(grad)\n",
        "                    c_t_norm = torch.norm(c_t)\n",
        "                    # avoid inductor lowering bug: compute norm explicitly and detach\n",
        "                    # c_t_norm = torch.sqrt(torch.sum((c_t.detach() * c_t.detach()), dim=None))\n",
        "                    if c_t_norm > 1.:\n",
        "                        c_t = c_t / c_t_norm\n",
        "                    exp_avg = state['exp_avg']\n",
        "                    exp_avg_sq = state['exp_avg_sq']\n",
        "                    exp_avg.lerp_(c_t, 1 - beta1)\n",
        "                    exp_avg_sq.lerp_(c_t.square(), 1 - beta2)\n",
        "                    g = exp_avg / (eps + exp_avg_sq.sqrt())\n",
        "                    bias_correction1 = 1 - beta1**step\n",
        "                    bias_correction2 = 1 - beta2**step\n",
        "                    scale = bias_correction1 / bias_correction2**0.5\n",
        "                    p.data.mul_(1 - lr * weight_decay)\n",
        "                    p.data.add_(g, alpha=-lr / scale)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#**Muon-MVR2 method from ON THE CONVERGENCE OF MUON AND BEYOND**\n",
        "\n",
        "fixed some error for mac M1\n",
        "\n",
        "mvr2 only works with 2 dim weights, other dims use method like adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEAqv4_vfqlD"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Muon-MVR For LLM Pretraining.\n",
        "'''\n",
        "import torch\n",
        "import math\n",
        "\n",
        "#fix for mps\n",
        "def _compile_for_device(fn):\n",
        "    # Inductor doesn't support MPS; use AOTAutograd there.\n",
        "    if torch.backends.mps.is_available():\n",
        "        return torch.compile(fn, backend=\"aot_eager\", dynamic=True)\n",
        "    # Inductor is fine on CUDA/CPU\n",
        "    return torch.compile(fn, backend=\"inductor\", dynamic=True)\n",
        "\n",
        "\n",
        "@_compile_for_device\n",
        "def zeropower_via_newtonschulz5(G, steps=3, eps=1e-7):\n",
        "    \"\"\"\n",
        "    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G.\n",
        "    \"\"\"\n",
        "    assert len(G.shape) == 2\n",
        "    a, b, c = (3.4445, -4.7750,  2.0315)\n",
        "    d_type = G.dtype\n",
        "    X = G.bfloat16()\n",
        "    #x_flat = X.reshape(-1)      # 把矩阵展平成一个 1D 向量\n",
        "    X /= (X.norm() + eps) # ensure top singular value <= 1\n",
        "    if G.size(0) > G.size(1):\n",
        "        X = X.T\n",
        "    for _ in range(steps):\n",
        "        A = X @ X.T\n",
        "        B = b * A + c * A @ A\n",
        "        X = a * X + B @ X\n",
        "    if G.size(0) > G.size(1):\n",
        "        X = X.T\n",
        "    return X.to(d_type)\n",
        "\n",
        "class MuonMVR(torch.optim.Optimizer):\n",
        "    r'''\n",
        "    Standard MVR:\\\\(nabla f(X_t;\\xi_t) - \\\\nabla f(X_{t-1};\\xi_{t})\n",
        "    Approximate MVR:\n",
        "        1.\\\\(nabla f(X_t;\\xi_t) - \\\\nabla f(X_{t-1};\\xi_{t-1})\n",
        "        2.\\\\(nabla f(X_t;\\xi_t) - \\\\nabla f(X_{t};\\xi_{t-1}), It has low computational complexity\n",
        "         and is more convenient in practice\n",
        "    '''\n",
        "    def __init__(self, params, lr=3e-3, momentum = 0.95 ,adamw_betas=(0.95, 0.99), eps=1e-8,\n",
        "                 weight_decay=0.0, gamma=0.025, is_approx=False):\n",
        "        if lr < 0.0:\n",
        "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
        "        if not 0.0 <= momentum < 1.0:\n",
        "            raise ValueError(f\"Invalid momentum parameter: {momentum}\")\n",
        "        if not 0.0 <= adamw_betas[0] < 1.0:\n",
        "            raise ValueError(f\"Invalid beta1 parameter: {adamw_betas[0]}\")\n",
        "        if not 0.0 <= adamw_betas[1] < 1.0:\n",
        "            raise ValueError(f\"Invalid beta2 parameter: {adamw_betas[1]}\")\n",
        "\n",
        "        defaults = dict(lr=lr, momentum=momentum,adamw_betas=adamw_betas, eps=eps,\n",
        "                       weight_decay=weight_decay, gamma=gamma)\n",
        "        super().__init__(params, defaults)\n",
        "        self.is_approx = is_approx\n",
        "\n",
        "    def adjust_lr_for_muon(self, lr, param_shape):\n",
        "        A, B = param_shape[:2]\n",
        "        # We adjust the learning rate and weight decay based on the size of the parameter matrix\n",
        "        # as describted in the paper\n",
        "        adjusted_ratio = 0.2 * math.sqrt(max(A, B))\n",
        "        # adjusted_ratio = math.sqrt(A*B)\n",
        "        adjusted_lr = lr * adjusted_ratio\n",
        "        return adjusted_lr\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update_last_grad(self):\n",
        "        if not self.is_approx:\n",
        "            for group in self.param_groups:\n",
        "                for p in group['params']:\n",
        "                    state = self.state[p]\n",
        "                    if \"last_grad\" not in state:\n",
        "                        state[\"last_grad\"] = torch.zeros_like(p)\n",
        "                    state[\"last_grad\"].zero_().add_(p.grad, alpha=1.0)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self):\n",
        "        for group in self.param_groups:\n",
        "            lr = group['lr']\n",
        "            beta1, beta2 = group['adamw_betas']\n",
        "            eps = group['eps']\n",
        "            weight_decay = group['weight_decay']\n",
        "            momentum = group['momentum']\n",
        "            gamma = group['gamma']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                grad = p.grad\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p)\n",
        "                    state['last_grad'] = torch.zeros_like(p)\n",
        "                    # state['previous_grad'] = torch.zeros_like(p)\n",
        "                    if len(p.shape) != 2:  # Only for 2D tensors\n",
        "                        state['exp_avg_sq'] = torch.zeros_like(p)\n",
        "\n",
        "                state['step'] += 1\n",
        "                last_grad = state['last_grad']\n",
        "                if len(p.shape) == 2:\n",
        "                    exp_avg = state['exp_avg']\n",
        "\n",
        "                    # Compute momentum-like term with correction\n",
        "                    c_t = (grad - last_grad).mul(gamma * (momentum / (1. - momentum))).add(grad)\n",
        "                    c_t_norm = torch.norm(c_t)\n",
        "                    if c_t_norm > 1.:\n",
        "                        c_t = c_t / c_t_norm\n",
        "                    # Update moving averages\n",
        "                    exp_avg.mul_(momentum).add_(c_t, alpha=1 - momentum)\n",
        "                    update = zeropower_via_newtonschulz5(exp_avg.mul(1./(1.- momentum))) # whiten the update\n",
        "                    adjusted_lr = self.adjust_lr_for_muon(lr, p.shape)\n",
        "                    p.data.mul_(1 - lr * weight_decay)\n",
        "                    p.data.add_(update, alpha=-adjusted_lr)\n",
        "                else:\n",
        "                    # For bias vectors - use simple update\n",
        "                    step = state['step']\n",
        "                    # Compute momentum-like term with correction\n",
        "                    c_t = (grad - last_grad).mul(gamma * (beta1 / (1. - beta1))).add(grad)\n",
        "                    c_t_norm = torch.norm(c_t)\n",
        "                    # avoid inductor lowering bug: compute norm explicitly and detach\n",
        "                    # c_t_norm = torch.sqrt(torch.sum((c_t.detach() * c_t.detach()), dim=None))\n",
        "                    if c_t_norm > 1.:\n",
        "                        c_t = c_t / c_t_norm\n",
        "                    exp_avg = state['exp_avg']\n",
        "                    exp_avg_sq = state['exp_avg_sq']\n",
        "                    exp_avg.lerp_(c_t, 1 - beta1)\n",
        "                    exp_avg_sq.lerp_(c_t.square(), 1 - beta2)\n",
        "                    g = exp_avg / (eps + exp_avg_sq.sqrt())\n",
        "                    bias_correction1 = 1 - beta1**step\n",
        "                    bias_correction2 = 1 - beta2**step\n",
        "                    scale = bias_correction1 / bias_correction2**0.5\n",
        "                    p.data.mul_(1 - lr * weight_decay)\n",
        "                    p.data.add_(g, alpha=-lr / scale)\n",
        "\n",
        "                if self.is_approx:\n",
        "                    state['last_grad'].copy_(grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dMWiYJf1Ou-"
      },
      "source": [
        "#**MuonMVR Training Loop**\n",
        "Here we train the architecture on training data and check its validation loss by using the validation set and saving the model only if there is an improvement ie decrease in the validation loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bFAzI_3Vwhj"
      },
      "source": [
        "## MVR2 Usage\n",
        "3.  **Optimizer Modes**\n",
        "    MuonMVR can be initialized in different modes to trade off between precision and computational cost.\n",
        "\n",
        "    **Exact Variance Reduction (`is_approx=False`)**\n",
        "    To achieve the most precise variance reduction, you must manually manage the model state. Before calculating the gradient for the previous batch, you need to load the model state from the previous iteration. This ensures that the gradient is computed with the correct model weights.\n",
        "    ```python\n",
        "    optimizer = MuonMVR(model.parameters(), lr=1e-3, is_approx=False)\n",
        "    old_state_dict = {}\n",
        "    \n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        # Store the current model state\n",
        "        cur_state_dict = {k: v.data.clone() for k, v in net.state_dict().items()}\n",
        "    \n",
        "        if old_state_dict:\n",
        "            # Load the previous model state to compute the old gradient\n",
        "            net.load_state_dict(old_state_dict)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.update_last_grad()\n",
        "    \n",
        "        # Restore the current model state to compute the new gradient\n",
        "        net.load_state_dict(cur_state_dict)\n",
        "        old_state_dict = {k: v.data.clone() for k, v in cur_state_dict.items()}\n",
        "        \n",
        "        # Standard forward/backward pass and step\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiicKSbbint0"
      },
      "outputs": [],
      "source": [
        "class MuonMVR2Optimizer(BaseOptimizer):\n",
        "  def __init__(self,model,criterion,epochs):\n",
        "    super().__init__(model,criterion,epochs)\n",
        "    self.old_state_dict = {}\n",
        "  def initialize_optimizer(self, learning_rate):\n",
        "    optimizer = MuonMVR(self.model.parameters(), lr=learning_rate,weight_decay=0.01,gamma=0.1,is_approx=False)\n",
        "    # Cosine annealing: lr → eta_min by the end\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer,\n",
        "        T_max=self.epochs,   # period (usually = total epochs)\n",
        "        eta_min=0.1*learning_rate      # minimum LR (here 0.1× of 0.01)\n",
        "    )\n",
        "    self.optimizer,self.scheduler =  optimizer, scheduler\n",
        "  def zero_grad(self,inputs=None,labels=None):\n",
        "    model, old_state_dict = self.model, self.old_state_dict\n",
        "    optimizer = self.optimizer\n",
        "    # Store the current model state\n",
        "    cur_state_dict = {k: v.data.clone() for k, v in model.state_dict().items()}\n",
        "    if old_state_dict:\n",
        "        # Load the previous model state to compute the old gradient\n",
        "        model.load_state_dict(old_state_dict)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.update_last_grad()\n",
        "    #restore state\n",
        "    model.load_state_dict(cur_state_dict)\n",
        "    self.old_state_dict = {k: v.data.clone() for k, v in cur_state_dict.items()}\n",
        "    self.optimizer.zero_grad()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvhesdndign7"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGej2WdOVHEs"
      },
      "source": [
        "##**grid search learning rate for MuonMVR2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HvRH5HvVE_X",
        "outputId": "4cec4db1-e77b-4c91-c2f9-44a9ad5358dd"
      },
      "outputs": [],
      "source": [
        "learning_rates = [1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2, 1e-1]\n",
        "#42\n",
        "result_mvr2 = [1.662683129310608, 1.5963122844696045, 1.4677045345306396,\n",
        "           1.3441717624664307, 1.3620641231536865, 1.5036698579788208, 1.4877097606658936]\n",
        "def grids_search_val(get_optimizer):\n",
        "  result = []\n",
        "  for ii, learning_rate in enumerate(learning_rates):\n",
        "    print(f\".........Learning rate {ii+1}/{len(learning_rates)}: {learning_rate}............\")\n",
        "    model = initialize_model(device)\n",
        "    optimizer = get_optimizer(model, learning_rate)\n",
        "    train_losses, val_losses, acc, time_cost = train_once(model,criterion,optimizer,train_loader,val_loader,learning_rate,epochs)\n",
        "    result.append(min(val_losses))\n",
        "  return result\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def muonmvr2_optimizer(model,learning_rate):\n",
        "  optimizer = MuonMVR2Optimizer(model,criterion,epochs)\n",
        "  optimizer.initialize_optimizer(learning_rate)\n",
        "  return optimizer\n",
        "#4242\n",
        "#on val\n",
        "result_mvr2 = []\n",
        "if not result_mvr2:\n",
        "  result_mvr2 = grids_search_val(muonmvr2_optimizer)\n",
        "  #result = [x.cpu() for x in result]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "encBhZ6_2tN4",
        "outputId": "a68916c2-74f2-476e-c4cd-08966809e1ac"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "plt.xscale('log')   # good practice for learning rates\n",
        "plt.xlabel(\"Learning rate\")\n",
        "plt.ylabel(\"Min Validation Loss\")\n",
        "print(result_mvr2)\n",
        "plt.title(\"Learning Rate Grid Search\")\n",
        "plt.xticks(learning_rates, labels=[str(lr) for lr in learning_rates],rotation=45)\n",
        "plt.plot(learning_rates, result_mvr2, label=\"Grid Search(Validataion Loss)\", linewidth=2, marker='o')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "min_index = result_mvr2.index(min(result_mvr2))\n",
        "learning_rate = learning_rates[min_index]\n",
        "print(f\"Best learning rate: {learning_rate}, Min val loss: {result_mvr2[min_index]}\")\n",
        "print([float(x) for x in result_mvr2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import torch\n",
        "# torch._dynamo.config.verbose = True\n",
        "# import os\n",
        "# os.environ[\"TORCH_LOGS\"] = \"recompiles,graph_breaks,guards\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#**training Model with MuonMVR2 using best learning rate**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rate = 0.05 #from paper\n",
        "model_mvr2 = initialize_model(device)\n",
        "optimizer = muonmvr2_optimizer(model_mvr2, learning_rate)\n",
        "losses_mvr2,losses_mvr2_val, mvr2_acc, mvr2_time_cost = train_once(model_mvr2,criterion,optimizer,train_loader,test_loader,learning_rate,epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBC_rC7Lj5L3"
      },
      "source": [
        "#**SGD learning rate search**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7HXCXm0kJ_3"
      },
      "outputs": [],
      "source": [
        "#optimizer = optim.SGD(ResNet18.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "class SGDOptimizer(BaseOptimizer):\n",
        "  def __init__(self,model,criterion,epochs):\n",
        "    super().__init__(model,criterion,epochs)\n",
        "  def initialize_optimizer(self, learning_rate):\n",
        "    optimizer = optim.SGD(self.model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "    # Cosine annealing: lr → eta_min by the end\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer,\n",
        "        T_max=self.epochs,   # period (usually = total epochs)\n",
        "        eta_min=0.1*learning_rate      # minimum LR (here 0.1× of 0.01)\n",
        "    )\n",
        "    self.optimizer,self.scheduler =  optimizer, scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_sgd(model, learning_rate):\n",
        "    optimizer = SGDOptimizer(model, criterion, epochs)\n",
        "    optimizer.initialize_optimizer(learning_rate)\n",
        "    return optimizer\n",
        "result_sgd = []\n",
        "if not result_sgd:\n",
        "    result_sgd = grids_search_val(get_sgd)\n",
        "    #result_sgd = [float(x.cpu()) for x in result_sgd]\n",
        "print(result_sgd)\n",
        "min_index = result_sgd.index(min(result_sgd))\n",
        "learning_rate = learning_rates[min_index]\n",
        "print(f\"Best learning rate: {learning_rate}, Min val loss: {result_sgd[min_index]}\")\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.xscale('log')   # good practice for learning rates\n",
        "plt.xlabel(\"Learning rate\")\n",
        "plt.ylabel(\"Min Validation Loss\")\n",
        "plt.title(\"Learning Rate Grid Search\")\n",
        "plt.xticks(learning_rates, labels=[str(lr) for lr in learning_rates],rotation=45)\n",
        "plt.plot(learning_rates, result_sgd, label=\"Grid Search(Validataion Loss)\", linewidth=2, marker='o')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#**SGD Training Model with best learning rate**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLI6pPDhg1VO",
        "outputId": "9e6f45d6-f63e-4d4b-e0bf-2fa1bcd452b1"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.1 #from paper\n",
        "model_sgd = initialize_model(device)\n",
        "optimizer = SGDOptimizer(model_sgd,criterion,epochs)\n",
        "optimizer.initialize_optimizer(learning_rate)\n",
        "losses_sgd,losses_sgd_val,sgd_acc, sgd_time_cost = train_once(model_sgd,criterion,optimizer,train_loader,test_loader,learning_rate,epochs)\n",
        "print(\"Training finished!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#**Adam Optimizer Learning Rate Search**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AdamOptimizer(BaseOptimizer):\n",
        "  def __init__(self,model,criterion,epochs):\n",
        "    super().__init__(model,criterion,epochs)\n",
        "  def initialize_optimizer(self, learning_rate):\n",
        "    optimizer = optim.Adam(self.model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "    # Cosine annealing: lr → eta_min by the end\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer,\n",
        "        T_max=self.epochs,   # period (usually = total epochs)\n",
        "        eta_min=0.1*learning_rate      # minimum LR (here 0.1× of 0.01)\n",
        "    )\n",
        "    self.optimizer,self.scheduler =  optimizer, scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_adam(model, learning_rate):\n",
        "    optimizer = AdamOptimizer(model, criterion, epochs)\n",
        "    optimizer.initialize_optimizer(learning_rate)\n",
        "    return optimizer\n",
        "result_adam = []\n",
        "if not result_adam:\n",
        "    result_adam = grids_search_val(get_adam)\n",
        "print(result_adam)\n",
        "min_index = result_adam.index(min(result_adam))\n",
        "learning_rate = learning_rates[min_index]\n",
        "print(f\"Best learning rate: {learning_rate}, Min val loss: {result_adam[min_index]}\")\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.xscale('log')   # good practice for learning rates\n",
        "plt.xlabel(\"Learning rate\")\n",
        "plt.ylabel(\"Min Validation Loss\")\n",
        "plt.title(\"Learning Rate Grid Search\")\n",
        "plt.xticks(learning_rates, labels=[str(lr) for lr in learning_rates],rotation=45)\n",
        "plt.plot(learning_rates, result_adam, label=\"Grid Search(Validataion Loss)\", linewidth=2, marker='o')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#**Adam Training with best learning rate**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rate = 0.01\n",
        "model_adam = initialize_model(device)\n",
        "optimizer = AdamOptimizer(model_adam,criterion,epochs)\n",
        "optimizer.initialize_optimizer(learning_rate)\n",
        "losses_adam,losses_adam_val, adam_acc ,adam_time_cost = train_once(model_adam,criterion,optimizer,train_loader,test_loader,learning_rate,epochs)\n",
        "print(\"Training finished!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AdaOrthOptimizer(BaseOptimizer):\n",
        "  def __init__(self,model,criterion,epochs):\n",
        "    super().__init__(model,criterion,epochs)\n",
        "    self.old_state_dict = {}\n",
        "  def initialize_optimizer(self, learning_rate):\n",
        "    optimizer = AdaOrth(self.model.parameters(),nu=1, lr=learning_rate, weight_decay=0.01,gamma=0.1)\n",
        "    # Cosine annealing: lr → eta_min by the end\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer,\n",
        "        T_max=self.epochs,   # period (usually = total epochs)\n",
        "        eta_min=0.1*learning_rate      # minimum LR (here 0.1× of 0.01)\n",
        "    )\n",
        "    self.optimizer,self.scheduler =  optimizer, scheduler\n",
        "  def zero_grad(self,inputs=None,labels=None):\n",
        "    model, old_state_dict = self.model, self.old_state_dict\n",
        "    optimizer = self.optimizer\n",
        "    # Store the current model state\n",
        "    cur_state_dict = {k: v.data.clone() for k, v in model.state_dict().items()}\n",
        "    if self.old_state_dict:\n",
        "        # Load the previous model state to compute the old gradient\n",
        "        model.load_state_dict(old_state_dict)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.update_last_grad()\n",
        "    #restore state\n",
        "    model.load_state_dict(cur_state_dict)\n",
        "    self.old_state_dict = {k: v.data.clone() for k, v in cur_state_dict.items()}\n",
        "    self.optimizer.zero_grad()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#**AdaOrth Learning Rate Search**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_adaorth(model, learning_rate):\n",
        "    optimizer = AdaOrthOptimizer(model, criterion, epochs)\n",
        "    optimizer.initialize_optimizer(learning_rate)\n",
        "    return optimizer\n",
        "#42\n",
        "#4242\n",
        "#on val loader\n",
        "result_adaorth = []\n",
        "if not result_adaorth:\n",
        "    result_adaorth = grids_search_val(get_adaorth)\n",
        "    #result_adaorth = [float(x.cpu()) for x in result_adaorth]\n",
        "print(result_adaorth)\n",
        "min_index = result_adaorth.index(min(result_adaorth))\n",
        "learning_rate = learning_rates[min_index]\n",
        "print(f\"Best learning rate: {learning_rate}, Min val loss: {result_adaorth[min_index]}\")\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.xscale('log')   # good practice for learning rates\n",
        "plt.xlabel(\"Learning rate\")\n",
        "plt.ylabel(\"Min Validation Loss\")\n",
        "plt.title(\"Learning Rate Grid Search\")\n",
        "plt.xticks(learning_rates, labels=[str(lr) for lr in learning_rates],rotation=45)\n",
        "plt.plot(learning_rates, result_adaorth, label=\"Grid Search(Validataion Loss)\", linewidth=2, marker='o')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#**AdaOrth Training with best learning rate**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_adaorth = initialize_model(device)\n",
        "optimizer = AdaOrthOptimizer(model_adaorth,criterion,epochs)\n",
        "optimizer.initialize_optimizer(learning_rate)\n",
        "losses_adaorth,losses_adaorth_val, adaorth_acc ,adaorth_time_cost = train_once(model_adaorth,criterion,optimizer,train_loader,test_loader,learning_rate,epochs)\n",
        "print(\"Training finished!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AdaOrthLMinusOptimizer(BaseOptimizer):\n",
        "  def __init__(self,model,criterion,epochs):\n",
        "    super().__init__(model,criterion,epochs)\n",
        "  def initialize_optimizer(self, learning_rate):\n",
        "    #weight_decay=0.01,gamma=0.1\n",
        "    optimizer = AdaOrth(self.model.parameters(),nu=0, lr=learning_rate, weight_decay=0.01,gamma=0.1)\n",
        "    # Cosine annealing: lr → eta_min by the end\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer,\n",
        "        T_max=self.epochs,   # period (usually = total epochs)\n",
        "        eta_min=0.1*learning_rate      # minimum LR (here 0.1× of 0.01)\n",
        "    )\n",
        "    self.optimizer,self.scheduler =  optimizer, scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_adaorthlminus(model, learning_rate):\n",
        "    optimizer = AdaOrthLMinusOptimizer(model, criterion, epochs)\n",
        "    optimizer.initialize_optimizer(learning_rate)\n",
        "    return optimizer\n",
        "#42\n",
        "#4242\n",
        "result_adaorthlminus = []\n",
        "if not result_adaorthlminus:\n",
        "    result_adaorthlminus = grids_search_val(get_adaorthlminus)\n",
        "print(result_adaorthlminus)\n",
        "min_index = result_adaorthlminus.index(min(result_adaorthlminus))\n",
        "learning_rate = learning_rates[min_index]\n",
        "print(f\"Best learning rate: {learning_rate}, Min val loss: {result_adaorthlminus[min_index]}\")\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.xscale('log')   # good practice for learning rates\n",
        "plt.xlabel(\"Learning rate\")\n",
        "plt.ylabel(\"Min Validation Loss\")\n",
        "plt.title(\"Learning Rate Grid Search\")\n",
        "plt.xticks(learning_rates, labels=[str(lr) for lr in learning_rates],rotation=45)\n",
        "plt.plot(learning_rates, result_adaorthlminus, label=\"Grid Search(Validataion Loss)\", linewidth=2, marker='o')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_adaorthlminus = initialize_model(device)\n",
        "optimizer = AdaOrthLMinusOptimizer(model_adaorthlminus,criterion,epochs)\n",
        "optimizer.initialize_optimizer(learning_rate)\n",
        "losses_adaorthlminus,losses_adaorthlminus_val, \\\n",
        "adaorthlminus_acc ,adaorthlminus_time_cost = \\\n",
        "train_once(model_adaorthlminus,criterion,optimizer,train_loader,test_loader,learning_rate,epochs)\n",
        "print(\"Training finished!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#**Plot Training Loss and Val Loss**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "epochs_range = range(1, epochs + 1)\n",
        "# 定义每个算法的颜色（可自定义）\n",
        "# colors = {\n",
        "#     \"mvr2\": \"#1f77b4\",     # 蓝色\n",
        "#     \"sgd\": \"#2ca02c\",      # 绿色\n",
        "#     \"adamw\": \"#d62728\",    # 红色\n",
        "#     \"adaorth\": \"#9467bd\",  # 紫色\n",
        "# }\n",
        "colors = {\n",
        "    # 🔴 Bright red — strong attention color; highlights the best or main method\n",
        "    \"Muon-MVR2\": \"#d62728\",\n",
        "\n",
        "    # 🟠 Orange — warm tone; second-best or related variant of the main method\n",
        "    \"Muon-MVR1\": \"#ff7f0e\",\n",
        "\n",
        "    # 🟢 Green/Teal — balanced, calm color; used for the γ=0 ablation variant\n",
        "    \"Muon-MVR1 (γ=0)\": \"#2ca02c\",\n",
        "\n",
        "    # 🌸 Pink — soft complementary tone; represents Adam (adaptive optimizer family)\n",
        "    \"Adam\": \"#e377c2\",\n",
        "\n",
        "    # 🟣 Purple — cool, classic hue; represents SGD (baseline optimizer)\n",
        "    \"SGD\": \"#9467bd\",\n",
        "\n",
        "    # 🔵 Deep blue — reliable, professional color; for AdaOrth-L (new proposed variant)\n",
        "    \"AdaOrthL-\": \"#1f77b4\",\n",
        "\n",
        "    # 🟡 Olive-yellow — contrasting, bright accent; for AdaOrth-AL+ (advanced/larger version)\n",
        "    \"AdaOrthL+\": \"#bcbd22\",\n",
        "}\n",
        "\n",
        "\n",
        "plt.plot(epochs_range, losses_mvr2, label=\"Muon-MVR2\", linewidth=2, color=colors[\"Muon-MVR2\"])\n",
        "plt.plot(epochs_range, losses_sgd, label=\"SGD\", linewidth=2,color=colors[\"SGD\"])\n",
        "plt.plot(epochs_range, losses_adam, label=\"Adam\", linewidth=2,color=colors[\"Adam\"])\n",
        "plt.plot(epochs_range, losses_adaorth, label=\"AdaOrthL+\", linewidth=2,color=colors[\"AdaOrthL+\"])\n",
        "plt.plot(epochs_range, losses_adaorthlminus, label=\"AdaOrthL-\", linewidth=2,color=colors[\"AdaOrthL-\"])\n",
        "\n",
        "plt.plot(epochs_range ,losses_mvr2_val, label=\"Muon-MVR2(Val)\", linewidth=2, linestyle='--',color=colors[\"Muon-MVR2\"])\n",
        "plt.plot(epochs_range, losses_sgd_val, label=\"SGD(Val)\", linewidth=2, linestyle='--',color=colors[\"SGD\"])\n",
        "plt.plot(epochs_range, losses_adam_val, label=\"Adam(Val)\", linewidth=2, linestyle='--',color=colors[\"Adam\"])\n",
        "plt.plot(epochs_range, losses_adaorth_val, label=\"AdaOrthL+(Val)\", linewidth=2, linestyle='--',color=colors[\"AdaOrthL+\"])\n",
        "plt.plot(epochs_range, losses_adaorthlminus_val, label=\"AdaOrthL-(Val)\", linewidth=2, linestyle='--',color=colors[\"AdaOrthL-\"])\n",
        "\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training/Val Loss Comparison\")\n",
        "plt.legend(fontsize=11, frameon=True)\n",
        "plt.xlim(0, len(epochs_range))\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#**Plot Validation Accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plt.figure(figsize=(10,6))\n",
        "epochs_range = range(1, epochs + 1)\n",
        "plt.plot(epochs_range, mvr2_acc, label=\"Muon-MVR2\", linewidth=2)\n",
        "plt.plot(epochs_range, sgd_acc, label=\"SGD\", linewidth=2)\n",
        "plt.plot(epochs_range, adam_acc, label=\"Adam\", linewidth=2)\n",
        "plt.plot(epochs_range, adaorth_acc, label=\"AdaOrthL+\", linewidth=2)\n",
        "plt.plot(epochs_range, adaorthlminus_acc, label=\"AdaOrthL-\", linewidth=2)\n",
        "\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Validation Accuracy (%)\")\n",
        "plt.title(\"Validation Accuracy Comparison\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.xlim(0, len(epochs_range))\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#**Plot Validation Accuracy with Wall Clock Time**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(mvr2_time_cost, mvr2_acc, label = \"Muon-MVR2\")\n",
        "plt.plot(sgd_time_cost, sgd_acc, label=\"SGD\")\n",
        "plt.plot(adam_time_cost, adam_acc, label=\"Adam\")\n",
        "plt.plot(adaorth_time_cost, adaorth_acc, label=\"AdamOrthL+\")\n",
        "plt.plot(adaorthlminus_time_cost, adaorthlminus_acc, label=\"AdamOrthL-\")\n",
        "\n",
        "plt.xlabel(\"Wall Clock Time (s)\")\n",
        "plt.ylabel(\"Validation Accuracy (%)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(max(mvr2_acc), max(sgd_acc), max(adam_acc), max(adaorth_acc), max(adaorthlminus_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWr3orBMiXrv"
      },
      "source": [
        "#**show losses for two optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "angPoGSTeykj",
        "outputId": "6fb5128f-3db6-43d7-9a0f-0f1f3f095d60"
      },
      "outputs": [],
      "source": [
        "# epochs_list = range(1,epochs+1)\n",
        "# plt.figure(figsize=(8, 5))  # larger, readable figure\n",
        "\n",
        "# # plot both losses\n",
        "# plt.plot(epochs_list, losses_mvr2, label=\"MuonMVR Loss\", linewidth=2)\n",
        "# plt.plot(epochs_list, losses_sgd, label=\"SGD Loss\", linewidth=2, linestyle=\"--\")\n",
        "\n",
        "# # formatting\n",
        "# plt.xlabel(\"Epoch\", fontsize=12)\n",
        "# plt.ylabel(\"Loss\", fontsize=12)\n",
        "# plt.title(\"Training Loss Comparison with CIFAR10\", fontsize=14)\n",
        "# plt.legend()\n",
        "# plt.grid(True, alpha=0.3)\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_lEna0p2Wp5"
      },
      "source": [
        "#**Testing Loop**\n",
        "The real test of the model architecture how well does the model recognizes the image and what is the accuracy on the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsqLF8SVfsnn"
      },
      "outputs": [],
      "source": [
        "# ResNet18.eval()\n",
        "# correct, total = 0, 0\n",
        "# with torch.no_grad():\n",
        "#     for inputs, labels in test_loader:\n",
        "#         inputs, labels = inputs.to(device), labels.to(device)\n",
        "#         outputs = ResNet18(inputs)\n",
        "#         _, predicted = torch.max(outputs.data, 1)\n",
        "#         total += labels.size(0)\n",
        "#         correct += (predicted == labels).sum().item()\n",
        "\n",
        "# print(f\"Accuracy: {100 * correct / total:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# test_loss,acc = test_model(ResNet18_adamw, criterion, test_loader)\n",
        "# print(\"adam\",test_loss, acc)\n",
        "# test_loss,acc = test_model(ResNet18, criterion, test_loader)\n",
        "# print(\"\", test_loss, acc)\n",
        "# test_loss,acc = test_model(model, criterion, test_loader)\n",
        "# print(\"\",test_loss, acc)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "name": "ResNet Implementation on CIFAR10.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "depthv2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
