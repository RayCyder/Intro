{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RayCyder/Intro/blob/main/ResNet_Implementation_on_CIFAR10_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#TestConfig\n",
        "#This file is used to configure the test environment for the project. It contains settings and parameters that are necessary for running tests effectively.\n",
        "class TestConfig:\n",
        "    val_epochs = 10\n",
        "    train_epochs = 50\n",
        "    batch_size = 128\n",
        "    train_samples_per_class = 1000\n",
        "    test_samples_per_class = 500\n",
        "    random_seed = 888\n",
        "    val_ratio = 0.1\n",
        "    weight_decay = 0\n",
        "\n",
        "class StandardConfig:\n",
        "    val_epochs = 20\n",
        "    train_epochs = 100\n",
        "    batch_size = 128\n",
        "    train_samples_per_class = 5000 #50000\n",
        "    test_samples_per_class = 1000 #10000\n",
        "    random_seed = 888\n",
        "    val_ratio = 0.1\n",
        "    weight_decay = 0\n",
        "\n",
        "import torch\n",
        "# class BaseOptimizer(torch.optim.Optimizer):\n",
        "#     pass\n",
        "class OptimizerConfig:\n",
        "    def get_optimizers():\n",
        "        #search subclasses of torch.optim.Optimizer\n",
        "        #all_classes = BaseOptimizer.__subclasses__()\n",
        "        optimizers = {\n",
        "        # \"Muon-MVR2\": MuonMVR2Optimizer,\n",
        "        \"SGD\": SGDOptimizer,\n",
        "        \"ADAM\": AdamOptimizer,\n",
        "        \"AdaOrthL\": AdaOrthLOptimizer,\n",
        "        \"AdaOrthL+\": AdaOrthLPLusOptimizer,\n",
        "        \"AdaGradNorm\": AdaGradNormOptimizer,\n",
        "        \"STORM+\": STORMPlusOptimizer,\n",
        "        }\n",
        "        return optimizers\n",
        "    colors = {\n",
        "        # \"Muon-MVR2\": \"#d62728\",\n",
        "        \"SGD\": \"#9467bd\",\n",
        "        \"ADAM\": \"#e377c2\",\n",
        "        \"AdaOrthL\": \"#bcbd22\",\n",
        "        \"AdaOrthL+\": \"#1f77b4\",\n",
        "        \"AdaGradNorm\": \"#ff7f0e\",\n",
        "        \"STORM+\": \"#2ca02c\",\n",
        "    }\n",
        "\n",
        "global_config = TestConfig()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6ikW2JiYwp0"
      },
      "source": [
        "## model from paper\n",
        "The Resnet Research paper can be accessed from here https://arxiv.org/pdf/1512.03385v1.pdf\n",
        "\n",
        "**Defining the Network Architecture**\n",
        "In this section the entire Research Paper is implemented to define the Residual Network approach taken by the researchers\n",
        "\n",
        "NOTE:\n",
        "\n",
        "Output volume for a convolutional layer\n",
        "To compute the output size of a given convolutional layer we can perform the following calculation (taken from Stanford's cs231n course):\n",
        "\n",
        "We can compute the spatial size of the output volume as a function of the input volume size (W), the kernel/filter size (F), the stride with which they are applied (S), and the amount of zero padding used (P) on the border. The correct formula for calculating how many neurons define the output_W is given by (W−F+2P)/S+1.\n",
        "\n",
        "For example for a 7x7 input and a 3x3 filter with stride 1 and pad 0 we would get a 5x5 output. With stride 2 we would get a 3x3 output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Pe9AqkGDIcm3"
      },
      "outputs": [],
      "source": [
        "'''ResNet in PyTorch.\n",
        "\n",
        "For Pre-activation ResNet, see 'preact_resnet.py'.\n",
        "\n",
        "Reference:\n",
        "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
        "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
        "\n",
        "\n",
        "def test():\n",
        "    net = ResNet18()\n",
        "    y = net(torch.randn(1, 3, 32, 32))\n",
        "    print(y.size())\n",
        "\n",
        "# test()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Random Seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "FEa-4QGLIcm3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "random.seed(global_config.random_seed)\n",
        "torch.manual_seed(global_config.random_seed)\n",
        "torch.cuda.manual_seed_all(global_config.random_seed)\n",
        "np.random.seed(global_config.random_seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import torch\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "import copy\n",
        "class TrainConfig(object):\n",
        "  epochs = 100\n",
        "\n",
        "# choose device\n",
        "device = (\n",
        "    \"cuda\" if torch.cuda.is_available()\n",
        "    else \"mps\" if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using device: {device}\")\n",
        "if device == \"cuda\":\n",
        "    import torch.backends.cudnn as cudnn\n",
        "    cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "# method for initialize model\n",
        "def initialize_model(device):\n",
        "    # resnet18 = models.resnet18(weights=None, num_classes=10)   # train from scratch\n",
        "    resnet18 = ResNet18()\n",
        "    # resnet18.load_state_dict(torch.load('baseline.pt'))\n",
        "    resnet18 = resnet18.to(device)\n",
        "    if device == \"cuda\" and torch.cuda.device_count() > 1:\n",
        "        resnet18 = torch.nn.DataParallel(resnet18)\n",
        "    return resnet18\n",
        "\n",
        "base_model = initialize_model(device)\n",
        "base_init_state = copy.deepcopy(base_model.state_dict())\n",
        "def get_base_model():\n",
        "    model = initialize_model(device)\n",
        "    model.load_state_dict(copy.deepcopy(base_init_state))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Lf-2lnBKIcm4"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "# base class for optimizer with scheduler\n",
        "class BaseOptimizer(object):\n",
        "  def __init__(self,model, criterion, epochs,learning_rate):\n",
        "    self.epochs = epochs\n",
        "    self.criterion = criterion\n",
        "    self.model = model\n",
        "    self.optimizer,self.scheduler = None, None\n",
        "    self.learning_rate = learning_rate\n",
        "  def initialize_optimzer(self):\n",
        "    raise Exception(\"non implemetation\")\n",
        "  def zero_grad(self,inputs=None,labels=None):\n",
        "    self.optimizer.zero_grad()\n",
        "  def step(self):\n",
        "    self.optimizer.step()\n",
        "  def epoch_step(self):\n",
        "    if self.scheduler:\n",
        "      self.scheduler.step()\n",
        "  \n",
        "  def set_scheduler(self, optimizer, learning_rate, rate=0.1):\n",
        "     scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer,\n",
        "        T_max=self.epochs,   # period (usually = total epochs)\n",
        "        eta_min=rate*learning_rate      # minimum LR (here 0.1× of 0.01)\n",
        "    )\n",
        "     self.scheduler = scheduler\n",
        "     return scheduler\n",
        "\n",
        "class TrainModel(object):\n",
        "  def __init__(self,train_loader=None, val_loader=None, classes=None,epochs=100):\n",
        "    self.criterion = torch.nn.CrossEntropyLoss()\n",
        "    self.val_loader = val_loader\n",
        "    self.train_loader = train_loader\n",
        "    self.classes = classes\n",
        "    self.epochs = epochs\n",
        "  def initialize_model(self):\n",
        "    model = get_base_model()\n",
        "    self.model = model\n",
        "    return model\n",
        "  def train(self, optimizer, need_save=False):\n",
        "    epochs = self.epochs\n",
        "    model, criterion = self.model, self.criterion\n",
        "    train_loader, val_loader, classes = self.train_loader, self.val_loader, self.classes\n",
        "    #start train\n",
        "    losses = [] #train loss\n",
        "    val_losses = [] #val loss\n",
        "    accs =[] #train accuracy\n",
        "    elapsed_all = [] #time cost\n",
        "    val_accs = [] #val accurary\n",
        "    best = 0.0 #best accuraray\n",
        "    #optimizer = MuonMVR2Optimizer(model,criterion,epochs,learning_rate)\n",
        "    #optimizer.initialize_optimizer()\n",
        "    start = time.time()\n",
        "    for epoch in range(epochs):\n",
        "      model.train()\n",
        "      running_loss = 0.0\n",
        "      correct = 0\n",
        "      for i, (inputs, labels) in enumerate(train_loader):\n",
        "          inputs, labels = inputs.to(device), labels.to(device)\n",
        "          optimizer.zero_grad(inputs, labels)\n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          running_loss+=loss.item()\n",
        "          correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
        "      accuracy = 100. * correct / len(train_loader.dataset)\n",
        "      accs.append(accuracy)\n",
        "      # step the scheduler once per epoch\n",
        "      optimizer.epoch_step()\n",
        "      losses.append(running_loss/max(1, len(train_loader)))\n",
        "      if optimizer.scheduler:\n",
        "        current_lr = optimizer.scheduler.get_last_lr()[0]  # single param group\n",
        "      else:\n",
        "        current_lr = optimizer.learning_rate\n",
        "      #val_loss = val_once(model,criterion,val_loader)\n",
        "      val_loss, val_acc = test_model(model, criterion, val_loader, classes)\n",
        "      if val_acc > best:\n",
        "        if need_save:\n",
        "          state = {'model': model.state_dict(),\n",
        "                  'accuracy': val_acc,\n",
        "                  'epoch': epoch,}\n",
        "          if not os.path.isdir('checkpoints'):\n",
        "              os.mkdir('checkpoints')\n",
        "          def _get_name(opt):\n",
        "            return opt.__name__ if hasattr(opt, \"__name__\") else opt.__class__.__name__\n",
        "          torch.save(state, f'checkpoints/best_model_{_get_name(optimizer)}.pth')\n",
        "        best = val_acc\n",
        "\n",
        "      val_losses.append(val_loss)\n",
        "      val_accs.append(val_acc)\n",
        "      elapsed = time.time() - start\n",
        "      elapsed_all.append(elapsed)\n",
        "      print(f\"[{epoch+1}] time: {elapsed} loss: {losses[-1]:.3f} val_loss:{val_loss} current_lr={current_lr:.6f}\")\n",
        "\n",
        "    print(f\"Training finished!\")\n",
        "    return losses, accs, val_losses, val_accs, elapsed_all\n",
        "\n",
        "@torch.no_grad()\n",
        "def test_model(ResNet18, criterion, test_loader, classes):\n",
        "  test_loss = 0.0\n",
        "  class_correct = [0 for _ in range(10)]\n",
        "  class_total   = [0 for _ in range(10)]\n",
        "\n",
        "  ResNet18.eval()\n",
        "  # iterate over test data\n",
        "  for batch_idx, (data, target) in enumerate(test_loader):\n",
        "    # move tensors to GPU if CUDA is available\n",
        "    data, target = data.to(device), target.to(device)\n",
        "    count = data.size(0)\n",
        "    # forward pass: compute predicted outputs by passing inputs to the model\n",
        "    output = ResNet18(data)\n",
        "    # calculate the batch loss\n",
        "    loss = criterion(output, target)\n",
        "    # update test loss\n",
        "    test_loss += loss.item()\n",
        "    # convert output probabilities to predicted class\n",
        "    _, pred = torch.max(output, 1)\n",
        "    # compare predictions to true label\n",
        "    correct_tensor = pred.eq(target)\n",
        "    # correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    # calculate test accuracy for each object class\n",
        "    for i in range(count):\n",
        "      label = target.data[i]\n",
        "      class_correct[label] += int(correct_tensor[i].item())\n",
        "      class_total[label] += 1\n",
        "\n",
        "  # average test loss\n",
        "  total_samples = sum(class_total)\n",
        "  test_loss = test_loss/max(1,  len(test_loader))\n",
        "  print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "  for i in range(10):\n",
        "    if class_total[i] > 0:\n",
        "      acc_i = 100 * class_correct[i] / class_total[i]\n",
        "      print('Test Accuracy of %5s: %2d%% (%d/%d)' % (\n",
        "          classes[i],acc_i ,\n",
        "          class_correct[i], class_total[i]))\n",
        "    else:\n",
        "      print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "  acc = 100. * np.sum(class_correct) / total_samples\n",
        "  print('\\nTest Accuracy (Overall): %2d%% (%d/%d)' % (\n",
        "      acc,\n",
        "      sum(class_correct), total_samples))\n",
        "  return test_loss, acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVwz1WydnN_7"
      },
      "source": [
        "## **CIFAR10**\n",
        "The CIFAR10 and CIFAR-100 are labeled subsets of the 80 million tiny images dataset. They were collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.\n",
        "\n",
        "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
        "\n",
        "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\n",
        "\n",
        "Here are the classes in the dataset:\n",
        "1. airplane\n",
        "2. automobile\n",
        "3. bird\n",
        "4. cat\n",
        "5. deer\n",
        "6. dog\n",
        "7. frog\n",
        "8. horse\n",
        "9. ship\n",
        "10. truck\n",
        "\n",
        "The classes are completely mutually exclusive. There is no overlap between automobiles and trucks. \"Automobile\" includes sedans, SUVs, things of that sort. \"Truck\" includes only big trucks. Neither includes pickup trucks.\n",
        "\n",
        "More can be read from their page at https://www.cs.toronto.edu/~kriz/cifar.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Download And Augmentation\n",
        "#**Downloading the CIFAR10 datset and loading the data in Normalized form as torch.FloatTensor datatype and generating a validation set by dividing the training set in 80-20 ratio**\n",
        "\n",
        "#**download dataset to local data directory**\n",
        "#**Image Augmentation**\n",
        "In this cell, we perform some simple data augmentation by randomly flipping and cropping the given image data. We do this by defining a torchvision transform, and you can learn about all the transforms that are used to pre-process and augment data from the [PyTorch documentation](https://pytorch.org/docs/stable/torchvision/transforms.html)\n",
        "\n",
        "\n",
        "we split train data to two part one is about 0.1 part of train data used as val data ,the last 0.9 part is used as train data\n",
        "\n",
        "test data only used once to check model at last\n",
        "\n",
        "so we choose 1000 for each class from 50,000 train data set  and split it to train data(0.9) and val data(0.1)\n",
        "choose 100 for each class from 10,000 test data as test data;\n",
        "\n",
        "we use val data to choose best learning rate,and use test data for accuracy and loss validation to avoid data leaking caused  validation loss overfit(val loss become much higher than lowest val loss at the end of training and training loss is very low)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKi4TfnSIcm4",
        "outputId": "0cf0110a-20a6-4ca3-a7e0-d2aa2f87e65a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/user/anaconda3/envs/312/lib/python3.12/site-packages/torchvision/datasets/cifar.py:83: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)\n",
            "  entry = pickle.load(f, encoding=\"latin1\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.4914, 0.4822, 0.4465]) tensor([0.2467, 0.2432, 0.2611])\n"
          ]
        }
      ],
      "source": [
        "import torchvision, torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = transforms.ToTensor()   # 只先转成 [0,1] tensor，不做 Normalize\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "loader = DataLoader(trainset, batch_size=100, shuffle=False, num_workers=4)\n",
        "\n",
        "mean = 0.0\n",
        "var = 0.0\n",
        "total = 0\n",
        "\n",
        "for imgs, _ in loader:\n",
        "    # imgs shape: [B, 3, 32, 32], 值在 [0,1]\n",
        "    batch_samples = imgs.size(0)\n",
        "    imgs = imgs.view(batch_samples, 3, -1)  # [B, 3, 32*32]\n",
        "    mean += imgs.mean(dim=(0, 2)) * batch_samples\n",
        "    var  += imgs.var(dim=(0, 2), unbiased=False) * batch_samples\n",
        "    total += batch_samples\n",
        "\n",
        "mean /= total\n",
        "var  /= total\n",
        "std = var.sqrt()\n",
        "\n",
        "print(mean, std)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5WmV1je_1kr",
        "outputId": "36adadf4-44b8-4210-aafc-6167f00cd0bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
            "Files already downloaded and verified\n",
            "Batch size:128,       Train batches:71,       Val batches:8,      Test batches:40\n",
            "Train class counts: Counter({'airplane': 900, 'automobile': 900, 'bird': 900, 'cat': 900, 'deer': 900, 'dog': 900, 'frog': 900, 'horse': 900, 'ship': 900, 'truck': 900})\n",
            "Val class counts: Counter({'airplane': 100, 'automobile': 100, 'bird': 100, 'cat': 100, 'deer': 100, 'dog': 100, 'frog': 100, 'horse': 100, 'ship': 100, 'truck': 100})\n",
            "Test class counts: Counter({'airplane': 500, 'automobile': 500, 'bird': 500, 'cat': 500, 'deer': 500, 'dog': 500, 'frog': 500, 'horse': 500, 'ship': 500, 'truck': 500})\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import Subset, random_split, DataLoader\n",
        "def sub_dataset(trainset, each_class_len,need_random=False):\n",
        "  subset_indices = []\n",
        "  targets = np.array(trainset.targets)\n",
        "  for c in np.unique(targets):\n",
        "    class_idx = np.where(targets == c)[0]\n",
        "    # print(class_idx.shape,class_idx[:10])\n",
        "    if need_random:\n",
        "      np.random.shuffle(class_idx)\n",
        "    subset_indices.extend(class_idx[:each_class_len])\n",
        "  # Convert subset_indices to a numpy array to allow advanced indexing\n",
        "  subset_indices = np.array(subset_indices)\n",
        "  return subset_indices\n",
        "\n",
        "def sub_sub_indices(subset_indices, val_ratio=0.1,need_random=True):\n",
        "  # train_set,val_set = random_split(train_subset,[n_train,n_val])\n",
        "  train_indices = []\n",
        "  val_indices = []\n",
        "  val_ratio = 0.1\n",
        "  subset_labels = targets[subset_indices]\n",
        "  for c in np.unique(subset_labels):\n",
        "    class_idx = np.where(subset_labels == c)[0]\n",
        "    if need_random:\n",
        "      np.random.shuffle(class_idx)\n",
        "    n_val = int(len(class_idx)*val_ratio)\n",
        "    val_indices.extend(subset_indices[class_idx[:n_val]])\n",
        "    train_indices.extend(subset_indices[class_idx[n_val:]])\n",
        "  return train_indices, val_indices\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    #transforms.Resize(40),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2467, 0.2432, 0.2611))\n",
        "])\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2467, 0.2432, 0.2611))\n",
        "])\n",
        "batch_size = global_config.batch_size\n",
        "# --- Load full CIFAR-10 train set ---\n",
        "trainset_full = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "targets = np.array(trainset_full.targets)\n",
        "# --- Class names ---\n",
        "classes = trainset_full.classes\n",
        "# classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "#            'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "print(classes)\n",
        "\n",
        "# --- (Optional) use only a subset for speed\n",
        "subset_indices = sub_dataset(trainset_full, global_config.train_samples_per_class, need_random=False)\n",
        "#train_subset = Subset(trainset_full, subset_indices)\n",
        "\n",
        "# --- Split subset_indices into train/validation ---\n",
        "train_indices, val_indices = sub_sub_indices(subset_indices, val_ratio=global_config.val_ratio, need_random=False)\n",
        "train_set = Subset(trainset_full,train_indices)\n",
        "val_set  = Subset(trainset_full, val_indices)\n",
        "train_labels = [classes[x] for x in targets[train_indices]]\n",
        "val_labels = [classes[x] for x in targets[val_indices]]\n",
        "# use all\n",
        "# train_set = trainset_full\n",
        "# train_labels = [classes[x] for x in targets]\n",
        "# --- DataLoaders ---\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True,num_workers=4)\n",
        "val_loader = DataLoader(val_set,batch_size = batch_size,shuffle=False,num_workers=4)\n",
        "# --- Test Set ---\n",
        "test_set = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform_test)\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False,num_workers=4)\n",
        "test_targets = np.array(test_set.targets)\n",
        "test_labels = [classes[x] for x in test_targets]\n",
        "# use part of test\n",
        "test_indices = sub_dataset(test_set, global_config.test_samples_per_class, need_random=False)\n",
        "test_set_sub =  Subset(test_set, test_indices)\n",
        "test_labels = [classes[x] for x in test_targets[test_indices]]\n",
        "test_loader =  DataLoader(test_set_sub,batch_size = batch_size, shuffle=False)\n",
        "#use test set as val set if val set is empty\n",
        "# that is, if val_ratio is 0, then val set is empty, we use test set as val set\n",
        "if len(val_set) == 0:\n",
        "  val_set = test_set\n",
        "  val_loader = test_loader\n",
        "  val_labels = test_labels\n",
        "\n",
        "print(f\"Batch size:{batch_size}, \\\n",
        "      Train batches:{len(train_loader)}, \\\n",
        "      Val batches:{len(val_loader)},\\\n",
        "      Test batches:{len(test_loader)}\")\n",
        "# check train and val set is balanced or not\n",
        "from collections import Counter\n",
        "print(\"Train class counts:\",Counter(train_labels))\n",
        "print(\"Val class counts:\", Counter(val_labels))\n",
        "print(\"Test class counts:\", Counter(test_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdIarqe4tnBs"
      },
      "source": [
        "### **Visualizing the Data**\n",
        "Obtaining a batch of training data and plot the same with its lables using matplotlib library. You can also see how the transformations which you applied in the previous step show up in these visualizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 637
        },
        "id": "xhhpI2ntAB8f",
        "outputId": "b5b37f16-3398-4977-87da-19b1cf330928"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
            "(3, 32, 32)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.49594647..1.5403687].\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.49594647..1.3638976].\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.49594647..1.2670603].\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.49594647..1.3241572].\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.49594647..1.5645559].\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.49594647..1.5645559].\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.49594647..1.5484309].\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.49594647..1.3877417].\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.49594647..1.0698197].\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.49594647..1.4274945].\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.49594647..1.4848417].\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.49594647..1.4831183].\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.49594647..1.419534].\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.49594647..1.362995].\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.49594647..1.5564935].\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.49594647..1.5161813].\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.49594647..1.5645559].\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.49594647..1.0062352].\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.49594647..1.5645559].\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.0905959..1.5484309].\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAB40AAAFeCAYAAACRuIkTAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQABAABJREFUeJzsnXl8VNX5/5/cTCaTyWQy2QMhhBBihMgmAu64417bIlVbi7a2+rXV+v3avd9W29r6rV1sf7a2tfbr8rWbW23dcEXcEBEBkSWEEELIvk0mk8lkcnPn90cg8DzP0QRkM/N5v168yHPmOfeee+9ZnnPOzP0kxePxOAEAAAAAAAAAAAAAAAAAAAAAAEhIrMNdAAAAAAAAAAAAAAAAAAAAAAAAAIcPbBoDAAAAAAAAAAAAAAAAAAAAAEACg01jAAAAAAAAAAAAAAAAAAAAAABIYLBpDAAAAAAAAAAAAAAAAAAAAAAACQw2jQEAAAAAAAAAAAAAAAAAAAAAIIHBpjEAAAAAAAAAAAAAAAAAAAAAACQw2DQGAAAAAAAAAAAAAAAAAAAAAIAEBpvGAAAAAAAAAAAAAAAAAAAAAACQwGDTGAAAAAAAAAAAAAAAAAAAAAAAEhhsGhu49dZbKSkp6XAXA4DDxu420N7e/qF+kyZNoquuuuojneu0006j00477SMdA4DDzWmnnUbHHHPMiH7bt2+npKQkuv/++w9+oQA4TCCOAkCDcQIkOhgbQCIw2nk0AGCIVatW0Yknnkjp6emUlJREa9euPdxFAuCIBbEUAIeeSZMm0YUXXjii3yuvvEJJSUn0yiuvDKddddVVNGnSpINXuIOI63AXAAAAAAAAAAAAAAAAAEBiMDAwQJdeeil5PB668847yev1UklJyeEuFgAAgIPM3XffTV6v9yP/EA0cPLBpDADYb6qqqsiy8MICAEZLSUkJ9fX1UUpKyuEuCgAAgCMQjBMAAAAASARqamqorq6O/vSnP9E111xzuIsDAADgEHH33XdTbm7umNo0PvXUU6mvr4/cbvfhLsoBAbs9Rwi9vb2HuwgA7DOpqakjLmqibgOwh6SkJPJ4PJScnHy4iwIAAOAIBOMEAAcOzEPAWCMej1NfX9/hLgYAB4TW1lYiIgoEAh/qh74cAADAkY5lWeTxeMbMj+vGxlV8BF5//XWaO3cueTweKisroz/+8Y9Gv4ceeojmzJlDaWlplJ2dTZdddhnV19crv5UrV9K5555LmZmZ5PV6acGCBfTGG28wn90aBBs3bqQrrriCsrKy6OSTTz4o1wfAR6G9vZ0WL15Mfr+fcnJy6Gtf+xpFo9Hhz6Wm8f33309JSUm0fPlyuv766yk/P58mTJgw/Pk999xDZWVllJaWRvPmzaPXXnvtUF4OAPtNT08P3XTTTTRp0iRKTU2l/Px8Ovvss+ndd99lfhs3bqTTTz+dvF4vFRUV0R133ME+N2lVXnXVVeTz+Wjbtm20cOFCSk9Pp/Hjx9OPfvQjisfjh+LyANhvRhNH2bZNP/7xj6msrIxSU1Np0qRJ9N3vfpf6+/uZn+M4dOutt9L48ePJ6/XS6aefThs3blRjDQBHIhgnANgD5tgg0QkGg3TVVVdRIBCgzMxMuvrqqykSiQx/PtrYaLeO3nPPPUfHHXccpaWlDbenF154gU4++WQKBALk8/mooqKCvvvd77L8/f39dMstt9CUKVMoNTWViouL6Zvf/KY6DwCHmquuuooWLFhARESXXnopJSUl0WmnnTYc89TU1ND5559PGRkZ9NnPfpaIhjaPb775ZiouLqbU1FSqqKigX/ziFyoW6uvroxtvvJFyc3MpIyODLr74YmpoaKCkpCS69dZbD/WlArBfYJ4NjhTq6uro+uuvp4qKCkpLS6OcnBy69NJLafv27czvgzS3d+8V7PafNGkSbdiwgZYvX05JSUnD/f9utm3bRpdeeillZ2eT1+ul448/np5++ml2zN36wQ8//DD98Ic/pKKiIsrIyKBFixZRd3c39ff300033UT5+fnk8/no6quvVu1itO1nN88//zzNmjWLPB4PTZs2jR5//HFjmfbWNDbhOA79+te/psrKSvJ4PFRQUEDXXnstdXV1fWi+Q01Cv556/fr1dM4551BeXh7deuutZNs23XLLLVRQUMD8fvKTn9D3v/99Wrx4MV1zzTXU1tZGd911F5166qm0Zs2a4W/Fvfzyy3TeeefRnDlz6JZbbiHLsui+++6jM844g1577TWaN28eO+6ll15K5eXl9NOf/hQLPuCIZPHixTRp0iS6/fbb6a233qL/9//+H3V1ddGDDz74ofmuv/56ysvLox/84AfD3wr985//TNdeey2deOKJdNNNN9G2bdvo4osvpuzsbCouLj4UlwPAfnPdddfRo48+Sl/96ldp2rRp1NHRQa+//jpt2rSJjj32WCIi6urqonPPPZc+9alP0eLFi+nRRx+lb33rWzR9+nQ677zzPvT4g4ODdO6559Lxxx9Pd9xxBy1dupRuueUWsm2bfvSjHx2KSwRgnxltHHXNNdfQAw88QIsWLaKbb76ZVq5cSbfffjtt2rSJ/vnPfw77fec736E77riDLrroIlq4cCGtW7eOFi5cyL6sBMCRCsYJAIbAHBuAoXl0aWkp3X777fTuu+/SvffeS/n5+fSzn/2MiEYfGxENSUJdfvnldO2119KXvvQlqqiooA0bNtCFF15IM2bMoB/96EeUmppKW7duZV+mcByHLr74Ynr99dfpy1/+Mk2dOpXWr19Pd955J23ZsoWeeOKJQ3lLAGBce+21VFRURD/96U/pxhtvpLlz51JBQQH95S9/Idu2aeHChXTyySfTL37xC/J6vRSPx+niiy+mZcuW0Re/+EWaNWsWPffcc/SNb3yDGhoa6M477xw+9lVXXUUPP/wwXXnllXT88cfT8uXL6YILLjiMVwvAvoF5NjiSWLVqFb355pt02WWX0YQJE2j79u30+9//nk477TTauHEjeb3efTrer3/9a7rhhhvI5/PR9773PSKi4brd0tJCJ554IkUiEbrxxhspJyeHHnjgAbr44ovp0UcfpU9+8pPsWLfffjulpaXRt7/9bdq6dSvdddddlJKSQpZlUVdXF91666301ltv0f3330+lpaX0gx/8YDjvvsRi1dXV9JnPfIauu+46WrJkCd1333106aWX0tKlS+nss8/ep+u/9tpr6f7776err76abrzxRqqtraXf/va3tGbNGnrjjTeOHJmqeAJzySWXxD0eT7yurm44bePGjfHk5OT47luzffv2eHJycvwnP/kJy7t+/fq4y+UaTnccJ15eXh5fuHBh3HGcYb9IJBIvLS2Nn3322cNpt9xyS5yI4pdffvnBvDwA9pvddfTiiy9m6ddff32ciOLr1q2Lx+PxeElJSXzJkiXDn993331xIoqffPLJcdu2h9NjsVg8Pz8/PmvWrHh/f/9w+j333BMnoviCBQsO6vUA8FHJzMyMf+UrX/nAzxcsWBAnoviDDz44nNbf3x8vLCyMf/rTnx5Oq62tjRNR/L777htOW7JkSZyI4jfccMNwmuM48QsuuCDudrvjbW1tB/ZiADhAjCaOWrt2bZyI4tdccw3L+/Wvfz1ORPGXX345Ho/H483NzXGXyxW/5JJLmN+tt94aJyI21gBwJIJxAoAhMMcGiczueviFL3yBpX/yk5+M5+TkxOPx0cdG8fjQfJuI4kuXLmW+d955Z5yIPrT//7//+7+4ZVnx1157jaX/4Q9/iBNR/I033tivawTgQLFs2bI4EcUfeeSR4bTdMc+3v/1t5vvEE0/EiSh+2223sfRFixbFk5KS4lu3bo3H4/H46tWr40QUv+mmm5jfVVddFSei+C233HJwLgaAAwjm2eBIIhKJqLQVK1aoue3uGEiye6+gtrZ2OK2ystK4F3DTTTfFiYjFLj09PfHS0tL4pEmT4oODg/F4fM/4ccwxx8Rjsdiw7+WXXx5PSkqKn3feeey4J5xwQrykpGTY3p9Y7LHHHhtO6+7ujo8bNy4+e/bs4bTdZVq2bNlw2pIlS9h5X3vttTgRxf/yl7+w8y5dutSYfjhJ2NdTDw4O0nPPPUeXXHIJTZw4cTh96tSptHDhwmH78ccfJ8dxaPHixdTe3j78r7CwkMrLy2nZsmVERLR27Vqqrq6mK664gjo6Oob9ent76cwzz6RXX32VHMdhZbjuuusOzcUCsJ985StfYfYNN9xARETPPPPMh+b70pe+xLT43nnnHWptbaXrrruOCcJfddVVlJmZeQBLDMDBIRAI0MqVK6mxsfEDfXw+H33uc58btt1uN82bN4+2bds2qnN89atfHf47KSmJvvrVr1IsFqMXX3xx/wsOwEFitHHU7vHiv/7rv1j+m2++mYho+DVDL730Etm2Tddffz3z2z3uAHCkg3ECAMyxAdiNrIennHIKdXR0UCgUGnVstJvS0lLWfoj2aMD+61//Um1gN4888ghNnTqVjj76aNbOzjjjDCKi4XYGwJHIf/zHfzD7mWeeoeTkZLrxxhtZ+s0330zxeJyeffZZIiJaunQpERHmFOBjC+bZ4EgjLS1t+O+BgQHq6OigKVOmUCAQUFJMH5VnnnmG5s2bxyRmfD4fffnLX6bt27fTxo0bmf/nP/959svc+fPnUzwepy984QvMb/78+VRfX0+2bQ+fh2j0sdj48ePZr5z9fj99/vOfpzVr1lBzc/Oor++RRx6hzMxMOvvss1lsNmfOHPL5fEdUbJawm8ZtbW3U19dH5eXl6rOKiorhv6urqykej1N5eTnl5eWxf5s2baLW1tZhPyKiJUuWKL97772X+vv7qbu7m52ntLT0IF4hAB8d2T7KysrIsiylWyCRdbuurs54vJSUFJo8efJHLygAB5k77riD3n//fSouLqZ58+bRrbfeqhb5J0yYoPQ7srKyRqVLYVmWagtHHXUUEdGI7Q2Aw8Fo46i6ujqyLIumTJnCfAoLCykQCAyPD7v/l37Z2dmUlZV1oIsPwAEH4wQAmGMDsJu9F/qJaDiW6erqGnVstBtTnf7MZz5DJ510El1zzTVUUFBAl112GT388MNsA7m6upo2bNig2s7usWN3OwPgSMPlctGECRNYWl1dHY0fP54yMjJY+tSpU4c/3/2/ZVmq3cj2BsCRCubZ4Eijr6+PfvCDHwzryefm5lJeXh4Fg0EVh39U6urqWD3fjezrdyPjrd0/TJMymJmZmeQ4znB59zUWmzJliprH789cvLq6mrq7uyk/P1/FZ+Fw+IiKzRJa03g0OI5DSUlJ9Oyzz7JfTu7G5/MN+xER/fznP6dZs2YZj7Xbdzd7f1MDgI8DJkF7E6jbYKyxePFiOuWUU+if//wnPf/88/Tzn/+cfvazn9Hjjz8+rENpGiOICHp6ANDoxw8APq5gnABg9GCODcY6o+nvP8rcOi0tjV599VVatmwZPf3007R06VL6xz/+QWeccQY9//zzlJycTI7j0PTp0+lXv/qV8bhyQRWAI4XU1FSyrIT9jRMA+wTm2eBgc8MNN9B9991HN910E51wwgmUmZlJSUlJdNlll7Evq31QXRwcHDxoZfugeGu08+5D3X4cx6H8/Hz6y1/+Yvw8Ly/vkJbnw0jYTeO8vDxKS0sb/vby3lRVVQ3/XVZWRvF4nEpLS4e/QWCirKyMiIZ+nn7WWWcd+AIDcBiorq5m39DcunUrOY5DkyZN2qfjlJSUDB9v9+uwiIZea1FbW0szZ848IOUF4GAybtw4uv766+n666+n1tZWOvbYY+knP/nJ8GbAR8FxHNq2bRsbZ7Zs2UJEtM/tDYBDwWjjqJKSEnIch6qrq4e/HUpE1NLSQsFgcHh82P3/1q1b2bjT0dExql9hAnAkgHECJDqYYwMwMqONjUbCsiw688wz6cwzz6Rf/epX9NOf/pS+973v0bJly+iss86isrIyWrduHZ155pnYVAAfe0pKSujFF1+knp4e9mvjzZs3D3+++3/Hcai2tpb9UnPr1q2HtsAA7CeYZ4MjjUcffZSWLFlCv/zlL4fTotEoBYNB5rf7l+vBYHBYRoNI/zqY6IM3a0tKSlg9343s6z8q+xqLbd26leLxOCv3/szFy8rK6MUXX6STTjrpiP+ia8J+dSs5OZkWLlxITzzxBO3YsWM4fdOmTfTcc88N25/61KcoOTmZfvjDH6pvI8Tjcero6CAiojlz5lBZWRn94he/oHA4rM7X1tZ2kK4EgIPH7373O2bfddddRET7vPh53HHHUV5eHv3hD3+gWCw2nH7//ferQQaAI43BwUH1ypX8/HwaP3489ff3H7Dz/Pa3vx3+Ox6P029/+1tKSUmhM88884CdA4ADxWjjqPPPP5+IiH7961+z/Lt/9XLBBRcQEdGZZ55JLpeLfv/73zO/vdsFAEcqGCcAGAJzbABGZrSx0YfR2dmp0nb/Gn/3uLN48WJqaGigP/3pT8q3r6+Pent796XYABxWzj//fBocHFRzgzvvvJOSkpKG16h2a77efffdzG/3WhYARzqYZ4MjjeTkZBWv33XXXeoXxLu/7Pnqq68Op/X29tIDDzygjpmenm7cDzj//PPp7bffphUrVrBj3HPPPTRp0iSaNm3aR7kUdh6i0cdijY2N9M9//nPYDoVC9OCDD9KsWbOosLBw1OddvHgxDQ4O0o9//GP1mW3bR9QeScL+0piI6Ic//CEtXbqUTjnlFLr++uvJtm266667qLKykt577z0iGqrwt912G33nO9+h7du30yWXXEIZGRlUW1tL//znP+nLX/4yff3rXyfLsujee++l8847jyorK+nqq6+moqIiamhooGXLlpHf76cnn3zyMF8xAPtGbW0tXXzxxXTuuefSihUr6KGHHqIrrrhin38ZnJKSQrfddhtde+21dMYZZ9BnPvMZqq2tpfvuuw+axuCIp6enhyZMmECLFi2imTNnks/noxdffJFWrVrFvmn3UfB4PLR06VJasmQJzZ8/n5599ll6+umn6bvf/e4R9XoSAPZmNHHUzJkzacmSJXTPPfdQMBikBQsW0Ntvv00PPPAAXXLJJXT66acTEVFBQQF97Wtfo1/+8pfD4866devo2WefpdzcXPxCBhzRYJwAYA+YYwPw4Yw2NvowfvSjH9Grr75KF1xwAZWUlFBrayvdfffdNGHCBDr55JOJiOjKK6+khx9+mK677jpatmwZnXTSSTQ4OEibN2+mhx9+mJ577jk67rjjDvblAnBAuOiii+j000+n733ve7R9+3aaOXMmPf/88/Svf/2LbrrppuHNijlz5tCnP/1p+vWvf00dHR10/PHH0/Lly4d/EYY5Bfg4gHk2OJK48MIL6f/+7/8oMzOTpk2bRitWrKAXX3yRcnJymN8555xDEydOpC9+8Yv0jW98g5KTk+l///d/KS8vj30Bgmior/79739Pt912G02ZMoXy8/PpjDPOoG9/+9v0t7/9jc477zy68cYbKTs7mx544AGqra2lxx577IBJF+xrLHbUUUfRF7/4RVq1ahUVFBTQ//7v/1JLSwvdd999+3TeBQsW0LXXXku33347rV27ls455xxKSUmh6upqeuSRR+g3v/kNLVq06IBc40cmnuAsX748PmfOnLjb7Y5Pnjw5/oc//CF+yy23xOWteeyxx+Inn3xyPD09PZ6enh4/+uij41/5ylfiVVVVzG/NmjXxT33qU/GcnJx4ampqvKSkJL548eL4Sy+9NOyz+/htbW2H5BoB2Fd219GNGzfGFy1aFM/IyIhnZWXFv/rVr8b7+vqG/UpKSuJLliwZtu+77744EcVXrVplPO7dd98dLy0tjaempsaPO+64+KuvvhpfsGBBfMGCBQf5igDYf/r7++Pf+MY34jNnzoxnZGTE09PT4zNnzozffffdwz4LFiyIV1ZWqrxLliyJl5SUDNu1tbVxIorfd999zCc9PT1eU1MTP+ecc+JerzdeUFAQv+WWW+KDg4MH89IA+MiMJo4aGBiI//CHP4yXlpbGU1JS4sXFxfHvfOc78Wg0yo5l23b8+9//frywsDCelpYWP+OMM+KbNm2K5+TkxK+77rpDfWkAjBqMEwBwMMcGicoH1cPd8+Ta2tp4PD762KikpCR+wQUXqPO89NJL8U984hPx8ePHx91ud3z8+PHxyy+/PL5lyxbmF4vF4j/72c/ilZWV8dTU1HhWVlZ8zpw58R/+8Ifx7u7uA3vxAOwjy5YtixNR/JFHHhlO2x3zmOjp6Yn/53/+Z3z8+PHxlJSUeHl5efznP/953HEc5tfb2xv/yle+Es/Ozo77fL74JZdcEq+qqooTUfx//ud/Duo1AXCgwDwbHCl0dXXFr7766nhubm7c5/PFFy5cGN+8ebPaE4jH4/HVq1fH58+fH3e73fGJEyfGf/WrX6kYKB6Px5ubm+MXXHBBPCMjI05EbF+gpqYmvmjRonggEIh7PJ74vHnz4k899RQ7j2n8iMc/eF/CFJ/tayz23HPPxWfMmBFPTU2NH3300ercu8u0bNmy4TQ519/NPffcE58zZ048LS0tnpGREZ8+fXr8m9/8ZryxsVH5Hi6S4nHx+3IAAAAAHDKuuuoqevTRR42vXQQg0QkGg5SVlUW33XYbfe973zvcxQHgsIBxAgAAAABg/1m7di3Nnj2bHnroIfrsZz97uIsDwGEH82wAwIeRsJrGAAAAAADgyKGvr0+l7daYOe200w5tYQAAAAAAAAAfOz5oTmFZFp166qmHoUQAHF4wzwYA7CsJrWkMAAAAAACODP7xj3/Q/fffT+effz75fD56/fXX6W9/+xudc845dNJJJx3u4gEAAAAAAACOcO644w5avXo1nX766eRyuejZZ5+lZ599lr785S9TcXHx4S4eAIcczLMBAPsKNo0BAAAAAMBhZ8aMGeRyueiOO+6gUChEBQUF9LWvfY1uu+22w100AAAAAAAAwMeAE088kV544QX68Y9/TOFwmCZOnEi33norXsELEhbMswEA+wo0jQEAAAAAAAAAAAAAAAAAAAAAIIGBpjEAAAAAAAAAAAAAAAAAAAAAACQw2DQGAAAAAAAAAAAAAAAAAAAAAIAEZlSaxo7jUGNjI2VkZFBSUtLBLhP4mBOPx6mnp4fGjx9PljU2v5eANgH2BbQJADhoEwBw0CYA4KBNAMBBmwCAgzYBAAdtAgAO2gQAnH1pE6PaNG5sbKTi4uIDUjiQONTX19OECRMOdzEOCmgTYH9AmwCAgzYBAAdtAgAO2gQAHLQJADhoEwBw0CYA4KBNAMAZTZsY1aZxRkYGERFNmj6erOShXWhPpl/5RR2H2T09YWZ/8qJzVJ6/3/dXZtuRqPJZeMVZzN4R3M7sVH5aIiKaXXkCs//v909wB7tX5Tn2lBJmT51zlPLJySpidnpqLrPffX+jyvPyv9cyu2PLTuUzFtldb8Yiu6+tvv4O8vvTdqW6DZ62sOW3OEx5YsIOGHzShd0hbJ8hjyQ0okf3QDM/S7Rb+YSdfmbHLN6GszKyVZ5kUb6+QX5cV7KhUYt710cDyqNnIMLsaDTOj9Av7xtRb5SXtzfGr+fLs//XUJb9IxHaBAD7wliuN2P52sDBYyzXm7F8beDgMZbrzVi+NnDwGMv1ZrTX1t2t56SjyMWsl/7nVuXRunkLs086/hRmexqqVJ78PLGklpLC7WkzdFEWfEkkJGsfMCKhUIiKi4vRJgAQjOV6s7/X9vgf/8bs2bOmK58Uj4fZjZtqlM8r/3ic2Rtfe43ZqT3tKk+8n6ctmDaL2ecdp8cJh/i6ZLSkSPm45sznPjl849A0VnrT0pjtxPnaayQyqPL09/M16r6+fuXj2Hztu7mb77k0tPO1ZSKiGdYOfgxxnx5crsfcxoYuZre2tSqflv6+4b/jRBSkxGgTa+ufpAz/rjXvfr1evn1TI7P/9M0/MjurvkXlsRprmT2g9iKI3hdbi0FKZXZ51iSV51f3LGF2JIXXn7sf5WUjIvKX8vZ5wsyzlM9AlMdgmcV87/LppWtVnoXn8r3K0hK+kerxBFSe7bW8rjY3BJVPJMyvqbWN3/9UfjlERBRs5ns7scEeZltJehv3p7ffpQ80CkbTJka1abz75+1WskXJuzaNk106sE12+M/gd28w78bt0RtkSVaSsPX5U1J5MV1ufm6XYX8pVZxLnoekTUSuFH7yVE+K8vGk8eN6PLwxpKTqPPI+JApj+bUIu6/N70/7iJvGqaSRbctr8JEddZ+wTXkksmya+ADvxWJu/aUOEu0vZvGNWl+G7gldxNOSB/lxR7NpnGyQZHcGeICT7ObHsaL6fsdTeHmdWFz5HCgSoU0AsC+M5Xozlq8NHDzGcr0Zy9cGDh5jud6M5WsDB4+xXG9Ge21+v/4Bw8jwOV66YW0qLYWvO2Wk8Tmrx7DW4/fITWNx3HS+QD+USZYfm8YfBbQJADhjud7s77Wlp/E10gyf3jBxiz4/5NUbZGkpfE3RbfH+O9WwqeEQL7M3mefxu/V45Igxy+3Ra5mudH5Nbh//cY7cyCUi8np5Hkf8+C85SedJSU4RPnorSZ7LK9ZV0zx6LEy3+DU5Yk3dbdh3conX6SYb6sPeHruvLhHaRIY/nTL8u+pAv/wxGpHPJ+qLiz/HVMOrii1Rd5PIsJcm0uRafYqhvmR4eX1IdvP6k+rWzz7Vw9PS03WbiFm8DqX7eJuWe4VDPqJv8PN27/HoH+X5fHwPxpuu7zc5/Jo84gsbHsOmcVS0c8vmx7WsUW3jjorRtInE3M0EAAAAAAAAAAAAAAAAAAAAAABARKP8pfFutq3bSbu/QFA8e6L6PL84h9nTj+Wvdz773Hkqz7OPP8XsmjX61c1uymR2ay3/uXYgTW/Pb13Hf0Kfnc5fkTtz9rEqz6VLFjL7V7+/V/k01r/NbLnr3lCvX41AhqTDxU1f+QSzl7+9ldlrVm04lMUZA1i0pxaYmpNMk7/U1a9J1z6aQeKvKAiKV0371S+PiVKMv2r+cLwpvE0HUvQxcsW3wVyiVXgpy3BknsdO5q+ytwyvno6JnzRHDb+U7kzhr19ps/h9CUd0HjvKjxvrG/kX2AAAMFru/v6plLbrVyi+cbo/rGrgcc/a+gZmO0X6zQsrX+Wvtml46aOW8shHjj7Tpmqf4gr+LWiXS387fJqfS5HkpPNx7rl33lN51tXz1w994sQAs/0e/YzueHBkCQgwOv57hXgDiOFbuS7xpWG3/jEYyRf/yC9Tuw1hnEwTX1YmlyFPTHzR2PTulJhIlD8EMHzRW6VJF9twIsd08hHyySyj+YaxI5xMkdRoyrI30Z4Q3TI1c2RHAEACsl7YbcLOUzl61r7L7NfeWK18ysYXMru7m78Gs65quy5KDe/0C4r4ueve0nHFxA089ks681x93NICbrvEWJhi+oWI/DWMjDtNElaj+fWVnJtHRvicSEtuyQHTNLrIhTPT29P2ztdj+BwAADTlFXx/IhrRr+KtW8b76/efeUv5rHrmRWbb1MTP4+FzSyIit5v/qnn9+veZPcmrf/U86zS+f9JY36V86mNrmV10Gj+34YfGFInwvln+0tg2TChcasJjeD21mEFI2zL8WtO2+Hg5tYifZ86aZ1WeaDWfl5927lzl07mXZGL/wCD9/kktIzoWeejZJ8njHVo1Gb9d/7I70sDv7yfyeB2bUrtZ5YmKibdleMNqo4gJHOLrLTsKtN5yXrbYkzt+JjPvPEvKeBwYmhueUGmnHnuJSOH1u1fFmERZmTzucRviFTvG7/f0ynJmBzL1/kpbC9/vLCnlbTorS9/LH9zyC5V2oMAvjQEAAAAAAAAAAAAAAAAAAAAAIIHBpjEAAAAAAAAAAAAAAAAAAAAAACQw2DQGAAAAAAAAAAAAAAAAAAAAAIAEBpvGAAAAAAAAAAAAAAAAAAAAAACQwEg18w8nvusfEdWv36E+/uTis5l95oXTmH3CtKNVnvwCLnxe4+xUPrXrG5j96XMuYXZBZkDl+ekP/sDsSJiLVGfnasHpDe/VMzvYpm9Pbs4UZm+vWcsdtGb8IWPO/DOYvb3mLeWzteZ9Zne29xzUMo11tnXVUcbgUF3yerPV59mpXHw+Rn3CI1nlcQkB9TTSAvbJFGB2Dk0SHjFjefdmcKCXHzNF1/cUyhDn6Vc+I1d6XX59Hn7uOAWVT4R4XQ1Sr/KJks3tEG/37U2tKo/txJkdCulzAwDA/rKpp5lSY0N9fTC6TX3e3tnCbJfF45NCt0/l+e5/nMrsuuN037yxmsdOGzbx+Kp204cU+ghEXuEaQ/nXbBoQKUHlUzeff1/ypHF8vFy6on3Esmxp4MedMG7ELOAj4JXhiSHsSBG22/C1WJc4jmV9+OdERG4Pt22Rx+Zhx64Df/h5iIi84hosN7dHM52QhzWVxREHMh03dgDmLo68ZpPPPp7H2bdZKgAgoYgKOyjsdJUj0trEbLtHz5eDQT6/bBV5YoZ+dkU9j7fOzs1itiscVnlW/vNhZme+/77y8aTwnjTSF2J27rhMlcef7Wd22hkzuENJjspD7RFuN+g5NkV5B97S0cHsqpVrVZYyH78P43P5ul9SgRhgiYjKxOhRUaxcOqw9D6EnJOsBAACYaanZyuxI1dvKZ+eLy5m98g3dN9dQF7NziK8ndkd1vzTOw/u7Ai/vq9eu26zybGttY/aK7ojysSuPYfZnZx/P7LR0vRZri3i8r1eUN1lH8ZaYzMQMM4qoHCBFHpel+/xtnXyNt7KEr4Pc+IVPqDxHlVQzO+jxK5+qrj0LBdEDMdH5mLD4xM9Rhn9o7WjVQw+qzx9bz+uZq4GP5TNCW1SeRuL1LmycTfJnHRQrN6FN3bosp/8Xs/tL+H7hTT+7WeX55LmTmZ2qt/VGpLl55L2SfrH30NzQpnx6uvh9KcjRC0K52TwtJW3kvZGiopE85J7SwQW/NAYAAAAAAAAAAAAAAAAAAAAAgAQGm8YAAAAAAAAAAAAAAAAAAAAAAJDAYNMYAAAAAAAAAAAAAAAAAAAAAAASmP1XizK8Bvy9lVXMLqrg7+tetXKjyrP2zTUjniqzX7wzvZ3rrDy3TB+jq1Nox4hDrH6Nl5WIaMdOvoeePSlL+XgCXC/vqHL+jvJPXsx1nYmI2ut5nl/f/mflcyDYvoFrNEypzFc+48u4NqLl4i9Mr6vV75rXurtaSyFRGchKp5h/SJvBS7q+RMX3MlzERevSDXmIpCZDknbpF9/3cMRxOg1aRG6u7RRr4DqazWGtJ+4rCTI7r7hEH1e8U3+QuPZTslHTmN+HPqELEiWt/RQW5+nuDSkfy8OP6/VwlcPCPK13EY7y+uwyaGIAAMD+kltxNHnShvqi9jatixRq433bMcW8P3dHdH/ojXCdmPlzxysffzEfuyeJmOzPHVwPiIioT8u+jzlWr+zkNnV+gOcH8/IbIkHLToMDSJmQCAobJAyVupJJn9j94bZJ01hOd6JCrsttyCM1jE3f0HWNcG6pN0ZE6iKlRrC8HiKtc2zSPZbZpD7xaJBlMUk9q/KPcMwkaBoDAD6QOmHLNQzdC22qqWG2y6s7zboGvp7S1cT1/9wRvQ6SGeHn3tHKyza1VOvytlVtZ3ZtlV6bsknoPzr8mixblz/axOND17aXeB6P7uDDvfy43Z16kM3M5nrEVMADn0iHzlPbwTUAgx18RC0MBlSeHLk2Up6hfTx7zpViGPcAAMDE2oe5lnx+jZ4Le9yDzN4o1imH0riGcaE8hkFzNBrlfa9ccXQZ1j8bGnhap9erfGYfW87sCPG+2O3i66FERAMiaO8UY5jHo8/jiMmBY4jiHTEpcotJh9+widQZ5WvdT6/ga9Qzpuo1juwTZjN7y/rVyifqqh/+u98ZVJ+PVdr/1UzRtKF1/+3PaH1iOo3v/9jncE3s+35Tq7LUklwgKlU+OW5eDztizwuPFpIU+y5jdmDCLGZ//Td/VXlmn/bfzK7YD03jyeUjigZTKmUyu6woVzuNfJj9o4/rKccd3p/YxkWCgwd+aQwAAAAAAAAAAAAAAAAAAAAAAAkMNo0BAAAAAAAAAAAAAAAAAAAAACCBwaYxAAAAAAAAAAAAAAAAAAAAAAAkMPumFpWcTpS0653ztn7n/vKlK5ndFe1g9rqlm/QxR/E67miUv2P/3bfeYfaKdVqL4MJF5zK7tpZr2GyvblB52mq5ho0vf5byaW3ieq2F6fxd5w5//TgREYU7giJFatTG6UDQEd7B7Cku/f5/y8eFX9q624SH6aXwWjsWDJFDfsrYpdnrMdw7l3i2aSQ1JQxCiAM53A4avtvR0s7tVKHj5OP1cqiA/D38aUXZzC6t1lq+1W+tZXa05z3lkz2N6y25hYZUstLEJiKh0eQZxfdXHNFdxdJ199XRw++DSwkU6n6LiGs/+TLwXZpDycybuYaXHdV63LFePpYUV8xWPlIQ0kVcS96dquuhT4hKBjziGG5dFyyhC2OQlqEc0YSVdqWQKBs6MDcdIfli0qrMFs0811CWHULybdkGbtcYmsSOFq6bEQ6LZ2JpnXVb3Du3pQvssfb4ONEQNf3EoA0yBgk2bqZUz1D9m5yj70tMjAuhTj7mnn7WLJUn3MbbxGsvrlc+L73BdffqaoSDQRdW1sPpZdzOm6grWX2InyemJWtISMdTi5Q0+riHGaahBRwwjhaaQUE9TCid45hhKJd9sURqERMRecQ8xSt8TDrI6rgmH6l7bAqVBFKSSxbXdIyokFUzaQ2PqGE8irBIyiuNRm1J6iBLkrQM25ilu7ub/H4dh4ODQ82rf2d21aZ1zG406LNe8907P/J5k5J0/AT2kyahM+nmnV1Pk9Ye3lbLfcYV5ymfNavfZ/bME7je3wnz5qo8LzzwELM3NfA1maMNmsYlEycwe/PKzcrH5eODRyAzwOy+oNaHVMt7Lt7r94p5LxGR48h6qQfLnna+yJWezDtwv2GAjYlmVNfMn1l/ROtM5vjTxUEMk6acvSZAA/o5j1Ve6s4kn3/oWTU3BNXn+bn8OZan6vWgVuL58sXnrzWRYp2QuNyul1GpvZ3PDyaWB3hZ8uWaI5HVyefqXYaYQMYnbkust9k68BHS35Tq6LppibWovgbdllJredokL187a0/X11Tr4yffoTyIIlF+oU4vv8iok6byuNw8jydNtwu3e0gPfKBvkP51wzbDmUHN23wfIVtOHohoh5vXyzbDWr2cDcsw39QrtYsIvJX4ZMZn0EH2x3ggPKFUi6hOLuQLT3kZvG64DJG/I67Ja3Efj2HiIuP8LVVa+3bd+o3Mbq3nnUWwWS8SdHTwNY1UD2+vhUUFKk9u2WTuU5qhfLJy9tyraP8AEb2vfMYiK295itJ29ZPrwhvV50889cgIRzDtB00Uthw5iDrUxFvuRel62JzDx/trv/slZr/+5lMqz3U/vIvZf7j9CuWTn8pjjSwax+xzTztJ5dEcoEloH+9PBsQ+qmWaMTu8/TliUIOmMQAAAAAAAAAAAAAAAAAAAAAAgEMGNo0BAAAAAAAAAAAAAAAAAAAAACCBwaYxAAAAAAAAAAAAAAAAAAAAAAAkMNg0BgAAAAAAAAAAAAAAAAAAAACABEYrnH8Yg70f+nE8wkWe1z2zaZ8LlDE+V6XZmVwMfbCfCz8H8gIqTyCQyezzLjyd2e+t0ULozz/xJrNrV21QPsl+vs8+YeYUZj/9xBqVZ+WKlSptb84pr9Blqa760DxEREmBbGZfeeV5zP704mkqz466t5n9h5/9a8TzgA8mTJ2URB4iIrIN4u5u8jE72sWbXJYrTx/UyuG2y/Ddjs4mZvYQF3vPyDWIo3eL9pvHBeGpZJLKUu7lba+m/T3ls+ZV3s7nnFrGbIt0v5FMfcyOUA+zwxRReULEReP7BvuUjyWegdvF738k2q3yuD38GkPRsPIBBw9XlNcPq1/XXXfeUcz2pvmUj2XF+HHdacz2eNNVHp+H27Zowm6vykLZWdxO10UhtziuPLXuKYhcYjR2i8/dhm7ALfI4hmYvz+XxczvVUBY/8Xvn9nI7FI2rPJYscExfZTQaHf7b6Y+qz8csnnoiTxIREdkx/fGEIv5wI+F+ZodadB/a1sDtZ5frPrNJhmDi2c+8WJelWNT5Kb4JzC4aV6DyHDWTjyWtoTrlU9/Rwez3dzQyu6VGl+XdZ7jdZ6jfClUPR5FnpGPs73HAASNF2F7DM7Jkf2ia4SSL44iPR/Pope0MGjKNcF4i8ziwP8dhZTGkedKEz4D2scX4MpqmJockW2QyHkP6mJz2OrBjeiAAjMDrz7/I7JPPOUv51NeuYvZ9v/kVs4O2bmznXf45ZheVztnfIoIDwBN/fJbZhcV8XcRxTVR5YnYGsxtaNyqf8srxzD7706cwu2jqMSpPS0cLs9956DFmr6japvKUFE5mdiSmO8R8Mdjl54tJiCE26WgL8ePafL4cS9WZvF4+GmZne5SPJXr9gOifex193KjF81hi4tIZ5GUlIqpfv5nZxS8qF6JFJ+/5u6/f4DBWsYhoaD4RCOhP5VzS9Osgw9T2Q49hOlCnnnKQ5eV1xuvl9bk7ooOPmKgy7YZTexweAZa5eBvwd+l247N5WTplgEJE4SZe9zJ26nWlaR4+Bzqnch6zX17xnMqz3s2PE52j77hLNK8JPr6YYItrJCKyRB5fuj6uMzA0q++P2ESk+xxA1NzJ59Q7DOusbRGeViEn0ERkibXMKPH6nb2rnXJ45L9DHOMY0otKYlWYXB09ymfFQ4/z49by8ejoaXwdjYgo2MXXRHfW84WFlkZ9nsYGPpdftf5d5bOd+PxervYY18BGsN3N63Wm1XwFa1r+OOVy9oV7xth+w7rUWGVyZz+l09Ba3XK3HpfniiQxTaRXyTSmyuPUG3zk+o/sd/UzGKh6mtl//tssZs85rljl6QhOZfZDj+u+7seXzzWU78ATF/ujTlSvbzpiPEpx8+BpIKbHHhLxoCPupX2I6zN+aQwAAAAAAAAAAAAAAAAAAAAAAAkMNo0BAAAAAAAAAAAAAAAAAAAAACCBwaYxAAAAAAAAAAAAAAAAAAAAAAAkMPumaXwI6GnUahb5nzyBJ/Tx936/8ZrWTv7XE/z96HOP5/ozVVVcL8VEmlfrF/QF2/i5l78mPEYWN5PaBJefXqZ8VgpNY63ESpRfxPUugr1cZ2DDOq0/21jDtZxv+s5FzP717U8azgQ+iD6KUPKuZ+6lDPV5pI03MbuKa2RlzeF6RkRE1Mr1ije9vFy5dLTxelhSVsTsHTVC8JKIeiO83cw7fyF38MuaSURTuWZ3WaPW+Op4r5PZq9/ayuzpxxeqPF6h+xGhILNtQ9fUJ3Q/nGQtqFOUwfU2Y+J7Mbm5XOuciCgkNHXDvabWBg4W0QH+HBtrdN/sL+D12+nSAqi+DK4DE+7hOt+WS6v35uXlMrulnve7BQVaczyQz8tCBg0vW+hOHHUs15cvLVdZ1De43FIGxKBbFhOZooYR3RaaYzEhDmqQkFZ6oba4dR7LoNMjdTN7pAopUWfnnmcSTyBN4+fe7qPkXbfD9E09KTfjErdm+VtrVJ5W4dNSq4+bJOSu5p3G7XQd4pAj8jSP4/37zpCuMF4Xb8NWke5DM0WFdnhTo5x8XZbP3MztWBdv4+PHibZIRKEQ78+31O1UPt3iElwBbhdoWSR66d/c7tNhJziI9IjQ2jE0JClLZhn0f2U2qXRvktCVI4dUejJIn45Kr1grRnJME7SRjmuSCJZ55BhgyjeSTaTLJ++3SQZ8NPrP7FxH3Cz18NFXw+ebnXU6VmoTuqoRIRgZi+on6fbwfjVnnO4AK+YKbbCMacrnSOLO3/yG2RvfelD5BGyuvWbxoYOOKtO9wV/+yI/7zf/RxwWHjh11W5g9seQ4Zm+u1Tp3VWJ+HPDp53zqWccz2y3Ee5vEegsR0fRzFjA7VMvn8muXr1Z5ojafk9qGESiQydNy8niwHeo1aLWGeVq0l48CprHHLXr5VMNoE8jkI2ZBNg8YI716YG7u5LqYXh+fV9mGkWJDLX+u3jd1TJlTsVf8F06c+UScHHJ2aaX60qUSJRGJtRKbtFairGXyqfkMc4MMkSbnCkREPjF5cRN/9pYhbpO119sd0E6t3Ku3h9cZX1i3m5jQe/T59BqAXcN9js3Tuq/HT+d9CjWFmels1NqfXrF0MHF+tvJxZ/GbUWTx8tmWvqYBoQ6baXhO0fBQkBV1maI2QEQ0Zf58ZlevWat82sM8IAhSr/KZINb8pRpxwNDTbiM+p+4SnwdJ635PD/CYzG2YdDRt4PHMPzas5XnUbIeoW7TPqLgek764xBSi5wo7LOzR1EzZVZjWThwxG9vcul37/H2P7q4dj4/izGOD42kt+WmonvzL0vHK2cKWNfVV41HbhD3J4CMneSFhm2axvHy19/I4O2aLxSAiWvLFJcx+7tFnlM/0Et66Fp+odeIlcbHjJnWEkwd1jU+KiDHA0TU8JcL77kG5YOvW4xNZfJxzbN4KHNehnSDjl8YAAAAAAAAAAAAAAAAAAAAAAJDAYNMYAAAAAAAAAAAAAAAAAAAAAAASGGwaAwAAAAAAAAAAAAAAAAAAAABAAoNNYwAAAAAAAAAAAAAAAAAAAAAASGAOrYKyCXcat2N9yuWlV99g9knzj2N2QbmUXCeKdXOh7Zf//dY+F60v3GJI/egi6mdm+5h93jQtTv9AdhKzX+nU523ZUMPsoy47ndmfOvMMlecPb77H7Irjy3meK7iwOBHR4399QKWBIfoGuyl50E1ERFEnR33uDWYw23FEk7O1WHrVO/wZPfj3x5XP9MopzN7Zzutqc5Ouuy6LnzszwMtbOG6cypNZXMIT/Jm6LKWzmP3ahqX8vDN5fSciShHNPkJNwtb3JdzHxemj4aDyGZdXwOyWjg5mh4JdKk+wt4efO9ijfMDBwxb1MhTSdTfUvIXZ4VxdVydWzmZ2a00VP489oPJkzZvP7FiU14+2Bj0etTU0iONqH49o5rkZXmaXlUxSeRxRPDuZ227DaC3Torb2aYqK44o8ebp5Uv1Obrc29PIEjx6zvKK8sdig8vF49vSH8aQ4RZXH2GR8CZErdWhMdxtiiNp13F5fIxwMX+9LmiASirRPXDSllSIMyjPk8Yn6kV0UYfYpM3kfS0TkSylm9sTcY5RPYy9vN+edfBSzYzW63XeFeJ7IOD+zcwIelaeorJQn5KYon9wCXunbLd6/bK3uV3nGj+d2pxiiAtkqC9Wu0Glg/4gZ+jaJS9Rd07diZVqvwUfiFraOTjSpwjZNtmRZpI8pz8p6bvuF00w9NFKnsHXt3j9G+tax6T45ySP7xPb6OyXZ4DBGadyyiXp8Q33TutceUZ9vefMJZrdUr1M+XtElpqbzmmiRCL6JqN/mTzLUF1M+tqiNc088ntmXfunbKk9yyQKVdqi49IpFzG7f8JjymTOdz2e8n+adeGOvDo5WrX+H2Uuf1POzcy/61KjLCT4aF1w+j9l+P59zhzx6zueprmP24gsXKp/CqXzu2+nwNpGbpTtan+gR5196AbMdR8crOzbxDt0fyFA+eSLetsRkwTFE0jEX97FSedk8hs7b4+KJlq2P61ZrGDyejRjmQ1FxGK+Lx5RWtE3lCaTz88h4i4goJ7TXmkDvgRrVjnzsQYfswaH5RFayjm8tkv237s9lTZS2WwYwRGSJOjMpbBica/m8b1we/9jHp8JERLRhPbcdwxKMqDKUGeDtL5Cq16Y8ImbM9+qTu7P4fKbcp9eTKczNtnZewPziaSpLeYxP4roadVvKKebntkTxXJZ+brEYXwcjS98syzMUsSYZ1hfBEJUn83Hj/XVrlc86sS4ZMszdRThOchqYqaJvGnHdo9UQoa8Phpit2ziRW8xUJojxqJaCKo+MBqVtKqufeLvPVDMkohbi44CsiaaaKc8lp3z6LET5IxyDiOj9yJ4+6aPv4Hx8SKdWSt9VB86WgzARzRG27B3l50REq1WK6ak0CVvGBKaZoxgoxIjUcP+DKsdPn+ALWiWzj1c+t776G2bPv+c6nqdcx1tJYhKVnCLKm6zbHrl4m0i29DyL3Py4tngmMUu3Cq8jaqwYhA/1L3/xS2MAAAAAAAAAAAAAAAAAAAAAAEhgsGkMAAAAAAAAAAAAAAAAAAAAAAAJDDaNAQAAAAAAAAAAAAAAAAAAAAAggTmwmsZuKWooNA0trb1BBs2UkfDmcX2XqdOPUj6vPP4mTxDvGyfL8B52oadclK/fdd7ZynUFtHrLyEwp4u81D/RrPZe7b/tPZl9513PKZ0fXNmbPruBafhVlWmfjE6d+gtkhN3+PfGHO4Ze5/jjhTt5T7T0h/Q57p4k/67pqrrFrW1y/mIjowb8/wext9VIfgMjr520g1cPrc10d14IkItpStZXZbWEu1FKQq7VcPKn8mpZ84fPKxxZ6L9PHzWV2sEqX3z2L2zGhRNHUbdATF7prLoMqRnCgTfhw8rP9JMnP5WkeN75LcyjxpvE2crRBl8KTmcVtKV5JRFk+XldLyrheuzdPa7EGCvi4cPTAScx2G3Q0ldaTobpYQp9YyKwp/SgirfXoFkOUSZ5IyKxRyDAgxYRPvpDpC+nmSa1NXAcuGuVjuccgsCxlyRxb3zzvXlohDrkNCjtjkxQ3UYp7SJvkmDLdB80o42oyJbVct6orph9siri9qw0yKz1SG1lqc7XqPDIaqV3FK+bqJ2pVnuJKPq65XQHlU1O1g9kZfl4RAzn6vtRv4gVOzuZ2blajypOdy/WJLY++MYESXp8XLCpk9niv1uze3MDvjP9YXn7Lp8/zhxXtKg3sHwa5nxF9LIPknuyuHTFN6TfkGenUBvk/pfRkiipG0jA2jRP5QnhqxzouCNjgm67y5IjxxyBjP6K+sqn8cvYmW4Dpvqn7b/DZn3nVWOAPd3yfUlOG5sk5Hh0De6PvM9vQtVGGuMHuZK6PFzVof8o4wlTvWoPcfuGvS5lds17MuYnolPP5fOHUK76gD5wx23C2j85ln13C7JpXw8pnYnQ5s8uuuIjZry6tUnmeXcF1pH935y+VDzSNDx2+bN5hN3Tyua9PSuUR0XnnFzPb49W6x62xHGbXhXhQ3xvlushERJmi4bhzeWd95hcvU3ke+9mfme3t0vU04OGxkSUmA55UHWt7RG8cc7iPoesg2+apLtJrdi6hNNkr1hHW1OkJhStlErOdEO+D7N6dKk9OEX9wTTF9X8p79npuEdMVjU0cZ4AcZ0jTmJKT1OdeEX1IDVIiIreYDMix3DQGeFv5HNv7ir7nkVX82VrjeXSUWzJR5fGt47qvWXlae3hKIV8DcAuNY1+qjmosoQXuDuoxtSiX9wUr1mxVPk/f+RSz58wNMPv0M05WeWKv8bW9N/88oHwumsjreGAWH7xjjtAvJiLLxdufYwqgdqW5XNA0/iCmzxT16UvnKZ/2O//O7FW2njCb2sne+A3Rtpt6hQ/XLT3ao9tIOMrbq1sp0BK1Eh/HLBFtm+YpLuL9R0CoGsuyDR2X57FJ12151bImhkgj50zyGCb1XJ8oi2me4tvrGhwiihh8xiJxSqH4rjtyCY1Tnw8QX5NJEWP75wzPfjWJCbNS9SYiKhO2jE907KRriHxKhsXY4FqeY5meOZb6i5j99M/4Xtr1P+XzBCIiKhS2KEq/17BPJvpay9Zjo7wCy8OP4zUs8iZbPJc8sy0nbwcZ7I4AAAAAAAAAAAAAAAAAAAAAAEACg01jAAAAAAAAAAAAAAAAAAAAAABIYLBpDAAAAAAAAAAAAAAAAAAAAAAACcyBFbCVGsbiZf8FJfzd4kRELVXbRzxsboBrUbZHuc5DYVlA5Zlzej5PSOaFySycrPJsWMM1KMaP0+X11HFdu5qqZuGh9UVIvBc+1M31DFJLZ6kcUy/9FrM/V60VAP77TxuYXd38BrOXrtR6GHMWLGB2Q5Rrfnw2W2sT3H3XHSoNDOGlDErfpRQRadffweiq5y/Df7+Kv///d3/5m8qzfs0aZl9w7gLlE4lxBZrGJq7Vsrlmu8rjWLx8nRGuPfPCo8+qPOPGcc0V26Wv8aJzuQ6Yx5XJ7NoarjFJROQv57oZnnSuz1GUqfVn+/p5u7EM3ZcT49eU5uL122MQxWjp5G3ajmjtJHDwiPXwNrKzVmumTpjB9cWciFRgIgrXbWJ2UUUls7ML+DGIiKJdXOvJDvM65nNrjQlfMReIjBqkNrauW8HsCWVH8fNEtH5rmpvrh8mW1tLWQZKIUHmZe0aG8ikX8qyrhUxfi6G6uwu43o8cFsLtemyJdnE9EU+GLou9181y+g03boxS+yJR8q6qNu8S/exLp3INI5eXa7x1hrUWV6Gb95G5A1pL7tE39Hi+70gNG039BhmfmBSLOD1Co6mndeR+d5BLn1FLp/ZpqeF9Q1GpjvV27uBiz7OP5Xo/N113hcrzcPBfzH6v4XVmxwxlAQcOIeVGlklWSNhakdEw6RFyf6PR4ZVaWyZVodFMruS55HlMemkzxTC2tp7PA96NBlWeBeefwmytmKbPbbp3EqlMNdL1EGldJ9Mo4HzA32OdaNcmiu+KsbtI9+deN++Ly7VMGflkpRE3sLGXFG5RWV2GiqdCIZEnx637/Pde/V9mL3/+UeVTNvdCZl9x80+4Q4qYy+8n9/7uIZV2zck8ZpkU4PMSiurxMz/A5zdbQzoue+oZfp0Xnr9otMUE+0h7E49FO4M9zI46Br1fMRfuMOjlUoRrI+d7eaPIsXR9t7x8vhyK8pHB69Yx8YQy3hs3vr5K+bREeXCRl8HLEjBMbN1ijSAitMyl3iURkSOSLErXPqIfqGnjbejJFY0qz9FcNpY8uVxd05Wun9GGKJ+XR+r0KNtTW7rnb9NEbIySnZJEvpShtUa3QXfSJ9RLU0fx+yBZg1Ib9Dyl5S+8bdE6fe5CD48cokF+7nCGnrsvPK2E2XmGeXdZhRzsuM9gUF9j0OJXlVNYqnweXcnb+Z82aY3O50VTvy3C78MVx/L5MhHRcaEpzF7+/ibl88bfedpnZpQz25Wt63Qsygdey9J9CjlD62mRnkEi0jq8gMjl4nPq2XN13bjyWq5B3/S7vyufHeL+VhCPV3J9ug+NhnkcZ8kZRFSr7kqd8k7DHDumVVPFMTRucVyfyDPBoMkcFeWNGWYqlmifncTHadvQb8lIL1NMzgYMaxGp4jhSo5mIKLjX34k0n0iiYkoaDtR1ndokNI23i3s5w3BM2dv0kI6dUoTO8THEx5JOQ30pE1raJxIPGuZb01QebxkfE/yFRymfoiI+tthOkDu8rNcyqYTH+KL4lFpq2OcL8DvTTz3KxRY6xxlRURvDPEYjIhr08jYds7ltG+Lbgwl+aQwAAAAAAAAAAAAAAAAAAAAAAAkMNo0BAAAAAAAAAAAAAAAAAAAAACCBwaYxAAAAAAAAAAAAAAAAAAAAAAAkMNg0BgAAAAAAAAAAAAAAAAAAAACABMY1ssv+k+7mwtaRJi2YPRqq129idlAITE8unaDyhDO4kLXb4pfaFexQeaZOP4b7tPcpn6iLX1PFzEpm7+zqUnlcOxp5nvQs7nDa6SoP9XEh8SWLL1Iu9zz1b34efyqz19RsVHk2/l8ds9uy8vgxWg6tqPbHHRd5yLVL1H3lm5vV58FqXleDNhejf2zZ8ypPoYfX3ajhkYTDvG6+seo9Zpu+DVJaygXhl73+DrM3hNtVnuROXnfdAS1g/+Wbv8LsqtffZ/a4cUUqj5v4fcimHGaH+kMqTyTM0zyG3ssRuvL9EZ5gi/MSEUW6upntstEGDiWOxWurHWpRPsFaXr/nnnSK8lnXtIXZ2556iB830qny+PwBZru9vA/1pHpVHpc7ndmzTzlD+dS8sYzZ9VW8TVjeDJUnM4OXxY6KumrpVr3wmkuY7XYrF2quFQmiemfqS6Sg8JFNwmXpTJ4cP7MdR7ejWGRPDBDv121xrDI+hci1q79qrA6qzx03r3cZJTxGSHOlqDxN9TuZ/ezDAx+tkGOGOLMaanV/QhRg1m+/yfuX/P5ZOkseP05ucTKzt749ONoCgv1A9n4uQ5CTItJMExyZpo5ryFMrwvplm5qZveDEQpUnIGxTVCHCFQoKe/XmHpK07+RjSX0dj9Ei1dwmItq0dgcvW8FE5ZORH2D2wpOn8bI6vL4TEYmwjQxDiULeb3xzeS96dg5XQI/hxrjElNSXpX0CPm67RagRkZWOiGSo4THEEU6M215x7gml4sRE1NnNL6J2fbPy+dfr9zL7n/dye9YpZ6k8V33jZmYXTTtX+Tz/t/uY/eajbymfHyyaz+xIhLcdv0+Pu6fM4+sE0Q26ZT/yyFPMvvD8RcoHHBgCWQXMttL4/Lm7T6/juMRAkZOdrnxiFn+udqSN2Y4tGgQRBUXM2+XwDrLfMJmPiYYecScpn21Bfu72MD9Ork/Py11ighxt4/PcaMzQEbhEmqET6nb4/dwYDDI7xKcBRES0vZvHTiVibdCnRkKi9XX8PDmObouNzp64OewkzkiSRWmUsWvkdBnWNLzUL1J0G5DEiMcwNY/rtcy07bxOOZaec1ii3UwqGcfsynLePomIKuZzH8rQbYvGT2bm4Coe02zp0Gu640t5u+4rLlM+N331L8xuaNWnlmQdx8cAUzs5cRxv+5GzKpXP72s2MDtk8efk10Mq2STujSOfNZF3eADHnOSDqKvmcz47quulP5uvmVaUHat8Wmv4Wk8LRZntCet9j1lefq7OCO+bw+IYREQx0aYjpNuenLvExPO3SI8tJOqTl3g/6zNE6I7or8OG2Y0tZgNBMS837QZNFuf2i3O3GOpzAfF5Sdjgs3c4axj1xi5HH0+UvGuM9OnOxFnJ55ch4v3R2YZDriM+wNfQccpnqqpTvGbaxGM2IqJs4hOKZDmDdvSeAHUHRUKd9qnazu082c517ESWqCWxXm67DePTdD7RSj1DjzWp40Sc6RbtxqPbZ7JLjLlR0TfITY+DTOJEWQAAAAAAAAAAAAAAAAAAAAAAABTYNAYAAAAAAAAAAAAAAAAAAAAAgAQGm8YAAAAAAAAAAAAAAAAAAAAAAJDA7JumcSrR8Cvxo2kGB67H0Bvi797O9Gsdx9GQIV5lHqjmWn62pbVOnDQuyrSzg+tzeB3DpY/j7+CPGCQCXb4As8umct2v0oh+U380JHRrW4TmWA3XKCMiouNnMDPr+KOVy5zps5gdCfN31rulCBURVYb59wSmnMjfu37nmn/psoAPxBl0kTM4VJeefP419flDf13L7CwX127pVdozRDVRLqry+0f/rnwCxOt3wyg0a0KbeH2ojRk0AgRSHaKuTuvC1lVtZbZbaJ2NO/VClWeg+1lmxwYbmN0X0tfjCGFVl0u34ZjQPU53ZQsP/T0ZfwbXGbDthFK9OOz40nm/NbFc68b4coT2eqoW85s5/0xmRyq47pBja50YSyicuISWn9fPz0tEFMjjaZOmai2c/JzxzLZlS/Lp8dPt4doaHlEPoxFd/lyhKSnljoiIXEKywytsyyC2KfXC/eJ2OxmGMVc0G3tA+1Bsz4GcaLLS8ByrdGfs0TTuzNIxQm4mv3lHFXGdbG+J1hO95vYaZvfqrnkMwrWfswwxZVeoXaSY1JMEot384Fv3K5fKU7gm03QhZe7S3QA4kIziK66Dcug2yPBKRtI4JiKyO7cz++k/386PEb1G5Tn7jLkjHne8sJ99l88Nli19WuUpGcc78NOPn8TsHTU8liIiamnhWk/R2nrls3MT10bsqeLjXCSmryDs4j7nXbmY2WVCqpCISKhDGTXG9k4zDGljlwEavvh0g9SWS9wMQ0hDHiGbVT6V61eXTtcxTfWG1cwOGWTghVwr5YrKW1mhNbyqNvG6mGuYdluivNvFuZ998EWV58l/87RTFn1S+TiNXPNyXqk+d3N9NbNLZ/BApyCkdTJT0viBvAF93S9t4nO4B+/7oz45OCBERM/qePg6TiymF3Lau3lMEBzQvYwnjae5bD4n9Xp17DEgOrN2se4Uiup5bdTLJx3RHN3wO0XsH+3mvejOeq0VTgEekFS38DopZfuIiDw219TLc8kJBpHH4eW1J3Cf0ytPVHm61vC2GGvijbyzVetQvs+bJh1bXqR8WtL2zB17DRqfYxUX7YlbXIbrtkWaYSZGNbXcfuEuXoeyuyaoPPMm8wG9MaBjDb+f17uKYp6nbKqeu1OGiApKDWvF4jJfqgsyOz9Pj2tuMdn9219fVz6j0TCWfOLC83hCiaEx1fMbnJ+utadPOCvAbDvA1wUiYf3kXA7v3xzDWnY0MtQnRvugafxBuFOFzmpU97uNLbzfzR8/RflcFOD93856rpUca+Xro0REOdm8fjdGeJzhJz2ZlGu+Ew2Rs4v4Wtr71MbssGG92a1SZJ3RC0SOSIsYylJL/JoMIaUik3gwKLXZTTtI+RbvT1xOt/Ip3qt8NsWpNkGUjfvbt1D/rv2x1M9fpz6fVcr7zFl//4HwaCPJBKEBXHqdnvvSH+4VCWKwMWh2E4n+MVdMHoOGPK3Lhe3VPiTihlYx4aji62hD5xZjSaY4boueF9DfxXHy/drnhoXcvpzvH9b5dBw6XuzZWWKh1WUd2t/+4pfGAAAAAAAAAAAAAAAAAAAAAACQwGDTGAAAAAAAAAAAAAAAAAAAAAAAEhhsGgMAAAAAAAAAAAAAAAAAAAAAQAKzT5rGN3zzSkr1DL0B/6Jzl6jPr/rCd5ld28Tf7R+ytaaDeLs4LcnX5/2vK09mdn0b19F4ep18XzpRLD3A7FUx/p77VsOb/HdUNzE7kK8FuWwX32evbd/M7HFSUIqI+oXerFXAhdaaXn1B5YnU83fJ/2udfoe618evIcvF3+3/5BNPqDwF1Vwzo8PNn0nh7BKVh94McDsc1D4Jiiu5j1zJQ++Yd+WZ3tPP9SG67J0Gnw+nV2lMEPWOQsNYMhoNY0mZhzfId3duUz7r3+Tt/ML//pLw0PocKZlctzbYxrUJ7JjuK5wYf5d/e5fW0wmkcU0Py+LPpL5Lt6NIJ9e3MkiBg4OIm7iOlsedqnxi3bw/rHtPP8escVy7YkoF14uIRQ0PVjxsqf87odSgvyS+ahUzSKaWzOT56mu5ZlN2nh4n0sS5nRgfJyxb6xs1VXE7xaCDWCdu1XYh+bphk9ZDCwsBQ9vFxw2TDrIvEGB2xNH94d66cE5/jz7IGGX1y3v+3v6+/tx9Ie/vIvZTzA5qCVKq1WGPQtaYw6n6Jmv8w7/8KrMfWdlEkn++tZ3Z00R7fPoH81SeLDHOuc77rfKRI6pUQ9NKPkSRTq5dWb2ea9iU5vN+bIi4IQ3sD0LiXWmsEhFJeR+T3M+g0DmW6lYmVaTyfD5VmlHA7ZYNb6k8XXNmMHtWph7XJE477xwqivQFzJvLNWqPncZj9hef1O1oYj5XA3Nsgz5xTAweYjCMRPV0sbqGx4PNb73B7NJPnqTypAnb1Cft/Wj3aZL6MceTSrRbak+rzxHZYtoa09NYKhBSlFnlZ/IESz4BonAXV5xzGeYpXtEwsgu4nevXOnEtYn5ZFFAuVCCkvzw+YRvkw3YGub12+b+Vz0Xz+Rxj/Gx9nKadncwuLeJzlXyPDqjGVfK2dJStlfa213FB1rrVq/TJwQGhPsTjyPAAHxiCoaDKE47w1pXr6Gdohfiakd/HG0DE0ECDnbzvjdg8j+XS53EyeQ9nlRQoH6rn12gJXWGPYT4Ryc7ldkRqNEt1eaKMcTy+8pTqNTBfLu8/ojl8LAn49Ag6wTuZn/tVXpbgTt3Iq0SUNs1j0KHs2+t+Rw0BwRjFon6yds2bI4YRVI7uaRRQPi/8Lsjs0Fr+eW6O1rO2PfzIR1UUKx+Xi886emz+XB59/DmVZ/p0HoFXLLhK+fT+m/ehZVMrmB0L6Yl4VTVfIwq936V8znLzuP2MCxcon8su5vOMomzR3tr0pK4pxNPec3UqnxkXHs/sqI93KlZUr5353DzNMcqzDiWmOANE9J7JIeGJisi/3bDmSBavl9OnzlIuMeL96JTjeV+39rG/qzwdIb5+6xcttsSr4/NAhPf5AZ8eSzrdPF55p5PPhR1DVOkTEblNvG+OkV43s8QsqZO0jvBOw7r13uiRZejIHH7uGUKzmYjIsnhZ2h09rhXSnknfAMWJSPuMRarbl5Nv1ziR/hP9jMpsofer1upNi+F8XksnTtQuf5CLU9I2KVxXCLuSm9OP0lnWyPK9aziunA+LNV3bsMbbLCYlMRGTRUy/txV5WquUx8D3+bwgJXYFs8dfMUvlsZN5fBgbENecemhnyPilMQAAAAAAAAAAAAAAAAAAAAAAJDDYNAYAAAAAAAAAAAAAAAAAAAAAgAQGm8YAAAAAAAAAAAAAAAAAAAAAAJDAYNMYAAAAAAAAAAAAAAAAAAAAAAASmH1SUO5praN+91CWR+7/k/p829rXmN0nPv/jb76j8nTf9Qdm33JKqT5xId/bLptaxuzT5kkxbyLq42evD3KR+zeaelSW1Zt28oQ0LdZt+blodmNXHbM76/RxA1ybngoruAS8kydEtono67+5i9m1lha9//SFFzD76FwuCN8wRwuUT3bbzI7a/D6tePs9lSejmN//cG+q8onbex3XiRM1O8pnbFJPRG4iIvriDbPUp3Yvr3et7byOnX3h6SrPN256nieEP1IBR82tX7hSpZ101nHMPvuKrymf515+gdkX/vcNozibj1nR0ACz3V7dNdm86lKoU7c1yzPI7E6H3/8d4V6Vp7O2jdkT8ot1ccFBI2rzZxZ15MhBFG7m/azd3618YrEQs902f/YNdbUqj+yl3D7eh3a2TFZ5IkFeX7q6OpTPCRd9itnNO7cz+/2V/HqIiOxolCdYbl5WRzQAIsrO5WNf1HErn6q6JmbH3NynYVONykMhcX/9Yoyy9BjQnS4Guh79jCi6V2c2qJ9zItDSqNMevOfgnGtgZJdDxtmlfmafP4P33+efU67ybP8yb1ve1BizswKizRBR02vbmD05V8dBje07mH1KOa/Pj1f3qzy1G3g78YtQNWLHVR5w4IiJ7s/0jVeXCBsswwzHSv7w4+gaRVSzoZ7ZBdn84U8p03OQcXaTSMkzHJn3xSfN4fOLvJyTDHnSDWl7WHDOqSotJVWOY4a+tz/IzA4RX7V28PGUiGiyuO6Nm95n9sO/4W2RiGj2wsuYXXl0ivIxjBwJQUnpBEpzD9XIjsYd6vPooEjwKRfyFwR4gmcKt2O6hvsy+Zy017NT+eSIw7rEcB9s3qzyZLr5CDTJMFV3RBDmzuS2pUMa8qcJu0K3iRPnjWd2QU+Z8iksETcwh9+rYLRB5bFCXczuNMRctuh4HCtN+YADwxvvVTO7tV32HnotIiDi/FivnmR7Unm+5nbeH0b7DSNQmPdllovHEblFvH8nIor28fMcNWeW8vGV8b737b89w/OUTlB5vBW8sU2fys/tiulrdnt4o47EYsqnUzTYWB+/L2FH994Btzh3Kj+PYxiowyQ6u1in8gmt2XMNkYHEib9i1Eu7n4wpDrKJ1+/7bwsqn01LuX3KdN4/OqJ/JyKK5iVxH0uvp9g2P3dqDq+b2e4TVZ4p55/CE1L0GlIs7bvM9rXwmKy+iffLREShKC9Lm5i7ExFdfsl8Zn/hH/9P+RCJceDlv/JzN7ytclRN4OOs75RK5eOt5DGhL7mZO6ToOb+MjixD/7a7NVkDpvyAiKi5jc8tG5t0f2gRX/fwpevWZrl57DGxnMcd0dotKo/rfZ5WFuDr++6wjtEiEVG/w/q5B8V4I0vrIR1rS2SP36hSiJrF/KGe9HxZ1rws4n3HBBKBHhGFiY8lE4VPRYDHqURE1UHeHtyk16b8e211xcghIt1vjUWOoTTy76oFdSHdR70ltgDni/qeRDkqT0r+0TzBuD8x0tqenksSreFmu6i97WfqLC6xVm+bzvu+sLcLW87Ticiawe3/9yVu/53vdRIR0Ztbud2Zr1xSSKz7/viP/HOXHvdSvng8s10x/sxi1qHda8MvjQEAAAAAAAAAAAAAAAAAAAAAIIHBpjEAAAAAAAAAAAAAAAAAAAAAACQw2DQGAAAAAAAAAAAAAAAAAAAAAIAEZp80jacWFpLHM/RO/PXrtSbjE3/jWhCXXM61q66dc4w+6GULuB3SmroU4W/H71vJ9QDSPIa97yKuFVEs7MsK9Pv0LysSukidWqtyUOgXrHb4u/HfbtTv9neEHHFlEdeaqWrT79cPZ/JMOaX6Xf7bOzcy+5Q8fh+uvELr5Zat5u//D63n1zhb3xZa6bzDbJ9L6yu7kvec27Edamhu1wcagzgUJGeXTkRFmb55P/3F2cwOR/n78wsLtM7WtDnTmH3Veb9RPm3NB16/x7K0dsWdd5n0XTh/XPYys0+54wfMPul4rUVQNJO3gQ3vcv1w9ziDUIJ4d3/E4NLZIo6Twetqe6/WeOkTl+1K1poY4OCRM0Foz5VpDVKn+2iZoHw8Pq6/4U/nfbUnoPXEpBqEN4vrrlpScI+IHKG1VVFSonykRNe0uVzPaIdfa4WEhTayK4XXw1i/LosvS+hk2lrFNuZwHZtuD7/GcETr3NgxoY3jE+ex9JgbE5rMVnaB8tm7CccHeinyrnIBY5SXarnm+Cc/+wCzr/4s124hImpq5xpkVSu41ubCBq056dg8njp93lHK57HneXzS0iO1cAxioUK8xy1c3Kkj5wH7jy01jU1feRVdpMcg92MJqUSf0Dh2GfSuisfxDv30Gy4U5+VaXERE4dBaZg/26fqRnHYWs/Ny5iqffSUlVY+fWgfZoIucmsvMHDHlkDYR0VRhn3IqH5fXNek5VNjLxxuT6t7eo6NB0nbMUlx2FHk9u+qao7W2nBAf38eXKhfy+OWDEuKUfi9JCsq5lq87JnXAiKif95ERETd3tunYQy4w+A1dZFQcJ1tkMlwijQtwu3C6rpxF+bzmuC193SmlYk2i8hJm5oQMtbOIX0QWaaHm/zydx3fhCJ8b3nrXb/VxwX6xpZG3k511vM85xhCfO2Fe6XpihucshLMdhw84uTk6hpe9VeE4Xi/dXl1Pt9TyGKYxpGMaj0+U18XLGyW9HtAe5JrosWyeJ5CpdbZD3XzsCxvqv8vi8xLH4T5+n27kLhFP7Wjh+sQtTXrdTB7F06PH5VjTnsE8ZieOpnE6WeTbpRGaQXqe1St0smve0BqSokpRrjfA7JhbLw+7LT6WRMNaI3hcPm8XZdNnMbvyjJNUHjVGGXR6fdl8/t5cv5nZRXn6Pux4h5dva3Wz8rl0kYy5qpUPdfOJ6hMvPsfs3z3TqrJcdEsFs6eU5SmfznAjswPiNvhSDfNsEvNsqf1NRK5dUZXL8BnYhcPrtzddxwctLVxH2CG97uTz8rHda/ExoKhQRzB1VVyPO9bF54nbOmv1eYQ9mbRmaqvN67tLRNcew5ZPVNSnXOL1tNHQFmuIz59NCrXjiU+sComPfTZpjfpc4m3cR3z9ti2otZ7biY/3UcOMwrfXNTmG6xm7dBLtGidKDPrEHvEMVtEGZlfSJJUnPUPUxL8tM5xX7iXIWGMkzWMiotXCrtEuttzj8msfkv2uLJuOt8gROuSbxHzoW4tVlneEPvFx+YZx7qGHRYIYa275s85TLK7pQj5nSY9C0xgAAAAAAAAAAAAAAAAAAAAAAMAhApvGAAAAAAAAAAAAAAAAAAAAAACQwGDTGAAAAAAAAAAAAAAAAAAAAAAAEhhsGgMAAAAAAAAAAAAAAAAAAAAAQAKjVdE/hFf+8jClJA+Jah971gnq89u+93VmL7/rZ8z+5LFTVJ5Tp5bwhEVf1iceV8zMzuu/zz9fpUXjiyjEE6Q2ujtdn6eJC6pTfbtySc7j++xlES56v7GHi78TEU0rKGK2P5f7/KFdC3H3FPL7ktqvpeY9A1XMzqvjIvKVJ2gh7lqnjdnz/+NiZi8qPVHlueBLi5i9qbVO+bi8e6rS4EDiCM0PdCdTLD70PG27S33ud/Mm5vF7mO2Qfq4nzBrH7GVvfkX5NFVxofYd1WFmf/HGNz+k1ENMzw4w+/Z7/6F8RiNXPyDsz3zrV8y+4oTlKs/lV3Ih+WVreRs2VEMii9errlCPcon0BZntyeL3tzUSU3lcXbxzyHJ3G04ODhaTKnKYLbtqIiJ3aiazvW7tY4nRzCN9nBySuHlzpFQvtwdNhbELmZmRqV2iouG4U7k9Zxw/BhGRy83TouLcMUO1FE2CenV3Qt7x+czeLo5rZ0xQeaL9wkd8vczRw5xC3lsiXt7BvhBteWTk44Cxgeytn2gV9p1v7fMxn99Qo9Iy3T5m+3N1PJKbzTuH95sNDUfCmxFtC3I7jfgYDA4stj3IbMvS33l1WUk8ITyofApyeH0ooJQRz50/gZ8rKXnkyCgz7WiRIu3RoGNKIj4Q9It4OxyKqhzV1SuYHQzquhoW40tEDEARW0Z6RDGH+/TEeHzV2qefUVE5n2cdM+sY5eNy9gzmvT36vGOVzPxcSk8bqo/hkJ4vuzJamJ2TrePZqMODmDRL9G1evz6xwwdrn1/Pj50wr/NRcdgeMeUmInKJGECXlsgW3bMvg9uWIY7wiiY79UTdtlwu3sd7KU8fKG8ytzNOFraO00ZDShq3s9LMfuCj43J4H58dEOO/Xy9z+RyeluebpHwCIrCfOrWC2R0dYr2IiJa//DSzveW8U3UHelUer2gU7S3blY8ri/ejLhcvvx3Vfb7l8Dxdnby8PpeeD+Xm5Agfw1pOjB+3XczDIzGdp9Xm5esWbdM2rET6hO1KjSufSSV7yhuOOUTUqA80Buknh3bf0QzSk+H0NN6JZlrNyseXyzulUHMTs9u7dYww3ubrklOnFimfkosu4gnlc5hZ+6f7VJ7cYl7vMs5dpHzWreQxjDXA692x06apPL53+drqrNkB5TNryVk8YfNK5fOTb/yc2f/9lK6Lkhc/xddnP/WtNuUzqYK3paISPnYfO1cfd1wGH8ds44rJUO1IMX4GiIiiIr7t6dHriZ1B7uPyjFM+fhdfd/eIgCVm68WSZTWbmT3XX8DsgGuiyrPZ3sHPY4imsomXpVP0hzG1WqvX20LiuFEaeT0/35BWKuItR5yp3lCWIPFxYq1YNXAZ6nOAeD9mk77f2/faD7Jp5LY7VniXosPj6HG0U31eIGw55m6n7fqgNS8xc0KNfiaZos7EidfnjcT7RiKiSn0mQdCQ9pSwZxh8soQtx0vDYjLxvrrp9VXM/p9HH1I5IiK0O+4nX1c+9NCzIkHOxep1nm/fz8yUqd9ldr/PMEE6iOCXxgAAAAAAAAAAAAAAAAAAAAAAkMBg0xgAAAAAAAAAAAAAAAAAAAAAABIYbBoDAAAAAAAAAAAAAAAAAAAAAEACs0+axhkDRO7BoffBp3Vrra2cyUJrQbxafkuv1nSYHuBaRFmzLjGcuZxZ+ac+yez2iEH3yxPgtiyMFFsiInJxHZAuW2vWZHm41lNOBdf0iFTpa2yMcfGn5hDXpXrXcC+bI/zt8gEnqHwuXXI8syvzj2N2zab3VJ78EqEdUnopd2h7RefJ5DpxtQbpPse95/sHiaMYQNTZFKH+nqFm5CKtK+TJ5GKmHi9/17/HoBkUjG5ntt+r69SUE/j3PVa7A8w2SWgtOJ2X5Rv/8TVmP3C/fk+/k891EKpq+5XPKi1ZzPjritUq7R8yTXx95YvFk/SBhPTAth36vsSIl88ttPtMmq+ZQsrECmmNdHDwkHq5McMzcglNYJNSj0ceR0i+uEbxFakBkSfVIHfhEY3LNhRGlsUR8i1GbT/6cB+TvJjCq5Oior+OCO1B2yDPaQtJQ484blh3dUpD2qQUEtnr2TqQWwIHAb+Hh7UvrW9SPtPyeIUOOHzcWBnTQU7mLG6LIZfW87AOHGAc2dG69PTFEYNJcZ7u9AsMmlfiwColKVlqpmrNVw3X2uzuWqM8auu45l6qm89BHGMvyucuvRE+UmyrrlM5Voh4q7FBz206u3ic3xPlg9aOBn3cqMPP3b1T6mYZOnmhATezUusidob2tD8ngQaK1LRU8niHnrnPr++Li7imnjtFD8SqznikVp/hfkY7+Xncut0IiVSyRDySLgXRiEjIy1O3IfAJia62ZDzXP3N5hMgxEVGUx+h5k7TWIKUGuN0jtc2IKL9cJOyfhjE4fBxVOJPZWTP5OlSkS+u59jTywdqfqwPnSDcXqVv+4ivMDoe1PnFBAa+rEYev7YQiemzx509gdrZXjy1NLXyu60R5Q7KienIwwc/XeqJiYmUZtIejMdFXxJKUTyzCxzVHZHFSheA4EcUs3nnkFHNt3FirnoQUtPO0FMNqpW1H9vo7cVaeGmmP/qTPMJsMVgtNRh0CU4VP6LW6+XFcHv1M/EJbu+SMU/SBy+dz+733mfnaq2+pLHNOPIbZldStfHZs3Mbs/AIxEZ9ZqvJMrd/O7GCm1vF+6oG/MvuPf9Saxk/tUEn7zOM/6xzRZ84V3Pbnao3LkhnzmO0i3d66d62D9YsYFOwh1M1jp2hUdzC2WLexHYN2vIiV3BY/zoSSMpXH7eLxypYQb6Azxsu4hMjbyBdutpMef7IpwOyJQjM1ZphP7BS6wZ3C9hnmQ/OJL8gFKFP5WELXtpX4+BM0aBo3i7TRqLVWiNXuiOG3kFV7aRqPZhltrPAn2rMOt8Tw+UnClpGHnPUSEa0hru+roysiR/RJ/yM0jP9kyHOmsB8Wto5EiEjVoY0GH6kPLmuAqUbwtHHl/Bi/ufZilaNnA58f1377R8qnlOQikWxbhnWF1ne5fRffcHHffLbOcxDBL40BAAAAAAAAAAAAAAAAAAAAACCBwaYxAAAAAAAAAAAAAAAAAAAAAAAkMNg0BgAAAAAAAAAAAAAAAAAAAACABGafNI2PHkwjT3zozeLjq7TmRFeU62TF8rm2zITpXIOXiOjp9Vyn4vSlv1M+RWeeyOwdffz9/1U9uiyxVv7u/mwvfzu+m7SmV6qba52FpYgTEWW5xS2byXWnJldtVXlWNHBdm2g3P4bLoEmSnyfes96n9UW2dfB3qJ92/gXMLjtursqjvicw8Bgz37xL3//Ons3M9vp1tYnae9KSnMRRDWhraaXe8FC9sWxdDynCtRLzsngdM+qUCq2hWLdBf07o8l235E1mmw77xa/NYvbUaVzH6dqbT9Pn8XD9mRde3qZ8HJvX39VvjKyjojxEgbfWhqQH9cS4hseqFVpfWWq6FlRyO9uguxZq5Lbf1loh4OAhNYy9hlFJagS7RiFzKHtvgwQmOeLcjvBxTOLD8jwmHWEpMyS6736DJrAlrik9m9sxw1e8nnjoZWZnlMxWPrmVXMuvO8g/l3rRRES5+eLc8hmZdJyFAI1h+GQ+g4kjQTam0ApaJJSTyBBdHTpOKOIV75SpWsdpRRPXfFvWw3WRMit1w/fm8lGraYNw0DLI4AAS7RVaW2m6JsouvsCgtTUypuiJaxHVvM91+jat4jEyEVHjJq5HV12zXfm8X8t9XIECZvuKJunjtvO4543n32b2oKO1h9VoaBi0Ul08bUIxL8v0Ej6fIyI6uoJrPc2edR6zZ8yZoksi9C2dkI6bH/jL08N/x2IDdI/pksYgLkoi1y71Lk+61jS2ozwu7h3QfVuu7KGFXh6FdGzd38W1wNyWfiYyNnKJU0/Q1UPlaTMIoIlqRxNLJjPbkkE9EdlRUb5MQ7Dh4/WXPFLbjIgyFug08LFiZglfHwraPFjtCvP1FyJSwWl7sEa52DYPcnuEhrE9oINgTxafYLZZPCiwo4Z66uXxuePWx5W67qFOXv9jLq2F54nxBum3eNm8QgtyqIB87HNbun9xWXzy4k4X2smyQROR5ebnsgI8vqrt0etbs8Rc3SA7TfZeI/5gAqlV9vUTWbvCgLTUYvX5jvoGZpvmsbGY0IMUIvVTyrVO/PxTuPYwzTpW+fT86wlmP/nEMmZPKuN61kREPV18xlD353uVT0iMWzOPFWUpGq/y1LTw+/A/v9TtfI2j0w4Xq1dwu6FOT5JbZgSZ7TEov9rD/0PT+AMRY4Dbo/tdO8b771hE6/D6xJ6ACG9pQtkkledTV3Hx6r/fy+v7ikauAUtEVOnm7bw1poPircTT5hKPgdwB3fY2Bt9jtuwqZrkmqDw5uQFmNwT15HdjlM9twkJPeafKQUKRmahcrCw0kY5dq0WazxAv7n1NibTstLdau2HJkbLFPsJU0V+Y1nrEkrph94poldAavuODCrgXjwr7LmHfOIpjqIVWIiLaIuzRrAnwRtx977eZ7b9XjD1ElFHOteZ3VD9hOG6OsGVZdLyllL3/+iwzk07V+ucHE/zSGAAAAAAAAAAAAAAAAAAAAAAAEhhsGgMAAAAAAAAAAAAAAAAAAAAAQAKDTWMAAAAAAAAAAAAAAAAAAAAAAEhgsGkMAAAAAAAAAAAAAAAAAAAAAAAJjGtklz2cH8sh3y6V96NjeerzJzs6mO3pbmF2ZclsledXT73NbHdTg/JZ3N3N7Pw0H7NXRbS0eU2YC7O3CW305mCbypPt5yLUbke5UHaoh9npvCg0dSoXniciepp4WZrFXn1hQa7Kc9ap/F6lU6/ycTdVM/ud/3uY2cdduUTlIRLPbd1SZp74rU+oHEs2FDP794+/pHw69rrEwWT18ZhlS107edKGnqfPFVGfx3q5lHwswitid8d2lScaFWLoLi01Xx+1mb15J/88t0KXtZOamL3s5X8xu7Gbt18iIk85byfRoBaadwuddqnbTlFdlpFo2aTTvv6f/81s+1N+5fPdX36N2THRzDs79XFt4TNzvBSj19cMDiCin3UM/W5MPoIU7eMS+Xyiz4x50vWpxQhoybIYRkivaI5WWI8/XQVJzE4TX88a4M2XiIiyRPfxiUY+7lUb8jz59pPM7q7aoHyOKruB2dGWVmZ7i/JVHkeUJRritilwUM9NNiMicvYaJxzdrYGPAfNk/05E3/48H3BWr9Fx3COreBzUKD4PGs6lW9bIrGzhtfOeq45SPq4dfFBaEeOlKZ+ua3hDZJDZqeJzT6EuS3fNhxQU7BNBMXhbPt3BBAqymN3RK2sZUXsdj5tfW/4as+tr61SeHD+P27IzM5i9/u33VJ5QK4+nsrPHKR+fL4fnEXFdqLNL5ZkxtZTZVy/+ArOPqtBzEI+Xd87HzjpW+ejvEMsabhiYSQ5KssVmkIbnqdn4jvJ4b8PGPd6Dg+rzsUpPR5icyFBw4/UVqc87u4PM3tGsY/b2dj54B3z8flcW60DCcXYw2+UaUD4pot/PFlPJpGw9j+0P8kHeoZDyCYjqmlos2klYBwopfbzdUNQQHMl4r8hU5yca0sDHiTee5+sR2zp57JFfqMcJfzqvL2HDpKM3wtuRxycagCF+7XJ4XBF1+pntRPRk2CPWpjxqAk1EOXxcS8/lc5toO58rEBF5LRHDRPg1RsL6mrP9fE7tcek4yI7xe1coOoLiqdNUHsviY0vH+vd5WYr0otHRxbz9umy9BpaZtWcsTI7ZRNSufMYiKb2Z5LaG5pgNtXrdac2/+YJQqEm5UAevUhRw80l2e7eOPWZGeaVPf2ej8vn2D3/D7GATH0uuvPJUlefUz17EE8r1+vJVc45h9v/+8Qlm//Uz31J53nydj2vF45ULhcSaUI2+nYeOWm7+8hbtEurcxuwzL9RjWGnO0FicpOIzsBuX6tt6tJPF13Ec27DyYYt7LMJox/DTvONOPp7ZDTt5A3126Z/0acRipmF5RS219hGvzLGIrg9dYiDLpExmN5OOv19tfpfZhu6FGsVcQN4GP4nNEyLKJ95XHEMBZhcarvol0ee7qV/57D2qOWReaxjr/N2Q5hLP9j8pjdkFhrVvOStJUx5EzftWNCO/EPaXDD6mc2vknDQobDnPJZK1NVMFe2/qLNU8rZL0fEjtv6k5tWlRVC52i1jv9y8b8hw88EtjAAAAAAAAAAAAAAAAAAAAAABIYLBpDAAAAAAAAAAAAAAAAAAAAAAACQw2jQEAAAAAAAAAAAAAAAAAAAAAIIHZJ03jqemF5N+tk+J41efz0/n7uo+fw8WKij/NtRWJiO4Uaa8uWah8+mq4Fp4rg79zP+bWbzZvDfN3tYfFu/zXNGi1vOIofxf+jMn6neQhsc+eHuY6KyXTp6g8pQFuR23+HnOXpXUGGqu2MPvMSq3Ld2YF1wbJcAtxqH+uVHnok1dz+7hzmNkwuF1lcbn4/c4O5Cif3r3EKu1kk/bZ2CTUa1P/4FCd6DeIYLe0cU29cJS3kdxMreXT3MLrlOPRx7VtXg/HZ/PPK0/gWhxERA1C68ldz9+fv7ONtzMionxRpfwB3e6z8rhuxpxP8XO/+5Rua74I17P45OJP88+ztM7AVZdyTeP6Dq1XeN2NXNP4qHn8/tpRrc8RERJSEUN7BAcPKScWM8k6iK83uQ2axrL5pfXw+m5n6z7UFs/eLURhxhnkxXwRrgkcDGkRpEzfJGYLSS/yGqQeJwX5+DNu/ePc9utMV0/lajLuZF1gS8gT/quJC61OLdXjZyCbn8sj+hfTI3KJZxLW0jLk3qv7iEWItmgXcITzhkGjftE9Vcw+2vB1RK22ve+UiIafn6tjkbwCnrYmGlA+rolcmaewm2uftezU40SxOEyRGBv7DNKrq3QS2E86O7h+a6RDx0XBdq779f0//lH5NGzieopOmPfn0ydr7eG5s7hmd1Bo++UVax3hcRUlzJ5YVq58TvJyYcHzLlzA7KTkQ6m5KnVsZS+vNSWJ5NgndRBbdJZ+ftxvfv8B5fLa6j3jWjy+P8rmH0/smEO2NVSvfdl6LN9Yxfup1SvXKJ9JE3j87Uvl9y+oqyFJefAsv/bJFmkeMXWJhrW+Mnl5P1tSXqpcPD7ZdviyRFuP1hj3ZkxidjqV6XMHjhEJc7QP+NjjWLzPLxzPg49Ajh4nnBjvt5qadAwfCnGNy1wRV0T7dCBkR3gf6jj8uD5Lz/d9fj7f9BpiJ1sIH2dX8HHB3aj7ZjvC+9lCPx/XYraO+50YD9p9holK+exJzA6I9aCY1FImIp8QRJfzoRmVk1We6ZV80hGs1WOJnbtHpDYe1TrsY5Vtb3STd1eV+NdDQfW5R3SZtkGnd6dY7skSmsbnLTpZ5Umv4H1q9aYG5bOxij+Hc07jfX6kR2vHdr//HrMzyz+lC1zJY7CVm3h92Naox59PXnkWs6++bJ7y+e5vHmH2r++v1uc+TPTo4Z3uu5Pf34oKXe9n5gzpgUPT+IOx0nk/leFO1z5pvOG4ZdBDJm1kjuPo+NXl4jHaCQuOY3bn28tVnik2b7BbQ3q9tl3otyZbfJ21K2aI0YSubb2I4VfbQvSbjFG9IpMmMHsi8TFr/kzenomIom18HcHXyfVbnahe+PMrTWM9EffRnuc2SHGqV1qyY5MvX/s9cruHxt7f3vV99flDwm4UGsZfI43cATAskZKf+GQh08XH8m5b110536ynELOfNeQwjBL7gWGhkuT+yf7MQU1tLVPYsm1pnW+9TSvu+DrDIHEQwS+NAQAAAAAAAAAAAAAAAAAAAAAggcGmMQAAAAAAAAAAAAAAAAAAAAAAJDDYNAYAAAAAAAAAAAAAAAAAAAAAgARmnzSNacL4PcKF47SW3H/dyjVHqXzf3wNeOn2uSgsFNzO74IwTmP2JScUqz6rlXBNg8+ubmL2Nvy596DxC98Pvk/pcRHYy11od1yF0bcq4Zi0RkSvE35/v6+G2HTU8hm5x3FrD+9Hla/mL+bn7Yvq4aUKvYLBjHbPv+OvTKk9jNX/XfNinv2uQ4tqTlmTQ9hurxHr7KWlw6P33MYNOQiyVa2DUxfhzDfXoe9ki9JaiEf3OfZ/D9YiE3Avl5uln//56rn1yUj4XMvPbAZXnnfU7mZ1veOV+dilvE9l+fs3hOi2oU/UG1zR48K8PM3vRokX6RILLP3+tShvkxSXfGfwayyu1VnI4wnUzQr2Jo410JCBbjWP6KpNwsg2PSMjWU6YjEgzyPpViGPOKbvckg1RezWZen7eFdN985rRJzK7nVYxKpbQFEa3r5XWz3s0L5/HpC5ifxdMmWFp/xmpfwez7YlyH6vQirjtIRHTydN7Q09xc46PbIEmSIcROauq0dlV+zh6NtHAP0YP6MOBjiByh1h0kyaC6GG/4dY16bEnr5B1I1LVR+Xz6U7xthRt40JKbJzVtiPwVfCwJWvyqo1xeHBxgmpq5kpbXEDZ3dfKK99nPnaV8ZpRdwezyYq4rnJahY3gimabjiCMbGZQbRA6FnhVRjbD1nI9Iit9KfVqpi0z0i1tuY/bjjz9jOO7e7S9xNI2Hvsc91H8FQ0H1qcfPAwdvbkD55JXxycCln72EO/TWqjwrlz/F7HerGpWPT0hpNUjdTIN239nncA06F01SPhkWn79Xr+H6ljG3CJ6I6NTTrucJBYuVD5HWvwNjj6gI2n2ZPHZ1G+YTjsWD1eICt/Kxs3iFH7R5X5acrgegnjA/TnMLj88jGbKPJWq3+bg2OV9PDiKtrcx2FXCfC86/QB93xzZmN/TxoL1gnEFfXLTxaEwvlPkD/P76M/h9sLXUM6X4+HiZK8pPTonK4y2dxOzWmNYc9RUFhv8e7NNjzVileTmRZ1dV27JUf+4St8Kbr308wue8c7nme/nVl6o8r/zlMWYve0XH1seU8th5ainXs5w/R+tXV616ndnzjtexRl8Vr2c+F2/Y37lBrxnNPPNYZr9R9aby+d0RpGE8Grxcwp3Ca/RkvKV0aI4f7kmgxdh9xCXE491+qdZK5ONVl7wyCCIil4uvd0YiPLa2bb1u44j5ccTmCZUGjffC1u3MDkX1s10X43WhweH9t2XY8skScVKn+C1hi0HzNUnE/VMsrU88fc5sZk8o4HODshI9z6pfy+9dpJlX9nbSezIlxNfCbcNCn2uv+ZpNcSLqVT5jkZ9//yvk9w89q9kVU9TnX/zq5cx+WXxuUuW9jXh9zzZoSHe6ef3ojvF1yRPKT9YH7uPHXbHzSWbrnakDpWls4kDMOU3HkHNquaGi40MiGfekCdukD33wwC+NAQAAAAAAAAAAAAAAAAAAAAAggcGmMQAAAAAAAAAAAAAAAAAAAAAAJDDYNAYAAAAAAAAAAAAAAAAAAAAAgAQGm8YAAAAAAAAAAAAAAAAAAAAAAJDAaFX0D+GNpu2UnjwkVj2/VIvGp3ZtFSlRbta3qzwt/25k9gsv/FH5nHn+8Tyh8lRmZmZoQfXpx3Kx6BUd7zG7s1ploWyhn76zTQt8e8ZxceuGcA+zg9VVKk9blIu3xzy8bLGY3ruv8HiEnal8ajZwmfKs0mxml5zLheeH4BeZnJPB7BNmH6VyvNCwndntTqfy6bP2CMsPkmM479jE2fWPiCjYp4XPg9TP7NY+bnvE8yAiag2L4xhu5zEB/tzmnTqJ2e9Etqs8oQFu97m5PbVyhsrzfvVOXjZdXGpr49ckqiHNPEPnaRVVqGsTF4BfueFZlecXf/kOszc3Pad8MudzO6sgxuzJFRNUnnCMi9G/8PpO5QMOHpbo/hxDfc8XdXW+7vIpsn4js332Dn7ckgqV5+KiZGbXd/PPc/RpaE1zCz8P9SgfWTxbXGOW4biRUCuzO3ramD2zoU6fp03cGHtA+VDoKWZ+JhZhttOhx4mMMA8Nuvt5nvRU3maIiDziO2h+O6R8mmv2PJNIb1iXFYB9QtexvihPW7mqS/ksPHE2s0tEg82drVuoq5O38wmUwux/7TC0PXDA2NnQwGy3Y4jPXXzw+N6Smw5mkY5QdBxKJPvafoNPkFl9HXw+l5Yj5nNDXsJOZda///GSyvGNn/3ZcByJ6RrGPm6Pi9xpQ2NvY1uH+rxmRxOzfQVlyufaH/yM2eklC0Y8b/FFMkXO5YnWvvhvZj/23e8z2w7yGIGIqKiRxwShjm7l0xHk/bPt2cbsr//4apWHCj6r00BC4knndSzLx/ug/Ew9lge7eWztzTIshfXy40TbRV8X0ROVQDafl2f6+Ln7LdlfEkViPK5wkv3KJ9fPAxRPFrfT5um5e3ERX6NzNrzPbJfHUP6sdO7jTlM+0Ri/htYe3k9lZxpmTdl8vpBenM8/d+v7ombhRUXKJ2/q5OG/U3tN49PYJLidKHVX+Dlrqn5GaT4em/oK3MpnRjGf95VfcjazW15+R+W5/vuvMrtUV1U680T+bLdX83n5nOl6vlleWsIT6vTY99orPP7bLsbCxno9P37nIZ5222/eVD4ft6j98ksnMntWRbryKS4YWj/uSdMxMhjC7eXjhmPp/sObw+9tqtujfEIh3n83NvI9jaYmXk+JiFrbgjwhhTckf6OOk/J7eXwVien+WxyVomKLJ2BYTJ4gVrlcnmJmV1ToPYHOMC/fMZVHKx+vl4+fdozXxR5DjB9y+DNwOXxcsAzln+zi/UnM1nFoaK919gGKE1Gv8hmTdG0hsofG3k9/6lj9uftvzPzily9n9jrDIf+L+HP8JiUrnwYx/mSEeR276muLVZ4vL1rCbNekY3hZohtUnrXCnqU8jnTkvNw0Gsn9QT53bzKsgR1M8EtjAAAAAAAAAAAAAAAAAAAAAABIYLBpDAAAAAAAAAAAAAAAAAAAAAAACQw2jQEAAAAAAAAAAAAAAAAAAAAAIIHZJ03jv9ZtJndSEhERvdSjtXsr07hmyuLKM7lDrdZXcHdxzVHfhGLl0+Xl76gv+ftDzB70aU2P1wq4hvHMC2cyOxLTmk0FVVywdaJb64BEvdxntXgHf8i0D+/i73enVP5e/lZLv6f/6k8sZLZVr3X5iitmcZ9K4eDRWgpEG4VdyqzLThaisEQ0adx6Zv/ynw8qHye8513sdjxxNI3tdDcle4baxLYarfW8ppbbfiEFXij0f4mIXEJfsSiQonxKiqYxOyJ0Vb2xJJXHcXg7sXKETlJAaz+FefMkf77WMk1z+Dv228P8vfx+nYXOvCrA7Fg7b2tej9YOWVF7L7NnnqUFll2iyY4v4pok72zSYuaegOg/tEQMOIh4xaO2Y9rHI57J2Q1vKJ+0d59gdrWQSerxjlN5BoiPC6tXrWL2Bo/W0dr8Dtd6mlyqNbyqGvhYV1XNOwJfQDeKbW89xuzsVauZnTSO66UREVEG77/jbbpNJPXw8fMzPq7z9N9rVqo8T77Bx4nIAH8ouXlaVNot9JSjUsiZiGo79mh4DMa0jhkAe1Ma4HZQyLd0jSLU+NG3LlRpbe61zBbNk+o36LF8ruiX3DbXbPLqcBEcQF56cQWzXY7u6zxu3ue8fNaLyueME+cwe2BwZN03W8T5lsXPbdu6IoYjXDcrYtB87ergPu2d7dzu0nF/czPX4+xo4T6hLq1x1N7ONciCXVozraedHycY4uUN9en73RnifXhvRGo06XYEPphQuIsG7aF4f3ut1nVs7+L3+4YffE/5jEbDeGSmqJRZZ/0Xsz/blcnsn950jcoTcfH4qS6s61B7N48bvvGN25idcwzXOgNgb+S461Wf64HZzgswu9NuUT4ei4/v2QGuneg4huBDJLkzeH0P2Xqs6eniff62Xj0BsonPF06fOps7bNqsy2IFmTm+mLfX1m59nvXVfG5QOlGvx4UifHxpiPB+qoD0ccdHhDZlB1+b8hbotYd1Yoxye/R8Yu81ukGDxmciYBt0sp1UnnZMeb7ymXfaLHEgfv/ufXi5ytMq7Gwd0lB2Np/Qn37aKcyO9uiYpmY7X48t69L6sn/861vMDlm8pT/7llzbJPrra0FdwI8R5Xo5lk44n/dLC84oUz7Ju7QyU0mPt2CIaJTfx5hBEj3aw3V3u6lN+bTVcq3t1g5evyMRfeBolD+XcITn8dVuV3nKvHxNt51M2r0S3mf6SI+FjUJXNSoGsYWnnaDy9PTyvjka0RrBjs3XhWNRMZ/Q4S21VnMle7dQaTYsDZJP7K9Ytt7WcidqO5g4m8g/pJedSXrOd/miM5jtofuZ/dkvX6XyyBX0r5OOaWaLeWFMjC2TZ+r5xYOPPsXsH978NWb/7CdfVnneFvZO5UEku1C9cnkk0T+iR1z0Qfk0weCl92cPFPilMQAAAAAAAAAAAAAAAAAAAAAAJDDYNAYAAAAAAAAAAAAAAAAAAAAAgAQGm8YAAAAAAAAAAAAAAAAAAAAAAJDAYNMYAAAAAAAAAAAAAAAAAAAAAAASGK0Y/iH05SWTbQ2JsefMKFKfL77mMp6Qfh63T52u8mSdKo5B39TnrfkBT3h/BzMtS0tb79jUzGyXwyXix5VkqTzedi5GH+vS4um5RaXM3pnD993943VZisJpzI40NTE7YGlxelcbl3zvrwsqn2r7PWbPLp3KHSyvykNRfo30ykZupx+tsuTNn8bsY4q1iPmb6/ao2tsmtfoxykBqEiV5huqA25+qPp8wgQub52UlMdux4ypPt7h/4fCA8snNKWb2+ob3mV0f0sfd1hDhCSEuaZ9ROV7l6RFladoZ1sddy+2AbFqGXsbjDzK7sjiF2flFmSqP7eLl99m63RTmFjB7SxVvayvW67JMKu9jdszRPuDgEYly2+3TPltEPbzwfx9VPvd7e5g9ccYEZq+I6gf7X7e9zOytNe8yu83W48T0zDZm17XuVD73vcbrZm3NWmZH7WyVZ2FhHbMnOnzceK9D1/eUWCuzq6u2Kh9fEW8TU8sCzG7vEmMCEf17PU9LcfH2mZ2j+xcf8fsb8CYrnwzX4PDfth1VnwOwN7O9hcz2FHiYvUHEUkREP/3+rcw+4Vg9llx313M8oYObQR360QviK5ZzA3xsnzuVtxEiouoNeuwG+0dbDe93yWUILFJ5n3PZdXcqF7+XP0iXw/stt+5myUW8QrhcPI9t67ElHOLjUbArpHyCIR57DISD3MHRecDYZcPG9eRJGaqfXZF09fmXbvgPZs8798pDUi4T82fNY3Znp/apE0323erNyucrN3+H2TPOWPKRywYSCDFh62jjla6toVdlCSbzuaR/oh5L+iOiT49yH6db9/k+MSZFHB4juNJ0TJyVkcvsfJ+ecwSjfMzqifC4Ip6uyx9p4EFNp5vnCTt8LWKogHzwC0X0Yk63uC9hMfZZEX7NREQOBZnttXieltptKs/WHj4/aNipY733NlUN/93XbwjaxihtLUTuXVUp2/DTn09cciKzZ1y1UDst5XPdW35yB7NfryLFKZV8jevyc49RPrMr+DpSVxdvj6mWbjebN/I59LI39Zz6Sb5cRQMk1rOqhP0x5IxF3P7OHXrNvGICn8+0DtQpn3EpGbv+Spw2sa/89vblzLYdXS8jxPsgD72tfCzi6+yyFpqWE30kx4FBZk2ldpUn5vBxopV6lI9H2XJc0OPETuJruuuifE+g5296ra2ogI9RUUOf39TUwn2i/DwV+Xqco3aexxb31hBiUnWUjx0FBp9s2tNvDZBeuxq79BHR7nUJvQ6SJh7BFV/isfeEEr3Pt2Dh2czuNpz1FZvvv5VOqGR2ZYXeZ/rvr3+J2YsXf4LZurYTTSLeJspm6/HoH2teYbY88wLDcfVKzqGhxXDmWuIxWIF4jqXE90MPNvilMQAAAAAAAAAAAAAAAAAAAAAAJDDYNAYAAAAAAAAAAAAAAAAAAAAAgAQGm8YAAAAAAAAAAAAAAAAAAAAAAJDA7JOmcebsceROGXoX/8mLz1Sfd7RybYUcqhUeRxmOqnVgJWll1/GEsmXMTOrW2oiF/65hdlU9VxrIcWulAX8+VwQYn5uhfSZzvY43+rcwOzOoslBhdg6zS8X7/3ObtG6M9Tx/T3+x1Nchoqoufr9Ty4VO5kx9vwfquS7MllquIeBP1d8jeKm3gSeE9Fvs8609Gg0xy6CVM0ZpiIQoZZc2ULehNZVN45qM4wNc83pztWwjRFs3ce0Hj0Ga2j2b16m/PcXrYQ2XFBiCP2p6xuJ6eUfdoFUDOoWuTV2DciGq52aH/Nwky5crXE7mekv17VrTo7Kc2wFPmvLpbOX5fOncZ+503Y48Pq4d57i4qG41Nao84MBhCV0Ny6Bp7Bc6kw+9p7W1Jzb+ldmVG05h9vwFWqyyMSQ06UVbc9xaE8jnCfAES2spRXt4H+lzCx1NQ6OwhSLNZqGOs6pO9xWdUV6+UERfo6+Na7pV5PIb7nbr/rrcI3TgLH5cq1fri0WiQV6WFn3vHNqj4TloQ+91rFJmaMM1uskynv3Wz1RaiZvX1VWruKZUwNIamXNLTmJ21tQS5XNU5W+Y7RK6fJvqdGHbRVK1GEqy/KjPB5VOEXfGDHptHq4J1GbpZ9ImNfU8PHBLchu+S+vwGN2TxvP4/XquQMTTXDl6rjOllJ/LEmXzZej+PNXF8/R18XEiFtPX7Ni28NH3rj3CK3hHB7/fpqg+HhVzr8iB0RVM8u8JEONxh6jHpGY29qhu7ia3a+hOX3T559Xnp17+7UNdpA/E6+Fxc6uehtPGWj4b6LV0fV5w4bkHtFwgwYjx/jDazvtqd0DPE5s7uc5qzNKT91wv145rC/P+MC9LazLaYgrtS+VjgCegdQVJxB4Br1+XN8on72+u2cTs0hO0kmNewVRmL3v9WWZHDWqbU8r4mlHZ9OOVT9W6DczevobHYDESQuakr2nW9EnMbnd0nsIsfk0xvUxGhXl71lMifQNE9I52GoP0holiu5bcZk7PV5/P+I9v8YS3n1M+9/6dr6N2ipBgfLE+7ze+x0V3Z8ycpHy6a7Yze0UV10jdXqX1q51krvH6bvWg8hmL0fVVN3D7mz/mWtRTM0sNuWR/ZlIV3c1YvGsHhk2OXmMciWSlWCzViDUl6nkRufZaByHSuseTaaLKY4s9C0/UtH3Dn3e7KK+HAoYcPLLvEmV7o/UVfZpWw6n3kUizXtguIblwwK+5yzBm7RBPwLTM0LPXfRjpeY0lehsbKblnKChxuQ1zyTzZyfO6cOo5Z6k88TjXhHYl6YFikLgmvWXJ+XJA5bnokguYnT/OpE7NaRRKx+dUnKR8vjqX96nxe/7F7C3E4xkiIjmi6khPaw/3ibrpMtQ0OQqLHTsKk9aQXkFBZp8u9lVLSeuJH0zwS2MAAAAAAAAAAAAAAAAAAAAAAEhgsGkMAAAAAAAAAAAAAAAAAAAAAAAJDDaNAQAAAAAAAAAAAAAAAAAAAAAggdknTWOrwEOWe0h74q11K9Xn29q4zuGi2eJd23lat4RIvg/9HIPPcm52c1078h2nchxVVMHsTuLvzz/ap/fLffQ+s3MMGiqWkKQpIK4te/R0riNDRDR/6jHMfu35l5kda9OaFJVBroOQVteifCryxTvqbfGe9TotMpUyi2vUVM4oEx6TVJ7z+vm5a9ZrLYLLLjlh+O9wT5Qe/sXtymcs0ha2KXmXFsyWOv25y+pittPL690rLxsUGIRGcOs47dJYw3Unapbvx3vtRfWJRnSF9/aKOtVg0EiRMhOyaQn9YiJSsiyyRvm0rBMV5nKlAbdBl29bPde+m1jONaVzDXpRm2u4UHO3c2g1AhIdj5Ax0aolREKSkaYfo/vZ9Y28TWyoev5D7SEC3LSEjoZXq1ms8HGtCm+G1imLWULLzMXrVNQg0LVlLdefsaP8TliG4bo3xPsXin2YvtEuVr/HbUvrc5BqA1LLR2geExEdYm2NjxcW7dGLGfuqOq6AjhEqCwLM3lDDteI9Aa6RSUQ0cQbX9OqJcI3MUxZzHRwioi/8+EfMvvzii5RP6cyTmb2x88+8LIbYzy9CsGYhSx405AEHEJ+oH3JQMOE2iGt7uXY8pXIdp7ht6Md6eZwWtXj9jkX0fGJQlC/F8B3diOjjvV4elMUcrUPlEprGMaFr77LE9RFRjLhPOKb7b8fFA7VxJVxfKWa43y5L6ImKawxH9Wg+2CKEP0O64cRZmkNEiaFpnFUyl1LdQ+P8JV86sudQvkI+dzeN/i1hHjcUlBylfArGTTuQxQIJRqyb90sZ7gCzj5k1T+Wp9PK01euXK5/Wer5elZ/D5wKebN03O6IvtvmUhHyibEREuQV8Dcnu1bqZG6rfYnbUxceff7S9ovIcVZDH7OZu3s9GHT0GBLJFgdP13OaoqTOYvWLTOu5g63Unx5XK7Pd28jl3oUHrefq4yaJsOcrHk77nPoQjiTP/yMwh2jVM0PhifV/ovXeZ+fhfn1Eu1U08gN3Gl29pRiWPi4iIGjfwtddwwxbls3MHX8A6uozHEQG3HgOyPHzevbJGry9T6OP1fLOmc/u3/+805XPFaVK3U8ZYptUQGUeadI93H+fjdc8OJXLVw23QKc0V6/udBsXciLjHg0JXuMTS89pZuROYvaOV94dhg3ZyrTh1h9AeJtK1pUaVV88Nmimu0g4F2wzX2GlI2xvTp4VCh9dluJ7mvf42taixitvtIbd76JnHbMMCRVu9TtuLVL+OcSiVr6nbcX2MGSU8vorKmMDRT2HOLL5PtqP+w8tGRFQl2l7f33+ifNKEQnHSdF62ivVa03ikFtFBGSqtgXhfnk46ppm9gO9LVi9/ltmbSSwqEVFQ9Peyv+kmwwaR0JQ+kOCXxgAAAAAAAAAAAAAAAAAAAAAAkMBg0xgAAAAAAAAAAAAAAAAAAAAAABIYbBoDAAAAAAAAAAAAAAAAAAAAAEACg01jAAAAAAAAAAAAAAAAAAAAAABIYFz74hxzeomcoX1m29FZj8rLY3b8qRpm7ywLqjye2WcyOy9jUPkM9HMB7zU1TcyeN65D5TnujCu5rTy2q5S22L3M9sd6lE80hwvJH589jdmz5n1S5aHeWmZ63PwYW8b1qiwvhLuZne3VPnYZ3/M/bX4xd8hYoMtCJcLmZWuhpSpHfTOXn58zVQtvZ6TvuQ+h1A8Xsx9LHF0xntzeoedQkNOpPvf283bSuoPX3XSDznxE3N7ikkLlE0vhwvJJXF+d8uVjJiJ/gNtTK7zMHl80oPLMdXNx97zjdFvr6e1jdlFWMrOPqxD1kojI4m066rKZHTHcF8vF76XX61U+3lAXs3e2tzA7P5f3UUREnmx+sq7GfuUDDh62LWyTk8PNnLmXKJf05zcyu5feFR61pAmK8wg7rHP0iLSeZsNhP044I7vsH/mGtL37k0Ei2mjwGYukElHSrr9N39UzVLSPMVU7dQxQOv7D44LF3/mqSrvthm8ye/yESczOKeCxFBHRyoa3mP3UT15UPjd+rpzZ4Rz+eSsfRoiIaOc6brvF8OPSwxE4kMR4zEBRwzjtTuF2IEv7JIv21yNia0ech4jI7WNmnHjMMGgatPp4xzpg+ZRLqJ/HSrY4btRwXFsMmNFenmdA3iciSrJ47BQPm6Z+4t6JeQrZhvviEoGaJQcTUyAnGpvb0B9aez3buE3UV6d9xiDfuuWn5M/Q9eRIJC0lZUSf5qY2ZheVVnyAJwD7R5HHz2xbjAGtbbrvcHl53zbOq8eJYl8mswtzuU+oU8+Fp80+htmxMO+rM726bR9dPpnZm2u2Kp85049idn0bH7OqarepPE3123lCKi+L49eBvyvE1ydCzz+qfLyiv/YX8+P4LR2TuUVaZwfvFyzDUmRT006eJxhVPrl7zedjEcP4NEY54WIXeT1D84lJGX71+dtLX2D2hFK9hvTCe/XMfqaVr71m58RVnsZNDczuLNJ1KJCfweycXF7vZlTyNkJERF28Xcyd26BcHqypUWmHi/mXcPu8y/T9vewzs5hdQYZYlLoNaSMh41697rhnjpk4bWJfsYfn40M4hoWQVuITwai690RxcRzJaqddJ7by59IjjruG9FzZEcXTrZ4oW9hBYdcY6ptceUgS16N7gQ9O3RdMRzBMu0dkhzjSSL+E/Ogl//iQkuahlN2xjh4+R6QvrPuWWJDHCF5PgfK56WtfZPY1N/8Xs99Y/rrKc+65pzP7lRdH3p6U0+M0o1crN9c/NeJxP7xFE+VQmU773rfFeVYqn01Ln2b2HHEF7xr68lPO4etizSIm80007PPt+ItOO0Dgl8YAAAAAAAAAAAAAAAAAAAAAAJDAYNMYAAAAAAAAAAAAAAAAAAAAAAASGGwaAwAAAAAAAAAAAAAAAAAAAABAArNPmsY3fe5M8u0SYc02iG1F13MtlqSLL2B2cY7QsiIiosnCTlYeKalLmF1ewd+pXlujtSpLx/WJlDnC1toyuTPFu9mj+l3t7a1cxyajoEh4aL3fpk1vMtsb5XoGU4r1efKncx3YcVGtTfFG3XJmvxfkOiAzMt5TeYhOEfZEZnlolcphubgmUDCsv2vg3uvd7P29+/Hy/I8prliMXK6h+3H6zOnq876OILPDQvf7qKlaRyMa5doyJ5TPVD6e/hCz//NmrhXm8ur64vLwNuEVAgDjLJ0nq4TXzclOpvKRVxDt4Vo5WTlan8Pn5bo3TWFetqhBP6+1ibe97OIJyie3kPcxnTF+7tZOrqVEpMtfPJ6rhaykEIGDh9Q0ltotRKRGqu5iOW4Q9S7+Ck9o4OMENRs0jXu45jW17hQO9aSRyiujEQWWahtaP1yLjsjhOd2QR/bFBg1JlU/6aA0yrYUkBFs9hrE8W+iF5+mxkPnYvUSvLTKceyyyd/82kmLKx5+ff+uTKm3aSbzO/PLf9zK7v1ofZ2XV33lCLa/LDyzVba+tc2R96Pfr+MlkNFsj9IuJiGiDsMVw7zU1I3DgCEsNScP0RWoah3u1jyOedrSH28av0opEj+jPs3RcZDixSokLDeCIPShsHQfF+8Q4YYv7IAdUIorHTOPNCOVTA7Mpi0h0yUag53Pkls/NJNy813NMIBGyjPzplOE3qdV9HNDjWkM774snte2Pah0AH0wgk88la+p5TO+k6nHCivC07Gyv8vGIscQj4maPX6/bVJQJrTvRtWUE9DgxKNZ2vF4dwx9TWc7sQDsfC7elqizU28T1K9OERn2vR3foMXGrgtGg8mnv4OOlx83HI8vR99uVxsfL/HSu7+qz9DW3d3J9P7croHzY/R3UH49Vajts8ux65htWau1El8PHkOZu/az/sOzDb9hDm3RawM/XTe16rTN8wcW8rrqzePt89721Ks/2TbxOjS+bonySiZ/roD1uQ/z356e5WuwXzj1TeOi+gEiuNem1J/N8fW9MQZeMCfX62p6GMZq4LzGROuomTeM+ca+TDTGOIwJUqXHcawhg3yA+L7FEbR7NUysm3ekHxDW4RGVuNGgyy9myJerk4BGuiy3vbgINAyNjJQ39IyLL0h2bbZgr8vyGvR4xf4tGdUw/sZT3h3HRRz32tydUnvJJlcx+7qkXPrxsRIdxZV6vJQ/+/E6eEHtf+UxVKuOcE5UqOVFRfgmzN51wNrOtqy/RB/oyNI0BAAAAAAAAAAAAAAAAAAAAAAAcBLBpDAAAAAAAAAAAAAAAAAAAAAAACQw2jQEAAAAAAAAAAAAAAAAAAAAAIIHZJ03jsrmnk9+/S/uld4d2KMrlds6FwsEk+jZpX4pARERZ6fO4PcMgvkENwpaaE1rvN+loodPYbdC5EWoDUZ/UQdBaj+OO4+8gb6vi564s1To33kquB5lj6/39ozxca3hGMddBbqrT71QfVyL024jrjWTSf6o8xxfdLVJM+h177lXqoEljY2xidYfJig1pBnjGGbScU3n9KJrK31mfnaeffWsT1z7xWM3Kp7GO67UWFHL9JZdB48iW+nMO16qwiWsgERG1CU3mmEfXw1JR76p7eBvxFfJ38hMRNe3kmkyb6/n1+HK55hER0WSh/Z1m6L0soQE4PivA7KjUZiWioM2fW3uLbCPgYBKVspMp2kdJXpmkgIqP5rZX9FNlM3QeeZydQsO4xqBp3Cvqh8vw3as0MdZ5RIM0aFVqeeKRtSrVBQjNqaGyCE1jjyib23AzI9EP9/EZ9JXlcVMNx91bIyXWQ/Sadhn7mL6rd3iUeHIMIZlfdJHhKNdoaotojSYZEXz9fx5XPu/W/w+zbSFZk3miLsuGeh5n2kH++UYtqUaUL+xW7bJ8Dbc/Wzqe2cdM14PLMuJl2STl0PXQAg4o4plYpumLaEdBqYNMRH7Rd0kd5GgfacS5ZNjfq7XCKJ2fJ8kyaBr38PLFU/h5Ug39eb8UyuwRfXWWQRO3X+Qxjj/yGkU/ZRp+pOaVS2gYRw3qbLa4v6bxc++yxKFS9nHg3HMXqrSlS5cyu7m5RfkA8FG4/OcvH+4i7MUjh7sAIAE47rRjKD19aKx9pGqd+nzdO1ztceXOA3Pe367kwfR0w/yhPVLN7OZWHjjUVeu14+1VfIx3+/Va2sGKAs44n9s/uXuS8jm+RGosy0WL7aM4k2n9Wy7UyVjIFHTJ2M20XrV7DRGx0wch9X5jBk3jZOFjGefuPG1Q+Zh0kKUt4+SRf8/XaihvUNUXPlc3rFAbaphhLgM+ntiDQ//IrGlsSmOfG9bLpYa6ZZjXzj12GrMzKMDsp594VuVpb+D9WM020+IORw5reldP97CFwtY7MKOhXaUkx17fryPtjd7pIXruIb7/9rn/9x1mb55umO8fRPBLYwAAAAAAAAAAAAAAAAAAAAAASGCwaQwAAAAAAAAAAAAAAAAAAAAAAAkMNo0BAAAAAAAAAAAAAAAAAAAAACCBwaYxAAAAAAAAAAAAAAAAAAAAAAAkMK59c7+AiHaJLqdX64/L3SJh8v6UaRRMFLZJ3r1L2FLu3bRfns9Ng0J2zjguq+3P46LxA/R3lcfVxm/z1NmzmB3N5ALgRETeNJHQEVY+M+bMEinFzBpXMo0024U9XdjJhjyfEHaRwWfvfCHD52OTCW4vedy76lJXt/q8z81F4xs6+bO2bP45EZHL5nXVtnT99uTw+uvEeD30uGQFIios5PXBn5HB7Fi3Ln/MShK2ciGfi7f7ypKj+HkcXZYI8ftwdNE47uCVfQlRgY83yKo1G5VPSQmvmzGHl7+tRbcjv+3hZbMdVVpw8BDVnTypI/u4dfWg1Fxez/o9wsk02kVF24rEhIOHFP0iT5pX+8h24pFtwNCQPOJcjjhPUI5pRCSr6oRx2iczXeQRmZy4ztMvbngKv3kp2XpwdLlSeILhEl17HSYedZNujYmAfGgHjzIft79980XM/sxln1F57rj7MWZvqelgdqxe18Mrv/QfI5bl9U3LmD1tKv+8Il83/NU2r7vtmZ3Mtt80nKjTkCYYFLYdbWR26cxKlWe7j5cvWMPH3PnH6PM8sXLksoBR4hrFdEX2545h7A4KH5fod23TfEKOA2KcCPbqLCKIj9OA9okEue33M7N/wHDNMdE3R+RxDddsyc7Y0Dlb4lwRcZyIbDVEJONMVx+31bhHRG45ThjKYhvuFTiiWXj26Spt6dKlzHYGD93YBwAAY5HTT19Afv9QPFqaPVd9/sLSbcxOe1yvlWyq5nF8S6hf+YzEekOotH4Dtzt7apn96XN0edsb3mP2jmq9hlgm7JpRlK98DrdvvSWgfK646DiRYpjzk4hZyC/soCGPXBcwLFqodWm5/mB6JjJeMsWru+dsGG8/iH7jveXIiHdQpewfhhWXfcZU+n1vwWAsE7dtiu9aPDX1BJZp7rX3547us5Ickcel55u26Nd+9P1vMPs/f/w9lee11/hx22JbP7RsRETrhf17yjWUpZ3ZjeLzBYbjnijsk0csiabPkPassJcJ+8+Gxatbr+Brdp6KPGZbcj33IINfGgMAAAAAAAAAAAAAAAAAAAAAQAKDTWMAAAAAAAAAAAAAAAAAAAAAAEhgsGkMAAAAAAAAAAAAAAAAAAAAAAAJzD5pGmdmZh+scgDwsaQwM0Bp3iE95/rOnepzp5DrIMZ6+Pvno47QGyWi7GzezsJS25SIsguFporQYrVsrUUQifC37EeivCx2VCtieAO8LG5DjzG5nGuXB+sbmB3t1FrJU3L5e/k7hY5gUGm9EEWDXDvBiWqf7g6uhePycFFPl60vINjO9ZWDPfq44OAh9dtjBuleS8g2FBi+7nRMAbejmVyfPaSl4ykq9KujObzd9Lh1+yTKYlZyZobykOX1+Xg9tKO6MG6h9djRIuqh16SdzDW7lX4xESUHeF/hd/P74pIySkRkizQpY5Ir7jURkZA2Nyot7X0Yp48SVNNYKxqlu/hzvPYSLpB73WWnqDyFZdwno7hcnypnmkgYL2zdN//4h7xBtqzfzGyXo/vHnNMuZXZt7QPKp+qt15hdftx0Zrv7dG2YX8LHn7/+g2u+GIoyOkT9fq2N25vqhTAbEbW2cnuikA8/Ol+INA8dad/LBszITslj6A9tUSFkx2XCLTouv+7PKSj6a3sU6mERoXMsBwUiIo84d0i0gRDXYxpC6v2KawxL/T2Dz2i+Lyy1no1joUCWX+okExFJDSaTVrVrrzHqEGs2gf3jmFlyrCHK8/OxpLDIEDgAAADYB4K0WyO3pblWfRoJ84D26sUnKB9XKo8BdnZw3ePjLyxWedpjfG3nv3+i4+RNL3O7KsjtKVN1fHJs+QxmP/nwKuUzS4R7W8RxF35FH/ekxTxIn5plGn9k/GGKn6SGscxjWhvPHCEPEZGYeJDUBzXFXHJmbdJK3n2dg0Sk1yUBAGMfZ8AmJ7ZL09igX6yTRIJp6iUX+gwS8OniMJcu5rq8Dz/xnMqzYv2rhpN9OPXC/oOhjy11VzI7HONrXs+Mon+cK2w9mhI1C/sNg0+DIW1vrjn3cpV27pcWMdvu4XPsmHeftnE/MvilMQAAAAAAAAAAAAAAAAAAAAAAJDDYNAYAAAAAAAAAAAAAAAAAAAAAgAQGm8YAAAAAAAAAAAAAAAAAAAAAAJDAYNMYAAAAAAAAAAAAAAAAAAAAAAASmEOroAzAGGNnRxt5IklERNQa61OfT0hOZ/a4LB+zJ+cXqDxOKm+WO5valE8sxMXQ/U6A2VbUVnnCQja+S/i4iB+DiGhHE5duD9sDyseflcbsUDvPE2noUnk8Lj+za2NRft5OnScguqtoV1z5NPd0MtuXw6/Rm6q7vOq2fmZ3RpULOIhEerntRLSP281tl+HrTjni0caEj5t0fXH8XmZHffxEwVCPyuNy8/ru9XqVT8yO8TwuXjg7mqqPK31ivK+IZejzWB5+HF92pvKR987HuySydFdBsX5hCx+P4f57Pdz2u7VPdK+2NTioP08E5lZOUmm/vfl8Zs+7cAZ3iIpGQkRUXCSPbDibeAgD65hZ/czfVY66Hbz/PmlmMbPTynIM53mHWWvef0x5FObxdtPWspnZG1/XY8tEamZ2iuibB/e3r+bNk+pXCXu9IY/oX8rP4HZekR7LiTbta8nAByLaQMwwfXFiOk3iFv1oTOSJGAYgEh2gX/Szft5XD5VF2DFDZXXEcV2iDZiuUYwTFAkLB3liInKJfsA2NRwRZ9riPLbo4ImIKEvY4j5EDeeR5Q/L8pMYgAwDFDjicLt1/Tjv/LO4jzf7UBUHAADGKDbt/s3PnAV6zhcZ5GP5e2+8p3xmTC9j9ikLS5hdfHSG4bz8XKc/e7TyaO7nc2arka/ltL2zReXxhPlcpnSqnh9vqOaT0vkn8c+vvmaSypOcLOMnU9wjY8Z0g4+MCWVMYlpKlz6m32gFhS3neaY8o4hxh68BsRMAiUp2+TGHuwiHmGaVUhvTafvKqhHsA8W9S/82qrTDCX5pDAAAAAAAAAAAAAAAAAAAAAAACQw2jQEAAAAAAAAAAAAAAAAAAAAAIIEZ1eup43H9Wk8ARmIs15vd19Yf3XONMf12TYpG+KsC+wa53RvWr49xxHGivfp1g7GBJGanOPxdr5bhLTxR8drC/n7+fGzDaw1jfdxnwPC2m0gPT+wT5Y1GdD2Iu0RZYtwe6NN5YuL1wqa3PQ7IVxSL4yT36+OKNwnToOE5HigSoU3sKwORELMd/ZZ3ShKvMnaStY8tfGxxnMGo4fXUos4PiteFOv361ZlOnFeQQcP7nR1RqRzxSk4nql+BKn3i/dwnPqjbZzyJv7LLiSYpH0dkGxRfFYsb2vSgaBPyLaqDhldPy+Oa3ow1uFdxB/uGnnuitYlBw3MM9/EbHuoRldf0mteQrEMh7aNeT83rczgi3kNORL2yLL3cZ6DHVBb+WrVIRHeiUdEX94t7M2B465rs82UbN7xxfv+QxzG84VemSaWGaN+BexVcorWJ0SEeQNz0fnuZZniQMl9c+oziuLLTlB2k6dQmH3kcVTZDWeKyjx9Fo1DHMVVwmW80jUIeV16P6ZqlbWo3e6cNnSMR2kQoZOrDPx709moJhdiAfN267uQ/ztd8uEmENgHAvjCW682ecWJPv9rfq2Pt3ggfl6P9euzujfBxtyfMJ3B7n+ODCBlignA/P67Vw8vSGzHEEX3cpy+mn6E4LEXFUBIK6VgpOVnGSnp+rCeqpuseaXJrqnMyj+k3WvJc/7+9O4+Pqrz+B35yM5lMJpPJZA8JIYQQERDBouKO+67FtdpWcWnVtl+tra2/tn6t2lr7bb9ttV9b29pFbau17ktVXFERFREBEQKEEELIQtbJZDKZTG7u/P4IBM45D2RAFs183q+XLzl3nmfmzsx97rPczD2J3NI68XlGKDRUNhnaBMCuGM3HzWh+b7D3JHLcpMQTKLVp0yYqKysbqRgA09DQQGPHjt3fu7FXoE3A7kCbAODQJgA4tAkADm0CgEObAODQJgA4tAkADm0CgEukTSR00dhxHGpqaqKsrCxKSTH9pRbANvF4nHp6eqikpIQsa3TeAR1tAnYF2gQAhzYBwKFNAHBoEwAc2gQAhzYBwKFNAHBoEwDcrrSJhC4aAwAAAAAAAAAAAAAAAADA6DQ6/8wCAAAAAAAAAAAAAAAAAAASgovGAAAAAAAAAAAAAAAAAABJDBeNAQAAAAAAAAAAAAAAAACSGC4aA4DR4sWL6aijjqLMzExKSUmhZcuW7e9dAtiv0CYg2dx+++2UkpJC7e3t+3tXAD53trYfANDQPgD2j/Hjx9PZZ589Yrk333yTUlJS6M033xzedsUVV9D48eP33s4BCOgrYLTCPBsgcViL3T9c+3sHRrOmpia6//77ac6cOTRjxoz9vTsACRsYGKCLLrqIPB4P3X333eT1eqm8vHx/7xbAfoM2AQAAAAAA0n333Uder5euuOKK/b0rAAAAAKMG1mL3H1w03ouamprojjvuoPHjx+OiMXyu1NbWUn19Pf35z3+mr33ta/t7dwD2O7QJAAAAAACQ7rvvPsrPzx9VF42PO+446uvrI7fbvb93BQAAAJIU1mL3H9yeGgCU1tZWIiIKBAI7Ldfb27sP9gZg/0ObANjz4vE49fX17e/dABg10AcBAMCeYFkWeTwesiwsGQIAfN5gng2jBdZi9x+MAHegsbGRrr76aiopKaH09HSqqKigb3zjGxSLxaizs5O+973v0bRp08jn85Hf76czzjiDli9fPlz/zTffpMMOO4yIiK688kpKSUmhlJQUevDBB/fTOwJIzBVXXEGzZ88mIqKLLrqIUlJS6Pjjj6crrriCfD4f1dbW0plnnklZWVn0la98hYiGTs433XQTlZWVUXp6Ok2aNIl+9atfUTweZ8/d19dHN9xwA+Xn51NWVhade+651NjYSCkpKXT77bfv67cKkBC0CUh2wWCQrrjiCgoEApSdnU1XXnklRSKR4cdt26af/vSnVFlZSenp6TR+/Hj60Y9+RP39/ex5tubRe/nll+nQQw+ljIwM+tOf/kRERK+++iodc8wxFAgEyOfz0aRJk+hHP/oRq9/f30+33XYbTZw4kdLT06msrIxuvvlm9ToA+9o777xDhx12GHk8HqqsrBw+rqV//vOfNHPmTMrIyKDc3Fy65JJLqKGhQZVbtGgRnX766ZSdnU1er5dmz55NCxcuZGW25kJbtWoVffnLX6acnBw65phj9sr7A/g0EmkfifYjjuPQ7bffTiUlJeT1eumEE06gVatW0fjx40fVrzxh76mvr6dvfvObNGnSJMrIyKC8vDy66KKLaMOGDazcjnKpPvjgg5SSkjJcfvz48bRy5Up66623htd8jj/++OHy69evp4suuohyc3PJ6/XSEUccQS+88AJ7zq35gx977DG64447qLS0lLKysujCCy+k7u5u6u/vpxtvvJEKCwvJ5/PRlVdeqdpGom1oq1deeYVmzJhBHo+HpkyZQk899ZRxn7bPaWziOA7dc889NHXqVPJ4PFRUVETXXnstdXV17bQegIS+ApIR5tkAO4a12P0Lt6c2aGpqosMPP5yCwSBdc801dOCBB1JjYyM98cQTFIlEaP369fTMM8/QRRddRBUVFbR582b605/+RLNnz6ZVq1ZRSUkJTZ48mX7yk5/Qj3/8Y7rmmmvo2GOPJSKio446aj+/O4Cdu/baa6m0tJTuuusuuuGGG+iwww6joqIievjhh8m2bTrttNPomGOOoV/96lfk9XopHo/TueeeS/Pnz6err76aZsyYQS+//DJ9//vfp8bGRrr77ruHn/uKK66gxx57jC677DI64ogj6K233qKzzjprP75bgJGhTUCyu/jii6miooJ+/vOf00cffUR/+ctfqLCwkH7xi18QEdHXvvY1euihh+jCCy+km266iRYtWkQ///nPqbq6mp5++mn2XGvWrKFLL72Urr32Wvr6179OkyZNopUrV9LZZ59NBx98MP3kJz+h9PR0WrduHbtI5jgOnXvuufTOO+/QNddcQ5MnT6YVK1bQ3XffTWvXrqVnnnlmX34kAMNWrFhBp556KhUUFNDtt99Otm3TbbfdRkVFRazcz372M7r11lvp4osvpq997WvU1tZG9957Lx133HG0dOnS4b+efuONN+iMM86gmTNn0m233UaWZdEDDzxAJ554Ii1YsIAOP/xw9rwXXXQRVVVV0V133aUmwwD7W6LtI9F+5Ic//CH98pe/pHPOOYdOO+00Wr58OZ122mkUjUb39VuDz6nFixfTu+++S5dccgmNHTuWNmzYQH/4wx/o+OOPp1WrVpHX692l57vnnnvo+uuvJ5/PR7fccgsR0fDxvXnzZjrqqKMoEonQDTfcQHl5efTQQw/RueeeS0888QSdd9557Ll+/vOfU0ZGBv3gBz+gdevW0b333ktpaWlkWRZ1dXXR7bffTu+//z49+OCDVFFRQT/+8Y+H6+7KWKympoa+9KUv0XXXXUdz586lBx54gC666CKaN28enXLKKbv0/q+99lp68MEH6corr6QbbriB6urq6He/+x0tXbqUFi5cSGlpabv0fJCc0FdAssI8G2DHsBa7n8VBufzyy+OWZcUXL16sHnMcJx6NRuODg4Nse11dXTw9PT3+k5/8ZHjb4sWL40QUf+CBB/b2LgPsUfPnz48TUfzxxx8f3jZ37tw4EcV/8IMfsLLPPPNMnIjid955J9t+4YUXxlNSUuLr1q2Lx+Px+JIlS+JEFL/xxhtZuSuuuCJORPHbbrtt77wZgD0AbQKS0W233RYnovhVV13Ftp933nnxvLy8eDwejy9btixORPGvfe1rrMz3vve9OBHF33jjjeFt5eXlcSKKz5s3j5W9++6740QUb2tr2+G+/OMf/4hblhVfsGAB2/7HP/4xTkTxhQsX7tZ7BPi05syZE/d4PPH6+vrhbatWrYqnpqbGt061NmzYEE9NTY3/7Gc/Y3VXrFgRd7lcw9sdx4lXVVXFTzvttLjjOMPlIpFIvKKiIn7KKacMb9vaPi+99NK9+fYAPpVE2kei/UhLS0vc5XLF58yZw8rdfvvtcSKKz507d+++GRgVIpGI2vbee+/FiSj+97//fXjb1nOs9MADD8SJKF5XVze8berUqfHZs2ersjfeeGOciNjYpaenJ15RUREfP3788JrS1nnGQQcdFI/FYsNlL7300nhKSkr8jDPOYM975JFHxsvLy4fj3RmLPfnkk8Pburu742PGjIkfcsghw9u27tP8+fOHt82dO5e97oIFC+JEFH/44YfZ686bN8+4HWBH0FdAssE8GyAxWIvdf3B7asFxHHrmmWfonHPOoUMPPVQ9npKSQunp6cO5XQYHB6mjo2P4Fg8fffTRvt5lgH3qG9/4BotffPFFSk1NpRtuuIFtv+mmmygej9NLL71ERETz5s0jIqJvfvObrNz111+/F/cWYO9Dm4DR7rrrrmPxscceSx0dHRQKhejFF18kIqLvfve7rMxNN91ERKRuwVhRUUGnnXYa27b1F5bPPvssOY5j3IfHH3+cJk+eTAceeCC1t7cP/3fiiScSEdH8+fN3780BfAqDg4P08ssv05w5c2jcuHHD2ydPnsyO86eeeoocx6GLL76YHb/FxcVUVVU1fPwuW7aMampq6Mtf/jJ1dHQMl+vt7aWTTjqJ3n77bdVGZPsE+KxItH0k2o+8/vrrZNs2xk3wqWRkZAz/e2BggDo6OmjixIkUCAT2+FrOiy++SIcffjhLHeDz+eiaa66hDRs20KpVq1j5yy+/nP0yd9asWRSPx+mqq65i5WbNmkUNDQ1k2/bw6xAlPhYrKSlhv3L2+/10+eWX09KlS6mlpSXh9/f4449TdnY2nXLKKaxvmzlzJvl8PozNICHoKyCZYZ4NsPuwFrt34aKx0NbWRqFQiA466KAdlnEch+6++26qqqqi9PR0ys/Pp4KCAvr444+pu7t7H+4twL7lcrlo7NixbFt9fT2VlJRQVlYW2z558uThx7f+37IsqqioYOUmTpy4F/cYYO9Cm4BksP0CDhFRTk4OERF1dXUNH8fyuC0uLqZAIDB8vG8lj3cioi996Ut09NFH09e+9jUqKiqiSy65hB577DE2sa2pqaGVK1dSQUEB+++AAw4gIqLW1tY98l4BdkVbWxv19fVRVVWVemzSpEnD/66pqaF4PE5VVVXqGK6urh4+fmtqaoiIaO7cuarcX/7yF+rv71dzDVObAvgsSLR9JNqPbP2/LJebmzvcLwGMpK+vj3784x8P57rbupYTDAb3+FpOfX09O9a3knOCreR4Kzs7m4iIysrK1HbHcYb3d1fHYhMnTlT5mreOp2Ru552pqamh7u5uKiwsVH1WOBzG2AwSgr4Ckhnm2QC7B2uxex9yGu+Gu+66i2699Va66qqr6Kc//Snl5uaSZVl044037vAvdwBGg+1/ZQ8AaBOQHFJTU43b49vlT5WLjzuy/S98tt/29ttv0/z58+mFF16gefPm0b///W868cQT6ZVXXqHU1FRyHIemTZtGv/nNb4zPKxdUAT5LHMehlJQUeumll4ztyefzDZcjIvrf//1fmjFjhvG5tpbdytSmAD6PEu1HAD6N66+/nh544AG68cYb6cgjj6Ts7GxKSUmhSy65hK3l7Oh4HBwc3Gv7tqPxViLjMKJ934Ycx6HCwkJ6+OGHjY8XFBTs0/2B5IC+AkYTzLMBdg/WYvc+XDQWCgoKyO/30yeffLLDMk888QSdcMIJ9Ne//pVtDwaDlJ+fPxxjMAPJoLy8nF577TXq6elhf82zevXq4ce3/t9xHKqrq2N/Rbpu3bp9u8MAexnaBCSTrcdxTU3N8F9wEhFt3ryZgsHg8PE+Esuy6KSTTqKTTjqJfvOb39Bdd91Ft9xyC82fP59OPvlkqqyspOXLl9NJJ52E8RV8ZhQUFFBGRsbwL4S3t2bNmuF/V1ZWUjwep4qKiuG/2jeprKwkoqFbhZ588sl7focB9qFE20ei/cjW/69bt479MqCjo4O6urr21tuAUeaJJ56guXPn0q9//evhbdFolILBICu39ddewWBw+PaeRPrXwUQ7XvcpLy9nx/pWck7wae3qWGzdunUUj8fZfq9du5aIiMaPH5/w61ZWVtJrr71GRx99NP6ACXYb+goAM8yzAXYN1mL3LFySFyzLojlz5tDzzz9PH374oXo8Ho9Tamqq+qvOxx9/nBobG9m2zMxMIiI1AQEYTc4880waHByk3/3ud2z73XffTSkpKXTGGWcQEQ3n1rjvvvtYuXvvvXff7CjAPoI2AcnkzDPPJCKie+65h23f+pfKZ5111ojP0dnZqbZt/ZVlf38/ERFdfPHF1NjYSH/+859V2b6+Purt7d2V3QbYI1JTU+m0006jZ555hjZu3Di8vbq6ml5++eXh+Pzzz6fU1FS644471BwiHo9TR0cHERHNnDmTKisr6Ve/+hWFw2H1em1tbXvpnQDseYm2j0T7kZNOOolcLhf94Q9/YOXkeAtgZ0xrOffee6/6BfHWP+J5++23h7f19vbSQw89pJ4zMzPTuOZz5pln0gcffEDvvfcee47777+fxo8fT1OmTPk0b4W9DlHiY7GmpiZ6+umnh+NQKER///vfacaMGVRcXJzw61588cU0ODhIP/3pT9Vjtm1jHQwSgr4CwAzzbIBdg7XYPQu/NDa466676JVXXqHZs2fTNddcQ5MnT6bm5mZ6/PHH6Z133qGzzz6bfvKTn9CVV15JRx11FK1YsYIefvhhmjBhAnueyspKCgQC9Mc//pGysrIoMzOTZs2ahdxjMKqcc845dMIJJ9Att9xCGzZsoOnTp9Mrr7xCzz77LN14443DE+6ZM2fSBRdcQPfccw91dHTQEUccQW+99dbwXzXjL9pgtECbgGQyffp0mjt3Lt1///0UDAZp9uzZ9MEHH9BDDz1Ec+bMoRNOOGHE5/jJT35Cb7/9Np111llUXl5Ora2tdN9999HYsWPpmGOOISKiyy67jB577DG67rrraP78+XT00UfT4OAgrV69mh577DF6+eWX6dBDD93bbxdAueOOO2jevHl07LHH0je/+U2ybZvuvfdemjp1Kn388cdENDQnuPPOO+mHP/whbdiwgebMmUNZWVlUV1dHTz/9NF1zzTX0ve99jyzLor/85S90xhln0NSpU+nKK6+k0tJSamxspPnz55Pf76fnn39+P79jgMQl0j4S7UeKioro29/+Nv3617+mc889l04//XRavnw5vfTSS5Sfn49xEyTk7LPPpn/84x+UnZ1NU6ZMoffee49ee+01ysvLY+VOPfVUGjduHF199dX0/e9/n1JTU+lvf/sbFRQUsAtbRENj+j/84Q9055130sSJE6mwsJBOPPFE+sEPfkD/+te/6IwzzqAbbriBcnNz6aGHHqK6ujp68skn99htFXd1LHbAAQfQ1VdfTYsXL6aioiL629/+Rps3b6YHHnhgl1539uzZdO2119LPf/5zWrZsGZ166qmUlpZGNTU19Pjjj9Nvf/tbuvDCC/fIe4TRDX0FgIZ5NsCuwVrsHhYHo/r6+vjll18eLygoiKenp8cnTJgQ/9a3vhXv7++PR6PR+E033RQfM2ZMPCMjI3700UfH33vvvfjs2bPjs2fPZs/z7LPPxqdMmRJ3uVxxIoo/8MAD++X9AOyK+fPnx4ko/vjjjw9vmzt3bjwzM9NYvqenJ/6d73wnXlJSEk9LS4tXVVXF//d//zfuOA4r19vbG//Wt74Vz83Njft8vvicOXPia9asiRNR/H/+53/26nsC+DTQJiAZ3XbbbXEiire1tbHtDzzwQJyI4nV1dfF4PB4fGBiI33HHHfGKiop4WlpavKysLP7DH/4wHo1GWb3y8vL4WWedpV7n9ddfj3/xi1+Ml5SUxN1ud7ykpCR+6aWXxteuXcvKxWKx+C9+8Yv41KlT4+np6fGcnJz4zJkz43fccUe8u7t7z755gF3w1ltvxWfOnBl3u93xCRMmxP/4xz8Ot5/tPfnkk/FjjjkmnpmZGc/MzIwfeOCB8W9961vxNWvWsHJLly6Nn3/++fG8vLx4enp6vLy8PH7xxRfHX3/99eEyO2qfAJ81ibSPRPsR27bjt956a7y4uDiekZERP/HEE+PV1dXxvLy8+HXXXbev3xp8DnV1dcWvvPLKeH5+ftzn88VPO+20+OrVq+Pl5eXxuXPnsrJLliyJz5o1K+52u+Pjxo2L/+Y3v1FjoHg8Hm9paYmfddZZ8aysrDgRsTWh2tra+IUXXhgPBAJxj8cTP/zww+P/+c9/2OuY5hnx+Lbx1uLFi9l20/l/V8diL7/8cvzggw+Op6enxw888ED12lv3af78+cPb5s6dGy8vL1ef6f333x+fOXNmPCMjI56VlRWfNm1a/Oabb443NTWpsgA7gr4Ckgnm2QCJwVrs/pMSj4t78wAA7EPLli2jQw45hP75z3/SV77ylf29OwD7HdoEAAAAQGKCwSDl5OTQnXfeSbfccsv+3h0AAPgMQl8BAAA7g7VYDjmNAWCf6evrU9vuuecesiyLjjvuuP2wRwD7F9oEAAAAQGJ2NG4iIjr++OP37c4AAMBnEvoKAADYGazFjgw5jQFgn/nlL39JS5YsoRNOOIFcLhe99NJL9NJLL9E111xDZWVl+3v3APY5tAkAAACAxPz73/+mBx98kM4880zy+Xz0zjvv0L/+9S869dRT6eijj97fuwcAAJ8B6CsAAGBnsBY7MtyeGgD2mVdffZXuuOMOWrVqFYXDYRo3bhxddtlldMstt5DLhb9hgeSDNgEAAACQmI8++ohuvvlmWrZsGYVCISoqKqILLriA7rzzTvL5fPt79wAA4DMAfQUAAOwM1mJHhovGAAAAAAAAAAAAAAAAAABJDDmNAQAAAAAAAAAAAAAAAACSGC4aAwAAAAAAAAAAAAAAAAAksYRu0u04DjU1NVFWVhalpKTs7X2Cz7l4PE49PT1UUlJCljU6/y4BbQJ2BdoEAIc2AcChTQBwaBMAHNoEAIc2AcChTQBwaBMA3K60iYQuGjc1NVFZWdke2TlIHg0NDTR27Nj9vRt7BdoE7A60CQAObQKAQ5sA4NAmADi0CQAObQKAQ5sA4NAmALhE2kRCF42zsrL2yA5BchnNx81ofm+w94zm42bre2toeJT8fu/Qxto1umBtDY8Ly3lcNUHXySzi8UcLVJEf3/M8izd12izu7omoOpMOqOC74nJYnEOpqk5ORSWLu9y6G22N9bK4v72HxaX5+aqO7I17Hb4v1TXrVBWPM8DiDfWtqkxkgD9PZiqPl320VtVZ8ubPWJx98GV6f5WQiL2GMnnbSodCVFZWlhRtYsbc8ZTqHvoLPteg/ku+NJt/Jw2bmlnc1dlneG4enz3nQlXm+EMvYXFhHz/IcooyVJ3MCZNYXEw5qsxI5m9Yrba1bN7A4lpRJt/tVnU+uPdZFr+/5H0W8xY+RD5Lm6FMdIRaF83+qirx5vv/ZvEffnMni8eXHqzqHDrnJMOrjywZ2sSjz/2VvJlD5wib0lQ5rz+bxem+XBY7Ln3sdgdbeBknrsr4fXksDmTxc7Hl0ufzfocfaV09YRb3RnXfYtsxscVRZXojXSwO9fBz6KCt66S5+PmjLxTkcUSeh4l6uzpY7KJBVcby8PN1upu/TmaMvw4RUY7F32NGmWgDmboP2Pj2n/jrdK9QZYrOvm/43329fXTNF69JijaxJ3ywsp3FR0ydwmKH9Bhhd5x21XdYXPPifFXG17KRxVVjC1UZW2TJyiwvYfEfXvyn4dV526/epMcw/auWs3jGAaX6aSzRg7j4eIpqqnWdIj+PK8brMulHig1BFr13y/+qKk4nb7cdR/pVmU8yhs4X/X0DdM/1TydFm2hoaCC/X38WMHpkZ2ePXChBydAmAHbFaD5udve9dXefK7aUG0r1i1jPHGXfTiTH/aaLMHLmysf5Dz/7tKrx7jK+plT9pp4bVIplsonTeNzCp0dEpIfoOTl8/PWfZ5tUnY9W6efZEwrEuK6N9LqH9N8XXcvisbn6eAhv92PbaKyf/vtv9yZFm0gLHE4pKUPz2VhUHy+Wxdc93GLua/rVqSV+uZzm1nP3uJijpnj583q86aqOHAM4Yv2zv5cf/0SkmlEsPKCLyCbr8H0bsGR7JbKJHxsFAf4ejz5CnyveepOvz9rqPEA0qYyvYcQi/HNp75PnG6LeKH+e6CD/XDyZeh3kymvnsPivv39cP29wW9uKOwPU0/pEQm0ioYvG+Hk77I7RfNyM5vcGe89oPm62vje/30t+f+bQRp/u0MgrLuv4PDz2Gy42Zvp2XoeI0tN4d5bm4hcMXKn6AnC6GPB4xEXjDMNFY6+HD3iihovGHksMXsQFsQyPHjTJ3nhQDJrchsGZW1wUcbn0/rocMchz8dgyHJJ+8fkmtmgnL9CYLhrr50mGNpHqtsjlHvpuTBeNXeJLsMR3lKK/VlUmPcNw4c2XyWKfxQ8yX5Zunz7xXfsN39lIMrN8altGWFyU8vJjzGO4aJwmJzHicdONdOS2xI4u2Ub0vsjJUmYG/+x8mfyz/jSSoU14M72U6dt60Vh/3vLY9YhjynHp84tt84u5cuJJRJQpJkbyeDddNE4TF41j4niJp+kGag/ICaDeF0dMWAfE69iGi8ZuMSGP2/x14mSaOPP+xnzRWLZH/jqeVN1nZYjzlldOYH36O/J4xAJCVB/rXsPF5mRoE3uCL4sfzynGs+Snl+bmx0OqpdtAqnjtNEMZuX9y4co89hDnXkN/k+blx5A/y3B+HumicaYeZ6oxrd/wvOlyn3l7y0zXbckR/V9fhi6T7uV9fDK0Cb/fj4vGkLBkaBMAu2I0Hze7+978fjlfNqzJqDUNPR4n9Qevso6e2+hZqlh38ur35E7n2wxLPSSXiOQyk2GKTenieT0evm+GZbO9xkpwtr49Txp/kxlu/T0allySok2kpLiGLxpTij52hx/bYaw/uBRxIdmy9LqTI8ukinWcVH0gpoo1lxQxd7dS9bxWNjXD9IL0dW9xQdvSf1ieItqslcrfo9ut5wXyPZlmXS4XPzYdMddJTdX7It+TFZefiz7ePRmZooz+vFMsvQaQSJsYnTd0BwAAAAAAAAAAAAAAAACAhCT0S2MAAICE9HYTbf2rsI4O/bhL/JWWLe8x0q3rZPK/lHriuQ9UkaYu/lda8kdmLsOfSK0Xt8r+pJ7fYmR8nr5tWqWb/4WWv6BIlVn9/ocsrq1uZPHUSfwW10REgRz+F2P+Mv68Uyfr23b3RfibdLx6X9au+ITF1SLebLjH78OPvcTib86YK0oY/upPDSdMf7nbv4N/j24/PeM3lLnll6h2t77Flcfmx/zLL/Jbra/r0feDys/i7cj9TqMq8+q7D7LYJ//UuMjwq+dKfvveI8sPYnFkDf81JxHRq68tZvHCd99SZcaUir9+dPPPobtZ3zurfSO/PZX8PZlpABs0bBsZPxYfef2PqoS8cU/TM4+yeGPH33frlZOVx5NOni2/bo05+pvMyeHnXpe4ffLmoD4OIxHejjwe/Ut6edtox+HfvWX4NUE0xo9D2+G3o7Zcuo78kXCsX9+uqjPMb7lli18ay18VExHVN2zgdcStsaM9PBUCEZET5be+9bsNtx0TvxpOc4tf+zp6/5tq+HlpoI236ZxJVapOr7htl8twpw57u+/ANv66A3bEEj83cYlf5eq/L989zUF+G2zbML4Ki9s5Oql6sBEV95gbN0a2WcN4ULynAzx6LNHaX8/i/k/qVJn0CpH3LVMciyWGXxF3NvC4Trc3OlD+EoHfrvqYW69RVRr//C8WR50uVaZoyy8gRr5pIwAAAJjJMcOHhjJjRKznHCvn8VRpU6sO5wUqDePXAZ5OI058DBFq0Slm1r3N4yzDXfA21vHRXbvIzXRAlV6TeW85/xxmHr+JxXK5bm/abFxX2rn/fuT/WHx84SxV5oDJE4f/HbN3/TU+r7wZmZSy5ZfAjqNTKMlbNcsj1WW465Z6CsPdvEj8slj+MlfeFpuIKBqV83Lxi1qX/kWzvCtYNGzYlxF+aUxphtt2i0OkrJCviV0592JVZ+nS37LYm61v9ez38/fdEOQN1LRG7Yjj1St+Se/x6M8yy8fvzFNSVqDKrO1ev12U+KwQvzQGAAAAAAAAAAAAAAAAAEhiuGgMAAAAAAAAAAAAAAAAAJDEcNEYAAAAAAAAAAAAAAAAACCJ4aIxAAAAAAAAAAAAAAAAAEASGznLNQAAQKIce+g/IqKKCv345hAL66vXs7g8x/C3TFGHhQsXr1dF3OkZLLZjURZbxJ+DiGj58jUsDkfErjZ1qjrPr3iRxUcfNlaVqa9rZvHi9kEWN4V6VB0nMsDi/AB//IRzT1d1xpQfyOKpMw5SZaw0/nkueG+5KiNNHJMntmwUcYGh1hgR24YykR38e3T77ffup7TUNCIicvoG1eNOLMjijta1LJ5cnK3qeEWbcPv051le4GOx5eLf2/Jq3haJiKzNfFj45FPPsrhp3iJVx3E2s7iA4qqMp5XH48cFWNzZ3qfqlMl9E3GdqkEkzzj5hjJLDdtGIlvs4pX8OyouNrUJ2JE0dya53ZlERGRH9bliIMLbiWXxMn6fV9XxePi5uKl5sypDafw4c0fD/HHebQxtEn1JrLefxeGobnthm9cJh3tVmd5e/tqpqbzdOIMxVSfUyRuSPSD2JaTbtFv0fe01TapMQw1vTXPmzGbxS48+p+oUF7hZbFXxeFrFFFUnZvF+zvHmqDKWa9t3a6Wqh2EnLP4VkNvrYXH/Hup2N4m25TKMr8LEj+9ovz6eI+n8+I3Eulgcb6tXdVIcPm5b98YCVaZrzUcsbvO4VRnawJ87x+9ncfmps3SdaDeP2xt1mTYxPi04mMfZU1WV0stO4/Hm1apMlXvo8+sJ9xHRw/p1AQAAYAQZIxcR4xfTJZOppx/ON9TwcdD7/5in6sR8/HldeXwuU5I7TtXpauJrMCvq9DrCSBYv6lfbOsSw7cPFPNY19qaBkYuM4M1WvT6RW7RtPWtg8NO/xudFikVkbVk0cbv0+NeW024XX2FxXCkjvoYrw6O2eXx83ckWCzd2TM/3HZsfiC53moj1/jsxXsexDJc05fzR4Rvcll5v9mZlsbgkL8DiilK91nP0UYey2OPLUmUsi89/8gOlLA6G+NyHiKgzxM8VuYVFLPbn6fmzIz7eSZP0Ovz6NRu2lXdG/p63wi+NAQAAAAAAAAAAAAAAAACSGC4aAwAAAAAAAAAAAAAAAAAkMVw0BgAAAAAAAAAAAAAAAABIYshpDAAAe05WMVHWUK5KijSrhz+sbmPxCy9+wOLT2mUmU6JJk3iu3vYenRsvRnyb4/B8F9GIzikpcxi30c5jk7pGnUNySfvO883UyyQfRCRTAjaIdMpVNTWqSrnIadyxWefwlLlMZEaXkyenqzqnXn8J39DHvyPKOELVIZK5nfV3xF/dlPN4dPrql+aQ1zOUtyjcrnM0TqnkeUmWzeN5s2vef1PV2bSU5zopztWvmz+JH8HBZv463pDOE3Pa+RfyeO6XWbzhEL0vz//+ryxuDurj0OPjr5WXznPhFM46RNdJ58fm4nnvs9iUnjMgYlPe4z1hdX8mi2edOUcXWjhy/vBk1RftJ8s1NAVxWfocZPfzE2Kam58vbMP5JWrzs1vM1vmroj08b5AzwJ/XZciLFI3uPD9xzNH7Eozw3Kf9pnyuogNyiZxSWV69L25RJtrL99/t1jmmLJG3qbhigiozPpt/BxPK+Anl5aDM70Z09S13sPiY877O4qfeWajqtDXw/XXS9Ykrtl2+9li/zpULOyZzGlsekft7D+U0Dorcvu6YPj4OCvDcfF844iBVJm96IYv92fy82lTzsaoTrt/A4hVvvKvKxMK8ndveTFXmoWf+zeJmMeb6233fU3WO+vJJfEOj3j8KipFjgcx7fICuUyTaQeBAVSS/bihXslslagMA2HPOPmya2nbK8Tw3+wFlOrdjrjjPejx8PJLh1XPfjs4OFnt9Ot9sQR4/P8pclI6X58AkInK5xPgprMdgLvE8YcOYUc5U7R6+pTsUVHXCITFG7NEdbyjI+6iN3bxOh1ygIKLWdl4n2BNVZdo7h57HHnTozbVN6nEg6qtdz+KMfD0HoWx5LBQbnmkMD32rWDjtKN1GMsfy/r+3kx+D8//ztqrj0sP6XeYv1Nui7Tzu3Y/LMjdcNZ7FhZP4gOyTFXqt7YWneGxoZvTUihf1xiTgcqeRlTp0XnSZluNUBX4c2hRXRbwZ/Nzs9ftUGfl7VJmv2DFM6WRqYTkNtw2/cZV5jl1u/cQxW87v5QRJfzDlZbxNz5zG22tpiZ5LlBXzdl69Ua992wP84PSnB1h88DQ9L8jw8tzIfQ7/HNbWblB1amr5GmNxSZ4qYw9s6zfiTuJ5vvFLYwAAAAAAAAAAAAAAAACAJIaLxgAAAAAAAAAAAAAAAAAASQwXjQEAAAAAAAAAAAAAAAAAkthezWmc7eX3Pv/iybNVmVmTZrDYVzpGlXm5bg2LH/nzg7xAZOQkTf9103Us/tHlF6syPnFfb8uj74/uTuN5D8I9/Lr7f9/Nc/sREd3390fElp3nu9xdh8w6hsVfPlznnXz+ocdYfN7pJ7DYHdWfZbSrh8WFBfr+6MHt8sv1DQzQzc8/M+L+jgYWEaVs+XeuT+dUmVDFc8AUlBaxOBwLqjr1ItfGxroWVWZwN9K8yb1T2Sx1ekuVNdA25LvI9fB27vfz2JOlTzPllaUsnjJ1PIsnVZSrOgdXHcri6VU6B2ZGwM835GSJEtmqDlE/D7sbWJgSqDLUgR3LIKKhPHp1yzeoRz+s4+eTTRF+4L30CX+ciOidGpFT15APNxLq2+ledbV1qG25Ih2HTMvHs/aZbWjSeVZGFO0fuYywcmWt2haJPc/i3Ap9rMZEI5Yt4vqrRI4+IiKawsMM+dmZsj0n8mlt33/u+mfwebVpXTV5towdxlo632+5OOTf//A/LG4K6ueUXUCloU8oCbeyONzA4yqR7pKIKFDNxzStkUUs9sb03xp+4Qg+bvv4k6AqE3V4fhy7n481Wppk3keiAybxfmBMCc/luKlJj6U6R4iJiObMOofFzyxaLEroPldasonnrDkrityruyLS30MprqHvz+3SeZFkTnrb4bnbrAzD9MUR/YKt872FQryxuXP567jdOrdZKMTPbTLHcaRP9z3dIqeqfD9ERBExd/H5+OdgpRnyE7tFo3XxeYvMi0xE5IgUqP5c2QsQTZ/K29ppZx7N4gu/dJmqQ1QhYv7ZnTDraJKWL+V51Tt6dU4pa/O2/iYa2Xm/DtzKKD/PdvvE8WA6ISaikueUjBXx/IsU1v3/ly8+l8UHTS9SZSZ98XAWuzeL/Gcr16k6DSuXsbilSY8ZbdHe2jp1mTdH+Cx+eO/LatuDxTzXcMWZx+qK7avFhl5dRpnKw3Q9Xs3a0mzjCT0ffNZ09eocgf7MFBbvq2zVNfXtaltVef4+enX4rKtZo+ebuTl8zNUbG6fKFOXyY8gvcjLm5+h8kKuX17D4pKMPVWXG+Pi4IS7GdlFTokwx+Y3G9Fgi0s/HjFG16kXkpPE1ClsspqUb5lFZXj52i3n02qpM7xwTY093pl5TtMT7dLsMC3dbigzYe2eteTRwBsQ8PFsfy3I8S6THEEQbeTiGH++ZNMlQh68xZorLHpdfVaZqfFjDr4MsrTY8rSD7EsNS8j6TImLTxSenXbTpHL62dt6Zus5ZJ/LYbVWqMgte2fa9xQYcuv9x3feNRha5yNrySfuy9FxSrueH+/g5ypOuvyWXW2wzzDfJ4uctS6xWuQznJUscIS5xhLhlLmIiskR+X2OyZJmXOZ3XccnkyURUGODXCXxePi9fs1S0eSLqCfLJRFNDgyrT0S3OH2J5omwM7+OIiIrFNVGvuK7qNXxHoU4+FyufNl6VCeRte4/OYIxCTaqIEX5pDAAAAAAAAAAAAAAAAACQxHDRGAAAAAAAAAAAAAAAAAAgieGiMQAAAAAAAAAAAAAAAABAEsNFYwAAAAAAAAAAAAAAAACAJGbKRb7HdEf6WPz35+apMrEjeeLt0848QZU5a9aBLPZ4r2Dx337+J8Or80TbDz/0CIsvOvkoVWNa1XgWuwZsVYYsnhD72Xkv8Nd57NkR92VvcTl8f/v7I6pMr823raldw+JJ5aWqju0WycW9+rCZMmnitteI9hM9P+LujgrZqUTWlvztXleaejwS5p93W3Mbf9wJ6Tr9MRa7vKn6hUUi+cGYeNyQD17/hQh/3t7Y7h2nHaKdyzi1Vddp7+Ab6+trWLwov0DVmXlwB4tXTw2qMiWFARZPmjSexeUVk/TOZGby2AroMpC4znqiAS8REb3+zir1cH1QtBOXl4WbOqOqTmjzZhbHwt2qTDTC21JHL2976w3HoeQTsamDlE1N7+3esT6ot21aXMs3LKlVZUoKS1gszwPz3/pQ1Tn3exvElgNEvJm0j0XsNZTJ2e7fPYbHR6enHr97+Fg6Mj9dPV7zn34WvxPkj5uOsbEiNnTL1NooyoghzUG6u6e8aAOLV8xbzmLbret4AoUsPqAqoMpsbAqyuLaet9dITPeFhYX8xQ6eMZ7FLksf769u4nFQlSA66Rt8/BeezD+81x582lCL66Ewi19a8tSIdWCbgVgvxdLiRETkOHrAEonwM5XLxY+F3LQckqI9vTwOGc4xNj+D2zZvXbYjz/BEkSh/nlgfLxON6rG2I7aFw2FVJtY/wGKPi7/n3r4M/bxuD4u9efzYjRheJ0q84cccPRqcWsHnWVkZR6oyWt9OH81Jy1Tbbvjm7Sx++DE9L+yJbtvfaNQwD4MduvG27/INsUZzwZ3ImxRQ2zyHlbM4WpTF4o423Ustb+XjvxWvL1JlqtL5WGKKU8Fi5+N1qs76NXUsrm/TbTYa4X1qi7Pryx1vV69Q2448n3++z/3xNlXm8CtmjvDM7YZtot12NOgi+VvGVHJODp9JAyLOyUzZL/thUlKSr7Z19cZZ/FnaXzAzfUNxw7aRyFnJmpAe02x8ZSmLiz9cqsoEvPyZctJ5P+HrNYyVxGt5G/T8fqnFz3mtzbxf6wqZ1gR4vxDsNoyNxPDC1E04Ys4Tc/FxmcvS5+OJVWNYPPOoKaqMr0CMj6L8jOF167W/TC+fV0ei+rW97qEdjln7Zu358yjzwAPFljZDqQ4RGybMlCdiObY2rZXIMl0sys8YQ1L1m3ytvtSwvHLC2TwONvKzw6sL9ZmhX20Z2cx8Pr75qH2JKiNfKZFz0ksvtrA46uePj31b15nEh6U0c3qzKmNTZLt/J4+eYD+lbDk3uY1jRn4cukXsFXNNIiLH4idI2zB3t8Wn7BYn0HyP+GKJyOvj59ScIj4+8fuzVR2y+eusWKHXm0M27xcci/cJTkzvf2szb7N1G/n+xl7Ta1WWuM5XUijPC0S2WFFujPB9W1GrzxUra3g/53Pz72jSpEpVJ+zi78nr0X3P5EmTt+3XQJQaPlFFjPBLYwAAAAAAAAAAAAAAAACAJIaLxgAAAAAAAAAAAAAAAAAASQwXjQEAAAAAAAAAAAAAAAAAkthezWmciEff4zept+0uVeaci09h8aXHHcpiV5fOZ3T/H//C4q52fg/yfz+pcw+X3/QtFvsMORnD3fwe6ivrxT3IXfoe8ET6/ud7Bs8dcsJxR7B45mSZh5KoKIfvn0f82UCuT+dQs0T+3Nw8fa/2yu1yGvcYcpaMVhPKc8mVOvQh5uXqfBcuD8/N29zJc2REIzwHHxGRK403y/wC/XlbIvddTORB7mzV7Yii/HljpsTHe8Gg4WXaOmXMc82k+3TuGdspYnF7t06u2RnkiWsnVPDvZOr4g1Udd5rIEeg1tWFIVNeHy2hwS16j5St1ztEwBcQWcb6ImfJD8pyS48p1zpdwL88PVCvy+5pymcjWJ3MaF/t1/tmYxdtee0jndezdC03LlCFYtoBmw+t2tzSxWL6jF+bpZM+n/emPLD792u+IEqY3KPP/6HMbUfcIj49OfbQtg/z6dp1FSGZikdkMTd99UMTVnbpMSHxNp0zjcU6pzIxMtLmZ5wRqEV+rHm0RhWv5MeQ27LAtxhoyDVi74ZCqq+MJio8+necizsnVfUD10moWTy5URagtsIzFB4tcULmTp6s6LnHMxyK8j8o1vOc3XtTbYIjlSiXLNdQqYrZuEzJVUiQs8hNbhiPRJf8OVudZdYtZT0zkNJa5k4mIYv28TEzkRR6wZfZKnafZsgx/oyveYyzC+z7bsC+RKH9tl3jeSJ/el85QkMW+Xn3unTz3Cr1/I5LzBfnaaapGXupsFuf6NqoyjZu25Zfr7zNldIcdqXvo93yDN4G/DRcpxvzFpsT1PIzIE3hIH1N/efJdFpeV6xyNnT4+v1kg8pKFPtTJtgqz+WuHuvRShlvkY1vTaeggd8NmMZ+fdd1NqsyGyv9jcfnJXxAlqknp+pjHNYY53NQtc3rDGBn2P5m3UZ/9PjuiUZ1l0peFHMafNx5DVuO+3chqfPDYcSxubdH9cn45nyEX5OrclD4xw4zU8QlEuF2fu+SzLHzGkLxUkKMC0xxJ9nymNQA5ijTNbGW9duJzflM23PkbN7D4odc3qDLnXDyZxYdNr2Bx1NZj3C6Za9rS61VW+tB42kpFTuMdk7NuU59axcMuvRb74L31LL7n3pUsdvOU3kREdN9f+Pd+6Ik5LN5Yu17VOZJPfem006tUmddfq2HxQpHDeHfyF5usbdfrertqilePBcum8w+rWaxyLONpnYmIaNFiHlev0d/j5O1SMJvWL0Yre9ChlC3zUJcpV7sjcg+LfLl2TJ8x5bWHaETPz7wevhZbNYav144r5Gv5REQRca4LjOHXTryGuXBuLm83TZsaVZlwiI/Xt+Z73ypq6/dYV8/7vvwi3ru4vHr9OdvPj91xPp2DOSIWwXrFR9fZZVhvjvB1j9YQX3cKreBtnogoHON1JlaNU2UmVo3f9hr9ic8n8EtjAAAAAAAAAAAAAAAAAIAkhovGAAAAAAAAAAAAAAAAAABJDBeNAQAAAAAAAAAAAAAAAACSGC4aAwAAAAAAAAAAAAAAAAAkMUNq7P1r/uIValu+n8dfKuVJqC869ThdJ40ngH//gw9Z7BiSXze2tLG41F+hyuSOKWPxFd+6jsWTj9L7csMNP2Jxb3CDKiNlu4tZfMaZs1WZceX8cygr5InDAwHxwRHR0V89n8VZ2T4Wx2M6Ibbd18tjR+/v9n994IRTdIFRasL48ZTmSiUiom9983r1eFHpASx+/sWXWfzIE4+oOq4Y/1sOjztNlfH6+PcWjYqM6hF9fLdHeUL4QVXis8R0DHlY1BmKqhILPviEx+8vZXFJNo+JiELdPLF8IEcnsIfEfbSqhjI9Q8dsU0e3ejxQFGCxE42xONrdpepYNj8vOTHdJtqDfSyuC468rxkiDovYHe5XdTwBL4u7DOfDvUGfzbVKj97WKZpJUDzu6FMFPfD4eyyePrmKxWOOm2F4dflBxAxltt8Z3deMVr20rY/UIxwiw1fAmA6xZhGHDIV6RTzVyWVxuHqTqtPSIF5bPK/HbdgZ8QbqW3QReTSIZkR+w+GySeyeY/O+cfwkPgYiIpqazd+AMzWgymx0FvHXzuLnk5KD9b64BkQshs+Wrfsj2LGc/GLK9GUSEZE9qM+zMTEWDYV6WBwlfr4nIvL7snhs6xNiNMYPNEecAJ2Y/ltaJ8bL2FHeKEzn0HBEvM6ALuPExPOIlu41nPS9orEFO/jn0r6pUdXp6+XH5klfOks/MRUatu1c/bu/YvHKGP/8HcOfJbdUr2Xxgo/0/gYmHTj87/6oPjZgJ2LiQDN1w0L25ACLQ2E5EiLqWF4rnlcc9NWGGYXoO0IVeaqIm/i2J/7z5A73c6vDHD7/sR3dzj0OH92tixg6pT1gcm6J2hbr1eNeznDC6OTrD/UrP1ZFysu3fFa9+twH+9aaRjm6IppUmrkf9mT31NQ2qG1uiw/uvnBwsSoDny2zZs5Q295cotc5RpJXxI/dQw4/QpVxidNsiT+gykTWbGbx2vYNLJ5geO1cSmWx2/B7ppjYFhKdi3x8aJsYtxle2y2ex0d6guMS+xcQj+ca5rJyfrbM8Nr/91g1i6+L8bHcIVNnqDrr6nk/3BnU/a7LPbR+Zg/uo8WJz6G298U35JarQUQFZWNZ/J3/WqLK3PPYxp2/ULvedPpJ/Hv/wcXT+L6F9PpWUyOfMP/jT3rc/OLKne/KntKjVpF2XTCij9t17/Hn9fFlJzrpTP08kXoeH1Gly5RWbPvs+qJxetQwdxyNbHuQUlK2nAvsVPW4xyPHzeJ8YelzqiVOogGfHvNUFPF1mXH5/NqU49LnpfbODhYHo/xcOGGMXutx/HyC7HIZzt1u/h49IjZdBO3s52vQS6pX8Toevf8HVk1ksWXYl/Himp0nk19rWLFar8e1R8Wx6uHvOWJYfIiJKfPaNev1/k7edm0qvguXgvFLYwAAAAAAAAAAAAAAAACAJIaLxgAAAAAAAAAAAAAAAAAASQwXjQEAAAAAAAAAAAAAAAAAktg+zWmcR/qe6hOLi1jsxHQ+y3Vr1rH4k8UfsfiIU3V+ri+dfzqLc3zpLF68ZJmq8++/PsHia75+kypTPobnL5oq44Omqzp+L79v+eUXXsbiPupUdaZP4vdHj/X0qDILXlvN4kg3z6P0agHPL0ZENPu4WSw+49yTWDy+Qt833pUtvjc7rsrYfdvuq24b7lc/WuXn51H6lpzDRx1/rC6QzT/Pb5SXsrhwjE5a58R4/rnxFeUj7ke4h+dU6QzqdvTq+++yeNHSZSyurqnTT7yP0o5WFvP2OXXG0arM+MovsDioU0qRR9zvv2NjDYtr2nnehCE8Z8Dmdn2egsQNOm6yna05WXQXEwvz3NrBNv6dhII8JxIRkdvhX7bPp5+3IxhksTz7mTKOytYXEGkoTOlb13bum0Yhs9qYUhPmJvBnX3Jv80XsNYwCWtt5X/KPf73K4ptcem9Sj/qC2KLzF3IjZfIdPWK07S/0Wg2Py69grIhF+l8i0sezHkXo/NXPruSlphjqyNeSr+M1NKQi0VBM+bdlRkmZiiXfR8oykVpzySKe5/HoMTqBUWA8fyJrrKHlZIg3IXL5mQ5NW+T7scXTOjL5M+xU2dhKyvIPnaWjUZ1DtSvIc3y6XYYDRIhG+Pfq9WSrMoV5MncZP+KDHTofqSuHx5Fe3h/FbH3A5Ph5D5SfrfdlQjkf21WOK2NxXtqu5xmubtZZ03Pz+TylKG2kc3Niyg+ZzeK/PML7iY9XvKXqBGs+YHFJ1aGqjDtzW8cWt1I+zS4mn5FyGBsGNYE8Pg+32wyD68U639auchw9J134ps4TOJKPW3ne5uLiHFUmGuLvYZBMx5Gey+6qG87Uvei4DDFfHxSfZyqf3xMRUT/vNX9879OqyO+OG5qv9/Sir9nbZMbF599YzmK3W+edrG/gI7mTjjiAxYnMLA2ZwSkkDqeahia+wZB7cPKBPB+xTA2/vlYkgySiaJjPhZHT+LOvZEzRyIUS8OoSnmd1vGE5KFcMYSb59EjfVc3n9/IIKjSch9PF75cGDL9nCovWs5j4Af2hqkGihJncm0MNtfJEKY/oN0wL6QUiPthQ5j0Rr/iI97GXfvWrqs4Lr6xh8WtralUZGFnBEWewuPF5ve707e/OY/H/vbVncuHKVcjvP6bH7J8Vpj7rnMKZLC7MMYzr1qxl8UrifZZpmHrC0Xx1yi7iCaHH8eVzIiJqEgsf/jzdZ7myt323LvenH/N9XngzPJRiDY1TYrEB9Xggh5/QIzF+fPv9Ol9xfk4uiyvGytUqIn8mn1M3NfO5fEMHj4n0+kqZl/drPo8+yzpiEcY2/Q7WzfclpvI26yo+cTy3d/H84XX1Op+418M/q9w8PccuLeXXg/x+Pm+xDD3JApHrORKVaw26jiVWjxsamlWZL8zc1iOlWqZRpxl+aQwAAAAAAAAAAAAAAAAAkMRw0RgAAAAAAAAAAAAAAAAAIInhojEAAAAAAAAAAAAAAAAAQBLbqzmN5b3wLz3pBFWmsJzft9yy9J3u7QjPB9TVxfOfbajTOR0skaNu0duLWLxwoc6jVFXF8+NNuOtAVWZ3XHjBmSxu+eltLP7vW29VdVau+Fhs0fejj4lsg5bIs9FuSEG1pobnGXj17fksPu9inueBiOiiS89hsSmXj2Vvu0+8lZY8+ZY21m+mNNfQkf7LW3+hHj/8SH7Mzz7xCBZfcPopqk56vrgXfsaeySv05Wv5vfEbavkx9vo7C1SdpSv58bK+UeciWFLD8wY312/kBUL6fvmZIq9abinPuTd2PM8FRUTkC/AsMS3del982bxMtISfT6IhXWdQpVM0Zb+FREV6bSJ7KAdQY63O/RBs5/m226P63CZNHMt7k4hMKEpEkW6evVdmfsglTXaAbrHBlEUzNlLOQNL5iGUeYVM2X3kYyqPQdFR2ilNtu6GQfI8yS6bL8KdjAyGe22TFGp4T46/P6QxS1xwicvvJ1KFExP9OLXlyVepWwMksWvJY8CTwGqb820ERvyJinb2V6BARy97c1LvLYVumYWTpFgd9UNQJBHSdsPggXhL50gKTG1Sd0NE8H03M1vlcLEfmNeKNINqvG7kT4+/c5fA6seQZ9uwRGem55E0fyolnx/RJKEuefG2e2NEyfN6TK3jSq4LdGTvpNNmfK5PHTNtLz6yzsV99449Z3JIznsUev851llsSYLHbq3tDt3vblxsfTJ4cZPuEod3Ur+F5EsmQz3JPiBgmpT0diWSe5Kz8CSxuinlVmYEwH2dmuMeoMn0x3p9ku3lP67L1gOqM6Tzv+EHZOh97eq/ocyJBHmfpfGcNYd5pLjCkOVxZO3QO7O3FHGVvk+tXc06cvl9el4hIpow8fEqJoRQnZ1UtnXxLbp6eET3/1gssHleu8xVWVFSw2OfnbaYoJ3nG9Z8Frj20iiuHW5ZhQuGIpZyWNSFVZqLozuUILN346vxNbCSdO/YtEa9RJXaPHF0sTqCUHNUcaagh37dpDidHPumib35h3puqznOLa9Q22B283y49UffJCy/ZMzmMP89yDatgoXa+jup1IqpMi8hhLFlqVYzI1clbSUk5f9xtWHsL8WEeLVvZosqcN23b+NCdHicynF9GI5fLRSnW0LnVsvQcu7ycj2X9uTxHvS9LL+L5PV4R6zLrajewuDbE84XnlcmM70Qei/cBuS7+vB63HhmFgvzL7+jqUmViDj+pun28BwqH+PVFIiK/m3d+48fwuUNWlp7Xrt/Ij/f2dr0vlsX7kaIi3ktMm6CTdjfVb2BxdU09i23Dyp8rnbejzna90udK23Y82Hbivx/GL40BAAAAAAAAAAAAAAAAAJIYLhoDAAAAAAAAAAAAAAAAACQxXDQGAAAAAAAAAAAAAAAAAEhiuGgMAAAAAAAAAAAAAAAAAJDEXCMX2X1Xn3kOi2dOqtQ74OcJm70+nVTbSuPXtmODPLH10mXrVJ1/PfY8i5uCQRb3692lqgBPCp5Rlm0o9en91/+7gcXzX3xJlXnhvXksdpFOAh4lnlTbK8pYLv31RqI2ixcvrWVxTfXvVB07zLPPX/31C1QZt3vb88bSBtXjo1Xt2k2UmjJ0fDY2RdXjXR08PnL6QSxOz0lTdfrrmnmZKXn6hbvXsnDzmgYWR8P670GKAzz5fNnUSSy+Yu5kVecKKhZb9Hf7warXWfz6W2+x+MN3P1R11qzh+98U5Mno17boxO2+CH+PnYYE9h5x/hjv422aYjpp/MbNIRb3tIrXjnWqOrBjG2o3kidt6Fz04aZW9fjunB18rbxWe3u7KmPzUxsVi9Pf5Kp8VSc3wAvV1bWweC0PiYjEWdds4giPm/5aq0QcmuK0S4ZdobCITf3aDBFfMoe3+1ffXaPqrKzjccy1mcUR+WET0UUr1rM45/AjDHuzfVvD36ztiPyuTQM12dtsNpQZySLDNnlMTRJxiaFORMS2o8vITerMIJ+EiApFLM/ErVF9DHnKvsBit6XfpUVBEbtErN9AzOGfuC3LWDied417y39EFqWrRz2uAIsnVR6wD/ZpNDD1UCmf+llffONRtW1DSz2L3TmlLPa4dJuwXPy79ru9qozj2MZ/QwLkRy5PZaaPs03EHkMZPw+LJvOz8/SCIlXllFkHsnji4RWqzFtvfMLie257kRfwTld1xk/n5/jqFQ2qDNESFvXFdIlSby6LT5hezuIDxmSqOlkxPu+I9vepMr3hIIszY7JMl6rjtfgX9cWTp6kyLTWNREQUMb0ZgO3IlYWqMWkinqrq/Oh7t7H4+WdfUGV+9Zu7WHzeBUfv3g7CHhGK7plzQT4/FZK3QJfJc/E1xhI5WSAifyc/j6WJDslrmM2sEJ3S/TvZz8+CHhEvN5QRHyf5DGXkyt6aTWLDwtWqjhzJ5RqmHR2G+RdI/Dh85G8fqRJLDHPSZNOmVgSIesTx5bQ3qzIjnZUcQ4nGNfwDl8u1h08Sg1Aiyq/g67crV+jX6vzrtuc1LF2NWsGuIFHKluPcsD6xYiU/v0ybPoXF+XmGTsDifcDH1etVkRW1NbyKn8/xJuXpuYLH4QeVu59/UV6fnidu2izWJcO6wcp6eXn8GPJl6snO5HF8nhLI4O/ZdGx3dvFeob1Vr8h9LBZWx3bxZ5pUOUHVKSnhn1VNXSOLzatOvOFEorLHInL6t712PDZgfBYTrHIBAAAAAAAAAAAAAAAAACQxXDQGAAAAAAAAAAAAAAAAAEhiuGgMAAAAAAAAAAAAAAAAAJDE9mpO46mTq1js9eh7hwcK+D3TLY/O8ery8PtzN23miWIff+xVVadO5DCWKsYdorbd+qtf7rTOHiPe4pXXf1sVefa9d1jcb8grILMc5xfyz7IwS+dktvt41ks7yvMBtHTq/K13/YxnGJlSpe9Hf/y5xw3/O2W3spZ+PnUEO8nakmQk36WzllRU8BxZ6VkiJ8MYngOOiCh9DL/PffW8J1SZ3/3+/1jc2szz8FpOjqpTXMT35bIreM7xQ790kqqj8+Pp3NqHTzl1pzF9Q+ceXrNsAYvv+vl9LK5p1jmNKcZzHOT6dH5ifynf5nGLU5ytP2/y8LxlG0Q+gN5NyGm8KwbIodQtmXf21pmgvEJmOyXKL+TnO18m/x79Xt0+LYe3taBIJNzWsnvfvUxNJHs+07MO7oHUVIajm350K2+P5154AovPWKPzAd5w24MsXiRzzTg6p/T8pTxfx/mH63MF0fb5RZB4aUdku5HHk6nMnrJSxE0iPsxQR2ZiyZUJvUgPNmvFIVRnyI82WcTyjB8qHKPqnHbRFSxeOk8m7CSifp6HxxaJjqJRnfjIcfjfWNoicbPj6NyWsGO9A0GyBoY+w7Z2eZQRfaFC51yERHz6/MVERGvqnmPxQ48/qcp4inn7c4v+1BWTmdeJ7DRexrZ1HqpYeFsOpli0Xz0OO3bmrTwLvSvG5xwnHKNzBJ9z5gwWV5LOPUwkc4rLeaApEbKklxzmHLOKxffcMZ8XiPD8aERE1Yv488z5xndUmUXz+HiweYU+fv0ufg4f6+XH5pEVun9xYjxHmuXX+dhDNu8rMkNiPpPH85IREeUV8Tq+fp2HLLYlh1usP/EcZDD6yLGfKfVmTJw2PeIwtQ3DlUsuuYDFp55+gioz46Dine5bj+F0/frrfL4/58xjd/ockLjx48pHLmRQKU7FpxzO58f54/Q6X5GYYTbU6TywfuIHgByz24bfKq2mz3cfr7NXErWK2NQ7yil/nYjnHqvb3w9vPZzFN//g56pMx8aQ2gbcmlf4uuQ3b9BrGmDWKa5HFJDONzuW+Bilkni/sYJaVJ2ow+cub4lk4S8uH/m4TjEkD1+/XcOKyyXt0cxlEaVsOd86ek0j2MPHmCuW8xzHAZ++jtAuxrI1dfKsRRQT53i/zXuBYGOXquNz8wtjYrmFWoO6TmeQ739uQF/zmjyjksVuNx8Iedz6zFySy7PNp4qlyu6oHjz5/HzRK79A54OOxvgZPyaux9Vs1POC9RvENhfvuC3TZVzxXZt+HdzZvu06aqw/8bUr/NIYAAAAAAAAAAAAAAAAACCJ4aIxAAAAAAAAAAAAAAAAAEASw0VjAAAAAAAAAAAAAAAAAIAktldzGrvFvbcDRXmqTCCX34PctvR17M4wv4f6fX/8O4trgvre+NKpx57N4kef/qcqk5On74e+Lxxy4elq25R7eb7ZFe89q8qU5PJcVccecSiLLUvnh95Ut5HFfSGeQ2CiV9/ffe0mnm/uT39+VJWZNWvGtucMmzLsjE45eS5K3ZrUmHTOgNkni9w9/iwe9+nspr31/Dv63s23qjKLV9SyWL7y7JlHqTofN/LMK88vWMbiymkyZxnRyiU8D9gxp5+iylDBRL2NyVRbJs3gx/xD/+bP+8SfdPt84ZWXWDx5kj6flJTxNuw4PGdAqFef8twrm1lsx3iOhjWb1qg6sGMlpWMpI33ruWfpHnnOwgqeDyW3vEyVcUQ+xXAvz38S6eggqbOd58lYUrtncivJbHQ6O92eIbNXzj1X5xvLLeDH/LxXXmdx2NL93gHTqljsKuRtJDBG59Fa380Tfww0L1Bl0sYcs12kc5KD2Z7KX1w5iY8JateMnBtRZpJ5xVBGnomPNgwBcv08x/Va8a509lMieWQeV8LjwLSDVZ1yOpPFk6tWqTIvP/u/LPZkyx5Uj0NdFj9eXQ4/3mWO4yHIPbkjLY1rKBwaGhtsqFmtHkdO433rnff/xeKHnuKxq0DnuS308vGsI9q9y9CqbYvPMWSu8CHb9+VoQ7vihdvlmCtjv+xH4sRJPZHc8OElLHzm9/eqIoeceDSLm1fop9kQ4uO9nDI+B4oa1iNmH8nHRtGIzmhZME2Mw0TuZBrQucuokM+TmjL181YEhtZC+qJ6rjlaLfqwkTJ9Q2N529bvOyDy2KWl8e+suFiPb3Oy1KbPtOY2Hje28Xyc7e0iZzYRRSI87+TG+k0sXvL+O6pOS1MDixe+977eGXEoB7v4vKo7rHMPtjTzY/nY1Yv088JuWbZY5xVOhE9Mv3Idngw0u12vldjd/Lv1h/WYt4BkzkU+7g8auvs609D5c26xiA2pVmVTUm6//2m90bQNdtn9f1rJYn0GhR2pFXEr6Qn/0SLP8dfP5/m5H37qeVXneZEr2S3OUVmGJSO3OJ/EDEOjnuS5JMHZDlHKlrOMIXevPDcHAjyHcX0DX/cjIuoM89VMy2W4jCjWQiwxju7o0Nc9fGOKWOzP5te3ohGZAZ7I7+dzm+KyElXG5eUHjVyn8Xn1mVku5aS7+HMU5egDsbuHn0Fctu7ocn38tTqD/PFV1etVnUYx+LNcYv6sVoGJSIyTXS49B1xdW7+t+IBp9c0MvzQGAAAAAAAAAAAAAAAAAEhiuGgMAAAAAAAAAAAAAAAAAJDEcNEYAAAAAAAAAAAAAAAAACCJ4aIxAAAAAAAAAAAAAAAAAEASM2Sw3nMK8/NY7PHqZMyOSJBtufR17LfefpfFC2uqR3ztI6cfz+Innn6ExVl5WSM+R31dh9oW7upi8dQvTBzxeUbiTdPbLv/+LSy+67/WqTKFWTwxeEt9A4u7DLmtM0Rm+ezcQhbbkZCqMzaXZ5Fv7+xVZR765zPD/+7r1wnLR6ujjp5O7rShZrTw7Xr1eH0zTyS/soYnOl9T84mqs2r5MhYvWVE74n7Ir/qZJe8ay23P8vGE6octPkSVWfDKYha//84yVebqy77M4pyqKl6gaIzh1WXy9lQWXXjtV1WNI6eXsbihWZ8HxpR6WZwr2nlnD3/PRER26ioWt7bzT3ONqgE7U1BUTF7P0HnGcGqjgd14Tpebd1WR/ogq43EcFkcj3fw5bFvVsR2+hzHq3429239k7xNr0+fvdasaWbxw8WoWr9ys33PeGN4vTJg2gcXlUw9XdbyZ/Dt6/LmFqsyXv1awLRjU3yHsWXnilH7ZTUeyeP1KfmwQEb30GO9v2kbufkiOlJ4zfLUFkUEWd+kiylIRzxBNOJBdNOJznFZ5hdr2sv0giyPd/HPwpOt+Imbt/G8sPS7TmLJupN1LWu+88hR5MtKJiKimdrV6/Oijp7DYn1nJ4izi5yhI3PsfPKa2/ekRPkfyj+fn/EBRuaojW0mwOcjiWLibJK83m8duryrjGtzWJw0OJs98Ys/Q8+zPNmfkIiOJfKA2Re0jxRY9Gu2jPhZ7xlSwOGjrXmpJNT+nHz/TML85VIyPujbzuEaf72jKhSz86wuP6DLkIyKiUKiX/uvWNwyPjz6PPvY0ud1DZ5o/3vs7QwneL+cVFrDY49F9+dVf5/PLWUfo8az8ScWxJ05j8cgrSFpHj9725GMvsjgW0+e7khJ+jLUG+QBr82Z9nFZV8f7SIxaa6sR6ERFRa0sTiy3DmMcW86hYjM+X7X7epoiIgkE+L2ltbVNlYPe8uXT5btVbK8boLa+0sNhLPCYiKhbxaWodh8gSo4KQmPHXO/p8r1/p8082dT0zp8/ZasPocvdT4f29C6OGoVuj/AA/D6xs4Ous80l//rLnuPLLPhZPO1yvHvp8fLzb0qz7z8VLtp3sBgaInn7asMOjkZVOlDLU76cYfiPqzw7wOJDD4nDEdIbi63zG0bvFxwhyzOBK03MUrxinybGHy6XHcSUB/jwef64qEwzxjs5y8+eNRPTx4rHSWSyvU25d3+bb5PPqRbDeXn6Er1uzlsX1jXoc51j887Ysfq3EsfU34IgxpGO41Ltyzbbxn7MLc2z80hgAAAAAAAAAAAAAAAAAIInhojEAAAAAAAAAAAAAAAAAQBLDRWMAAAAAAAAAAAAAAAAAgCS2V3Ma2w6/j3k4pu+Pnuvi96xvatd5hF96bf5OX6dq7GFq2wN/f4DFieQwlr57w/fVtuYGnuDvrtt4mePPO3uXX0ffqZ3oiFOnsvjAg3W+WbtuCYvranlO3XVh/Vl+oWoGi/0+nl+sNxRUdfIKeN7AwrI8VebZ/yzYtl+DeyBH1edEdmYGpW/JtxoK6fvR/+lPD7G4ZiPPnbi6zpDbSmR2KNS3zydH/LlHnyF/9Ui6QzzLSrsp6YpvLAsXrFisinife4XFB1Ty+/S3tunj8KBpB7K4ooLnK846eKaqU3rEiTzuL1Flumt53owskaMhK9ev6hw4nmfkWFrYrMpA4tI86eTekqtyxrix6vGQyDGxLsjbzbiAzlOa7eddVWGezoMYEH8D1bKZ582qqdEHeEsrj025WT7L5Jm2qVHn0QgGec6rtSJH7SZDOov3NvEP5qulpSxe9cxbqs4XDuVtukynF6FH73tm+N+RPuSq3NvO+SrPQhb18jx2+dP0EPCiSp4LL9jGx21NtfoYc0I8d6nP0mOA2nfiLG7bZNhhQWSCpJdFe536gc5rP3A1f51Syldlvngmz2n46vy/iBKGv6cUuWNsZ1A8bhrJwY68/+Y8Sksb+pxrG/UYobmJn42D3k4WTy1FTuPd9dB/nlLb7Fw+F3Dl8LGT6ej2yo15Ip+hYSoQteU8UOc2s2Np2/1b5zGD0aR3DzxHqdpS/e5HYsvIx9Gv/vAoi0+bOUGV8cf4uaqiQs+Hy0lM2qJighbUc0Ui0Z/QHEOZrW3HNFkbnZrbWygtbWuOOTkiICLi45GO1lpDGe7NBXyM85Nbf6DKHH36F1n8y8KfsLisTM8/HTEmWPDGOywuKdBzm+Zm/p421NWrMp7j+HynUdTJzuHnbiKiwlK+bf4jr7I4auvcw15/JovlGh4RkUvkBvc6vE40rJ+3tZ333c3NyGm8v8lvSX9rmlyKMi07hYgfM91imXmVIZNvMuT2TYb3+HkSH7kIfAr/DPJzfsFiHifSA2ys5nOD1gZdxpXFW5allwbpsKO2rQFEow49/XSnLjQauVIpJWUoB67P51MP+3z8ulhTE/9WZC5fIiJfboCXMeTuldM+OY7I9gdI8mfzfbFsPpZyHD2ZdLll3mNVhCyRE9jv5/Nay/C8dpTvryOf10ojyefn+9/ZrT+X9Q382oLlEd+JpVegVcpiMSRzGd60pbbp2Xswum0xLb4L1+zwS2MAAAAAAAAAAAAAAAAAgCSGi8YAAAAAAAAAAAAAAAAAAEkMF40BAAAAAAAAAAAAAAAAAJIYLhoDAAAAAAAAAAAAAAAAACQxQ9roPSdmx1js9uaoMq6MdBYv+ugjVWZVU6vatr3rvv1ttW3SweMT2EPu/l/+icWvvvKqKuNxeGL2H9x4I4uvadms6pz1jat3+roxw7amJh6ne3Qia8vFPzufSKodD7erOhsaGlks82VHevXepGby127p0Mm66+u2Jfh24nH1+Gi14I13KHXLn160RvXj8xe8w2KXj7eB/MJiVccV7eZlAvpvO6I2/w6am/pZnGrY14Cbb7VEcnqPv1DVmXw43xbR+d9paf06Fnc4/INYX9eg6ry14hMWH3bIgSz+2sE8HpLJw/RKVSK7qI/F3Y38eM/Oc6s6hTniO8krMLw2JCzYQRQdOlCOPVCf85s6+EmnyC5j8Yrly/VzhvJZ6KsYr4qMGVPE4g2NdSxe0rLDPf7ckj2jbWif0uHTeXxOeYUqs6imi8XB5g4Wv7Foo6qzZAH/3u68Q/d7oY7Q8L/7+gdG3FdI3KlfDahthWOyWeyQw+KIw/sNIiLby/sbTzk/qIoLvKqOS4xi8l1+VaZ5Mz+GaJN+bSlPxJtEXP3cW6rO2rN5nzv1nGNVmS8fdBWL33vvCRZHo3ocZFn8c3GLsZPH2qvD6VGntLyM3OlDn1l1rfxmidZ/ws/fZ5152D7Zr32LD/Q/Xv0ui4ty9FikqGj2Tp/RdFZ9+Onf81dt02Oy4ikHs9glhp1uyzA3iEVYbIlxqYv4fImIKBrhdUgPycjnytoW2LYuAKPGO6vf3wPPos8fFAqJDabBEW8tDUH+PI+/LvosIrrsbN7+uqxsVaa8T7SVRjGhNxzzev/0/J1oa9vRc/DRauIBEyjdk0FERDklB6vHu5pWiy18jDPrpPNUncISPldIzy1TZc49+1QWr65ew+LW1jZV5603+dhjYy0fJ0+pmqjqtLfzYywU0t9tWBzLMXHe9fn4+yEi+uuf/8biR//yUxan5x6i6sw4hH++nZ2dqoxkRwdZbDm6zGCMr2k0iXk5fD7Is2yI9Fqfm3h/HRHnNd6KkodpnRe/2oLPoyzRpnuMsw5O95ZaiohfXjRyHXlppMewDv/II6ax1OhnWS5KSRmaY9u27pjb2/g6nyPOSN7sLJLcHn7dye/W4+pQmI9hHHGtoS+mvySXmw+KXWIg4fca1p08fH9bW/W1Qpebrx170vnzZPvEdQUiinTx/e8N8XmslaEH8O7cAIsDpaWqTOcKfq2kK8J7BZebf7ZERI743mLia5RrekREbnFhL2acQm9fxjBo2wH0WQAAAAAAAAAAAAAAAAAASQwXjQEAAAAAAAAAAAAAAAAAkhguGgMAAAAAAAAAAAAAAAAAJLG9moTN7eH3/TbdNbtlM7/T/YdLP1FlZNaMWTOPY/F3v/eVXd635x56Sm37zf/+lsU9MZ3PxSGeM3XDxiCL7/zJz1WdZ5fxPMdnfOMmFvvy9H3Mqz94j+9Ll86tlO3iN/PfFNT5w6SOKL/ne6ya31ve79X3ag8HeT6aYK3OuWM523IaJH539M+/tS3b8jCYMjT6/TzPdHEZz52UE9D5sPpErqRYr84G4cvkz5tJG1jca9iXmMP/RiTYzb9HOxJUdc6bcw6L8706F8HyJSJ/gYfnCDjtwstVnboanq/wgMlVLI4365x7ZPNPOKVsgi6Txz/f1kWrWFxTp9tRi8PzNoRtU/YZSNS0Yi9lbcn5cPJtV6nH34ONDmMAAEBESURBVF9ezeJ7/8Nzkm2u1efDrs38iN5Yp3PHbwrxxA3VtV2qzGgjz8Q1tbrMYVN5PHM6z/Fx5BE6T9wRs/i54qX5S1lsVek8Jm/V8Lw2a5YuU2Xy3dvaVnRgUD0OiasS3+sxp+oc77aH98YytYnH0n83GBOlbFvkrLNETlIiitm8fUYjOv9Me9POcxiffqrOrV08gY/+Xv/jBha/0yLzVhIduYDnOTblNCYqZJGHeO71aJ/uJ/x5GSyO2XwsKHPwwM4Vjysjz5Y5QmHuMvX4e/NfZnG+GPOE1dFMdO6pcz/1fnXTKr1RHLrBTj4mbq7TubVb13zA4pghsVAsxtuW3c+PIV+BzoF5/tyd5zT+7T/+W2174dF/sNhfrHNVeRx+/va7+bnBZen97xFzg1AXH6s6tiHXmcW/R5crQxVxubaVGXSNnC8NPr/+cN8TIxfaLfJ8bPobeTnW5Md42JCvOFA1g8XrdFdBM4JiniTzcvtNSY3lcgxyeRMRFRcXkidjaDxx0w++rR6P9vGTszeTn09mzz5G1XGn88//N/f8TJUpE1/9Zt7dU6E+bdHs2dNZLFP3ba7XuRXfnP+2fiKhrY0fZOEo35kVK3Sf9e9H9RrX9vo7m9U2254y4r44Dh+TRXr4ulOo05Rvmx/bxWPGjPg68Nkj15XMKyXi+BBrpjrrZHLQPYl5zRDgs85OIIfxSEztoVvEubk8bg3qOiqHsUeXIUOe42TgOA6lpAyNg6NRvW4j5RbxdRHLsKYRE/mKfTl6LhmJ8LG2x8vXV0I9+nlX1fDxSMDnZ3FhtmHMHOTPs7FBD8YnVvH3RAPitQ3zWitbzH1T+Wv3DugDqr29nsUTKstUmeJyvm1ljbi2Y1hCcqtpiyhkyyukROTiPbMl5x9E5Nsu77GT4tDIVw+3PFeC5QAAAAAAAAAAAAAAAAAAYBTCRWMAAAAAAAAAAAAAAAAAgCSGi8YAAAAAAAAAAAAAAAAAAElsr+Y0Dob5Heqjjr6vdkuQ57tYvrLR8Ew8B9bNt9y2y/vy1L/+w+JvfvMmVWZzZMOIzzNSTo/Olk2qTutTj7K4sYXna508dZKqs2kNz/sZbNLJKouK+Q3/I+Je5ynE7wlPRBQX+UV6iOcEdEUNuQgdfi98Ux6OSYHA8L8H43Hq6pbZCUYnK4UoZWtSY8P96INBnnt18lSeM8hlqJPnL+JlRExEtLmdH2ce4q/j9+i8sP5SfpyFu/h9+d97Z4Gq89UrzmfxSYfqnEeHVPD7/Vc38+Pl5PPOVnW6mvlRFGrkx/fCtz9WdRa88TqLf/ijG1QZKueJPseWlLD4sQcfU1VaRBKMlg5TTiZIVP6MyeT3bflMK3Te6SMCPAfGq68tZ7F71kxVZ2kNz3tc37ROlaks4Oe7ogKeNWVNq84nNtosM2zL4unD6diZ/KTjMeQtqRQ5Pa468xAWuwKnqjp33f1X/rwR3ZcfMu2g4X9H+m0iU/7QJDRTdLudIv1Mk6HOKacWszhKOr9vLCL/LpAP+YyZFWXKF0phscetxwi2I3LYdOeoMpvW7Dyb2fjJem8Ck/i2k87nY8HXn9KZWBa9+wKLzyad45WI577xEE9Q6M/IVDW8bv7ZucRYKhYbOWcQbJM26FDa4NDBVlpUoB5fu4rnUf9DPe8DvPk6K9axp/KxRg7l7fJ+LXx7odp28zU3stgnRsGtDTo/e5M4HPJVCSLTbGd7k6t0Wzvhwh+yuLq+hsX3/Fzn5/SLmV5uvs4fTn1BFlox/tmlGv7EONzN25Fti1xKDj93EBF5LN627Jh+4mhk21yxvx+5XXeNzHGlv4P9R4/bHrn38b30WiIJrfFv5Hee93jQ0UskIYf3FeEM3ddRfimP6xfzuFSf73RvbNrfrXOV5Mnzfd/v/0pW6tD3UFWl5xNjy/hnfcAknge+qUnn7p0l5hglpgSLQpEhh7GUkyo2iGFEZ5osQGSLfHOxfp0pdn3tBl5GHJYRQ77CCRXlLLYsfs6vWf6BqhMKhUSs58LBLrG2I07P/RG9L6Vj+XcyffpBqgx89lWJOJEF5KDoj5LnzMVhVQlGCzmy2h0u0nMbIt53bO7kj5bxJQ8iIio+Ko3FRWP0GvV/fr9cbUsKjkPxLTmNU116PGlZfFusn18TcAwXNVpb+VWwcCioyngD/NqUL4uvzTqGHLsNjTy/b21EXOOw9NhJjpE9br2GNLaMj6ecGI/tiB5v9fbxeW00wj8XO6pzGjth3rNFAvrzLi/l13L82XyAGGrT61nyO/KK7zEa063Rdvh7crl0om8XbZ/TWK9f7Ah+aQwAAAAAAAAAAAAAAAAAkMRw0RgAAAAAAAAAAAAAAAAAIInhojEAAAAAAAAAAAAAAAAAQBLDRWMAAAAAAAAAAAAAAAAAgCTmGrnI7pPJmGNOhirTuDnI4o6ITgQ9feZZLD7/vBNHfO3nnvwPi6+77noWt0U2jPgciRhQW/rVFlewnsWrXmlkcfuyAlVnUlUliwsD+quKkfh83fxvAOJ2j6qT7s7jextrZ3HE0fuvt2iTysuH/z0wOEjrursTqPX5l+8nSk0Z+rc7aChg8SPEitaxuLFZJzEvyhjD4pKS8apMbS9PUO8OTGDxpGkHqzr+7DIWL373Qxa//NpHqs4jf3+OxScdq5+3tIy/diHffeqo26jqRHp4ovmuzgYWR219/KSJ00d93RpVprx8KoszZsxg8czpi1Wd92p4+/R40lUZ2AUBD1HWli/LiunHK4pYeM7MsSz+2aPvqSpRmz9PMKjPvGO6gyz2uJLvb6JMHXpXhMeRMD/nRKP6HFQ+lrdpKuXnDsryqzrnnH0sixe+uVCVCbi3fY9pjm3Y29HpwIlEqVs6ipVr4urx6ceXsNgl/p5vbCVvI0REZ1x1PIsfmvd3VcbJ4M/jdqfxAoYmYtu8bcVivO3ZpmZl8+8y1NSmish3Lc+y3oB+4pDD+4EJR05m8YYP9fn8oYXvs/inbbrMsgjv64I9vI9ypaoqFAzK45V/Lo7jECQu0+2mjHQ3ERGNKy1Sj68Lr2Xxkg83sPiEM6erOrHeLvEieaqMVF2zmsWXnPVtVaYnrM+Ru6px5CJK1cwvqG2eTH6Wf/2ZR/nrVOvnKTyEx3ZIt89QK99Dt+VmcSDTo+q4o3wcF2rnz9vZrF8n3NHJN4jXISLKragY/vdAbFA9DjszUr+aZtgmx1MRQxn5vF4R6/m9VFPz+ohl9hx53JjOz7JXEmU8marGsmreV8QcfW6YGzqIxSmBLF4gWz+v7E/Mo7mtfWTy9DWNa9fR1vdt6mPb2vg56PlnXxrxOS+77EssXr2mRpVxu/l5qaRUzsvFRJeIcnKyWdzc0MziV//zqqqzbOnHLB40rNvI9jh11kksNn0utSveYfG5X76GxaZ57noxV/dl6eNUfi6WGLa5/fq4DeTw+YJtJ8/YfzTxEx8Yt6tzrO4lmvbi/nye7Oxsvisq3LwtFft0Wwpv6c8H43Fa1YW2BntXni9fbesItxtKbuMnvYbUYRx3bjP7VJ/adspcfq1k8Zt6zpGz3b/jRBTc6auMIhZRypbrE6YxguzLo9Eoi33pem4mxw2mkajLxc92Hg+PM116XNHRL8a/MT4nsW29buYS17wstz7Lhnr5NcVImM9TWur5OgMRUSzGx20u8byWYRHMX8DH+OFWfRz6/eUsLirkcyi7V3/e0Sg/f1tiwOXz6flcNMr3N2ZYht/dKUTyraoDAAAAAAAAAAAAAAAAAMAwXDQGAAAAAAAAAAAAAAAAAEhiuGgMAAAAAAAAAAAAAAAAAJDE9mpO41w/z+/iytD3sM/N4Rlzp009VJU54/QTdvo6H7yjc2BefsmVLO62d35//b0pKnLUeGyeG6yziedzJSJqSOU3HJ88eZIqE+7h92oP+Pn9/rsjIr8bEfXHOtQ29rghR4lUZOmEfycdP2v439H+GL30yScjPs9oMLUii9K25KqsWxNSj8tb7Pc189xJtk7hTb0RnlcgaOmbz4fD/LVCNn+htpCuM+PgKSyOiFwnK1bwXJBERK+KvKTRmM41PLuK5+SuPJLn4XM36uP7oX/+hcVfPJ/nLc/J53kqiIgqKngOqdzcLFWGelbyOIufg06/8nxV5ZBPVrF4Zc16Fv9KvwrsjMca+o+IyGv4u6TUHBbOOP1wFk+Y95aqUt3IG0rUkI+hoYkfm4GYoXGNMjIzyDhDmQqRbiYk8nPWNeg+IRLj39vkbJHP0tFJMmYfwfP4RTt1Fk/3dok0BpIoL19bkMiydE6WrVY08sxfBYEUFn/p9EtUnUMPupbF76/QOemX1PC89RGRAyYi+hoiIrePjyNcLv7dxwy5cRyRCyfcbcrLxx0wlediyS/PUWVaRIa0iMglc8G3LlV1/vhrnuP1jt/+XJUpEjlerXTen1qGPKuOyMPnDPB9idnIvbor/L50ysgY+pwLcnXu4VCAn928IgVjQPTtRERFmRNHfN0+4ue/qQcczOK4yu+6/xxxTIXa1trMx1MfvMfzxGbraRZ5xOk7GtPvMdLN+8+WWB1/3UF9/ooGee6nuhqeE7PFkMjZJ5qWV3+N1Ju27Tsy5bKCnZF5g+UHLHMRE+mlAFMeRDnakGM7fc6k/qdZeMO1txued18xHUcphm3biW5Qm95+hW9b+slBqsx/ffkAFk8tEJ9vj2EentUrNhg+T9rakJPp7/1t2vp+Ozv0msb4Cp4nzqnn58fuJp2z7vU33mbxykV6DYlIvpYc95i+A3k89Yt4z4wRImF+rDQ1NxtK8XN8KMTHOGeceYqqce+9f2ZxIGA4OQtOnxijRXWO76bGzSwOh0f/3Gw0Corj15SBVPYke3WR+TNMrthOTaDOOhFvNpSR85B0wzkltKXtJ88MG/albNHP/eAbel31nt8/xuJIhM8vct06r22dKffqdh7+p+43Fq5YzuKooSsMbvfvpJpNOETx4a9Kv3OZ59jj5RNFl0tf6/F6+fzBlCs5GuHfU2sjL1NWVKT31eZfviXmILalexI51nAMZ7zWdj7W9meJ62/t+vqEHdrA4uKxE1hcWMrHnEREFOP7u7lZj/GzLX7Mjyvl1zA6WvV7lPmIHYe/ji9L14mK9dtYdOcNyxlMfFyaTDMPAAAAAAAAAAAAAAAAAAAQcNEYAAAAAAAAAAAAAAAAACCJ4aIxAAAAAAAAAAAAAAAAAEAS27s5jXN4njLHcI16TD7PYzfrkCmqzPRpPK/Xz+68l8X3/vxOVWd/5jDW+L3kPeI+8S6X/lyiPTxnzVkj5HUmIvrd7//G4vZOnfPV7+cJzyxb5DSM6RyHHpEH8ciZOofT9vvX0xuh//fHB0fa3VHh/DNOI69nKDfjq4/9Sz3uiI8zLFpcRobOqeXq45liYr31qoxH3O/fEffTb+Wpk4mIKFhRxuKxIo1gnUfnA1hfW83i/Dx9yijx8Dc5bhxv0yuW8ryaRESvv8Hz1vZZ/Bg7//wLVJ0DJh3I4lS/ITdbmsyTIXP3jVVVig7iz+OJydxisEvS3ETuLfnYbMNnuZEnHelYwfOfV+SJBIxE5BYpGTzuDFXG6eHHbzjM2xHPiD3ElAlsezqjx57KSjbya8nXSSNtgohN2Qp9YqPHwzdEDQmie/t4/rP6Gp73u7xsvKqTlRtg8Vln6j6rabv8yZ6+z07u0L2tvIoo1TV0rj/2GJ271/HwHMCRCP+2n3xN5/k+8cwrWPytS7+lylz1A573uKWZ5+lr65Q594giNs9HUzCmkMXy+CEiKsnlOWocr849SMTHZDmlvANqj+g8yBHRz8V6eX95xVe+rOocceIcFv/hr3erMk4nH5PZ/Tw3jm3Ia+vN5H1LpJ/vW3SEvDHAObZNzpYc1R6XHlf4fXysWjaO59q+7Irv7NbrHnvU2Sz+LOUwlj7+4H217fIL+fvu6OC5wtyGVKhieEgOpasy0ZBof6JfiET0ucLj4ucp2QQMaajIFh1Z0JAYMbpqW3s0pMyCnej+7Y9YbIl5d5Zf9z9xcXyk+HS+OfLl8zp5PH/48gY9d/zFrx9gcfWKFv28IzKNfOScWR/PRCHDNmmkDHf6mJeOrtDz7O5OPmcbtHh+s1S/oWFkyf01Lc9sbWC6rxy9OmlrruC+dt3HLnxb5DkU54s0f6mq4xVJ3medcJIqExM59mReOFMuP5c8+Yoy0aCeD9nyeUwnPDE/zs3nY6e6lR+pKnO+ej2LC4sKWByTyfKIyC36YZehX4708rGS5fAxmd+v24Mvm3/eE8rzVBn47JNHzHhDmXwxk81387zYr8U69+g+fVbJrJdjya/KhMR53JtAxtWII3KGW4YBH8ZMI5Irr5/1XLe//eFhLG5s1F/yL/++hG8QU/Uf/O4YVad8Ej83//Gnr7F4+Ty9LzHi53h3TK+u+X18vjbZX8kfz9RreNmBAIvPmns6i0vG6b68uILXmVqh13gLrG39TSgUpuySWarMqGQPEqVsGTsYrjPJMYyVOvLvSKNRPs43jRHkGCYW42PbmqDuAzwufh5zicljzDgs4vtrGfalK8LHXC29fCzuGNaoo118/cop4fvmDvBrKURE66vXsNijekuinCJ+7va4eFyQz/tKIqKoGG+Fw3z/o30JjOMMXYRru8/OGUz87IdfGgMAAAAAAAAAAAAAAAAAJDFcNAYAAAAAAAAAAAAAAAAASGK4aAwAAAAAAAAAAAAAAAAAkMRw0RgAAAAAAAAAAAAAAAAAIIkZMljvOeWlPGm52+tTZUIRnjz6vbcX6DLRARa/+uJLLO6KtO7uLu4XlkgSHvD5VZkLLjyXxVdd/w39RH08gffKj6tZnB1YraqceOJxLJ46eQKL8/1eVeejRe+z+MBKnQS8ctyY4X+Hwjqx+Gg1tWIC+bzpRES0IV8f390tYRZ3ij/TiLp1AnK/OBy8Pt1MA538GKqK8dd2eXTW+DXvP8rijMwsFleOyVF1YrEeFq/+ZIUqU+TnidinThvD4qefe1HViTr8OHt/xWYWt3c8repMm1zJ4ksvOlOVySgrUdtGls6i7GkH7sZzwLDOFKLYlgPdG1YPn3DoN1nc38kf32B4ypiILVefKtPaybfJZqPPbESzRJwn6njcuk5nhMfLDc/bJeJ0EY8z1JGtfIOITfsvt2UZ9jcgmrU/kC9e11Z1YvYgi5vb+DfgdetzfIGX701GcZEq4+/f9u+UiPxWR6+JVTPI7U4lIqIvnneSevzhF/m52XGlsnhVg2gkRHTP7/+XxTd+62ZVJtfNx2AbOnl7bG/uJ6l+EY+r3WJ8FVBVaAx/GTr2kPGqzKTp3Sz25PL+Z2OjbDVEZImxkot3jn2O7rMOPfRsFl+fo/vCNz/kn/fG5gYWe72yxRKFe/hnFY1GWew4uh3BTljOdt+vPhc0NdWz+IKvXMHiGQfPGfElLviiLrPkvTcS3MFPJ0XEeqSnpYo4HNTnWRfx444c3qbdhlmdLfqsvo6QKtPVyLdVlBez2OfyqDqhsOiHLd5ufDm6TQREh9Ta2qHKUP92HZkTJ9PxAWZOfZDFoW5+3mq3m3Ud8TVFw/p7q+/g59HVMX7+frm6RdXZLPr48mI98qlv2qi2cabzap6I9dyLSA6GDMdZQq2SO3MqHzVePedYVeaDN/k6xrQ5B7A4y2UazcmG224os3XslkztYfvznR730wjrPwPRgNrW3pLN4pKSUlUmJ4cf39GoOFb4ENnIId5mBjx6rce2Rx43xGL8+45GeB9QUKHnrIEc/h6nTZvC4o7Nuj24RK/lRPVxNraokMVVlRUs9hvWkFauWMli2c/B50Nk5CLkiN8iheR4JUkUiqPcbejHXKL/0SMsLSTq9JvOH/g52IgmBHhcG9wfezEklS/J0N/v+Yoqc5Cf90fhcECV8Rfx9fyCWfzcfPQc3U9MTR3P4rkv/YjF3pQTVZ0+4nOFa2+7QJW54Z7bWfytC65i8YYPV6k6r3/4odq2xw3quc+o5QwSpQz16SmWXtOQKyOxftHfW/pE4vNlii26jKPOSfyVHEevydhijGO5LfG47n38Yi7pduszaKSX9z/hMP/++yO6f0pz87FTfhlvVyE7Q9UJO7ksnjpZzlGIDppawOIDXONZXLRSj8leal/KX1ssqcds/Vn6PHz/wkE9jrO3+94MX8cOoWsBAAAAAAAAAAAAAAAAAEhiuGgMAAAAAAAAAAAAAAAAAJDEcNEYAAAAAAAAAAAAAAAAACCJ7dWcxvm5/J7eMtcFEdG4sTz/aV6Ozk1UW/MJi0ORtk+9bzLfGJHObmQq4xZb3eojNF2H5zcMtx2eY6rKkCP4xm9/TWzR91CnDL7tnLN5rsTWdn1/9FiU3xB91iE8z03pQYeqOsd98XS+oWezKkN1G7b92zbkHRqlNq5voMz0odxZroi+MbzMuNgiDt0OQzqSmJ/fuz/g6Hv5e138OBwX4G2trEy3o+W1/MVr6zexuKsjTdVxLNGGo2NUmcaOAI87+es4ls5g5Pbwe/t3dPF2tPT9+arOR2X8PGBH9Yd30vE831jlUTrfmMZzbVKabo+QuK+e99+UtiUXhmN1q8ff1OlZR5Ql4p4E0ofKtlVg6O2mT+N5Pib6+fE+JlOfd/t6eO7V8TX6Da0X6f1k+rMcQ/IieTi3i5hnzBhSxtOLkc+Q0zhXpBjziHyttlv3WTGR5MItcpTUNfB8o0RELg8v4x+jP3DPdkmXY0n0J2vtmy1ypQ29YVvlYyR65l8ir2Nw5Ofsb+Zt68ZvXa/KuGyem6V6Oe//NzcanlimP5GxTlNOzbwroSlfkdnCiQIZfH/v//08FhfpKjQ2m/dzlcU8J9NHH+h8mFVjD2LxpMrJqkyw7ygWr6hez2J3mm6g4V6eY9Ky+PHtJGnutt1lueJkuYZG3dGYzt3b2hZkcWeH6WDl7n/gJyx+6rlnd38Hd4FpriDPu4mknnOL83euX59DF73/HIs72/n4MF+n+SaPeJpNpo9SDF+DXbwTi5oSGopm4srkfUthoc5rn13Ex35pmbJ3J7KcbT2mbTvU3LRJlQGzFY28LcWi/IsN9+gv0pfJc61aLv2dtLgGWLx4ZR2LGwz9gs/Lx/7jigwH54g5jU15h2W+X1P+X0nPb4gGDNt2XuP/buE5//xletB1+XW/YvGhZfzzPKZikn7i5pdY2LZcH/MFY7fkRg6jr0lcUG2pW/OOiPfWa4v5gzVelcgu5OfD3DzdRvx+3j6b6hv40xrG0uPG8jzN3/vuf7O4OEePQwNZ/HUmV1WpMiWlfA1gXW0Ni//54P/pnYFRQWYDNZzyKSzOqU/Gdn6OHa0Cam145HXRQALP2y83GNq+s2Ws6Zi6TiAiomNP5ee22sdqdlByV8m89TqHbo6f5xrubFuvyuyOoy799M8hV7yqxuqxSs0m3mGm54wd8Xnzq/gzL12RRLmF9xfLIkoZOkHEbbkKSWrg4KhrVToXbiTCz2uWpUfJ8pTkxPhru936mgC5+DjacvHXCRjWb31eXidiuCYQFTmL7XYep7nERJ2I+uW81sVPpNk5uk57hhxP6eO7soK3k7IKfv3NMNyirsYeHr/DnzcU099RQOyvy9LXpmKsfzIcGzuQRMu2AAAAAAAAAAAAAAAAAAAg4aIxAAAAAAAAAAAAAAAAAEASw0VjAAAAAAAAAAAAAAAAAIAkhovGAAAAAAAAAAAAAAAAAABJzJBaes/xuHmy6MycHFXGX1DAY1+6fh6RrHtsPq+zsb1J1XGrLVwiV8vdpJN1u0WaeL+PJ733eWUaeaJotJ/FwVAbi2cfN0vVKTqQJ8imXv0ea1esYvHL/3mFxW+98Zaqs8jNP5ljDzuIxaUHHanqEInvLcvw6ZX2bft3T9jwHKPTx8uXkidt6DiJNUfU4zkiL7stcqM36Rzm5BFlmjt1GZePJzrPaWtn8aYmHhMRxXw8Li3lcbB+QL+O1cFit52pyrTX1LP4ySdfYnHY1knWLZG8PdzczGJ/UbmqM+hNY/HSlTWqzJJF77P49pv6WDzmuINVHfpgGY8rynQZSNjr9a2UsuXffsPjF1bxA/HAY09hsWXolSZP4uf8/77lflWmNrrz/Zo2fZzaNnZSEYvdPn6u82foPqvE6mZxWcUaVWbCkjoWt3bxx8VpgYiIWsT+B0S7zzfU8Yvm6DN8dlke3o+Fwz0sdmXq83k0yNunz83j3NwsVScS5WWcUIcq49muf7Qs06cwOrUHQ5TqGvoePC49RiDDOX4k62r4Ob6tdrUqs75uM4s360N1r5g6+QtqW8VY3j/e/9A8Fm+u1c+zOcT7uXW+91g8tuBYVce6cA6L25uDqkxV6UQWjy+awOv08TEaEZHl4eetaIw36hg5qg7sWDTcQSmDQ316Q0OdenzJEh57fHzDzT/Qz3nzf/1ij+3f9rJFLIb9ZBmGxHKbbTjdFY/hcbhXFIjp82xh9gEsbudDJwrrYSj1601KqofHfjFeLCzi8zkiIp8/l8VuL++QcnICuo6PlynML1Vl3Knb2lIsZtPbb2wy7TIYrO3g5yFbzDHyC/TYuiXKD86mzX2qTHUj78+rg/xx02JCeTkfX1mOnvBUUoDFheMqWfzeRnEi2G16fiPN8lax+LAxei5QmSeO16lFqsxZ517M4pde5POS9jrZ0Inu/sdrLD6q3KfKnHbYcURE1Bsb+b2MFsfNvpxcrqF1i5Ur9Rhnc8sqsWU3BlN7jWhHToMqEcjhYxGPx6PKZGfzc6Y7na/jxAb05CcS5dtuvf1mFgc36/H5e28v4vsWkD0f0b//9SSL29r5GPPEU7+i6nzjm1erbfD5I886rYYyr4pYr/6MPnpVjMhFvK+LUVyVGWHJIiG2g3nH7rjrnltZ/OBjl6syRx57FItvvulLqsyiBR+zePbRJ7D4jPO/qur85n9uTHQ3PyU58jetucjJC1+fuOtX56ka//r7C2KLXieTzjiTX2vYVP/JiHVItZkUYynYAdve9pEZJqmOi68NxsSYwSI9Fon28wmmy6Xnhe60VFFGvE7McE3A4ucxOU+0Y3pOEg7xtcyorWchjs2fd8J4vtZz7IkzVJ0Fb/JrZ2uXLmNxcXtQ1Tly2uEsHl+me4Wyiilq2/ZyDQvmBQH+HVSW5rF46Ro+/iIiirn4+NAyLKp7XNuupwwOJt6H4JfGAAAAAAAAAAAAAAAAAABJDBeNAQAAAAAAAAAAAAAAAACSGC4aAwAAAAAAAAAAAAAAAAAksb2a0zizgN97mwoLVJl0kePogCqdb2l1Lc93dtbps1n89CPPqDq2M1K+RH0Pb7nFRTq/siWus7td8rq7ft5YjN8nPjfA7/8/oaxC1fnoRZ4X9uknX1RlVq2sZnH1mrUstqM6f1RU5JJ1YoakuiPSeW4osN13a+n74I9W7y1ZTWlbDoHcFv34LPHVnnbudBZv+mC5qlO9kcchQ4por8hbFxFpBboMh78rwOMCcct9t+FPSKIid9bmzTrXo8vD758/7ogDWdwZFAldicgW+1sxmX9QvlJ97/9YmOeSDQWDqkzAzc8xz//nbRbPdemEf+tr1rN4cmWVKgOJe+jmi8mbPpQvIWrIz+W4ec6UmSeexB+P6eMlIHLqLjl1qirz6+dWslhmQ1m8RDQsIiopKGGxy8uP5bpmnq+biMgV5Xk0yg2JKA6ZNZ7FG+saWdwR1jnp2kW6M5nDuKJQVSGf+HhFUyQiot4ozx/ii/ITim3IUx7ezNtaVJwc/AH9vfryAiwO9eu25vNv61NdruTJxdTRtZ6s1KHkMhPGBtTjhx3Lc4MuXjByXj55+vvjX3+nyjQGRb4Teagajhc1hJH9TwJDhqUfyzyDRFd95XQWf+0rh7C4tU3n2FtRzRO21i3neW3+8PvfqjqlY/ibKja0m0vPP5TFh1TwPJXPvs/HUkREdg5vAzF3Gosd2/Rhwo6sXPQGpbuH2sT8eTpfkRzDtG0Y+cDzuvmUptuQ31cqF19bfrEuky2mMvJ5g/x0SUREYZGwzpBWiNpFVyeH4+Or9DgoHONP1JHAe0xEQJwbcvN4P52frxtSdgEfb+UX8dgxJDT0i/ycHo/OibX9PKu/f3fmKMkrnM4PVjuVn7c8bp2Dt6GN9xMfrGlWZWo6+Zcpu4lCv843N7FiPItbmnVDufOHt7B4ymEzWPynR/6u6rw872UWd4Z1dk09itTkDP/H37iOxW5T4zpsJgsbl76iihx0GF+jOGnWsSx+7L6/qDrV7Tx2hfXEL88fIiKiqClB+ig1sXIyud1D483DDz9KPe4Wg16Zwy4YCqo6Lc38eK+r03ODDXW1LO5ol2VChr2V43rZJvQamN/PT7zl5TqPti2+73HjeJnFC99VdUIi35/b5RKxHq9sqOPznWhUZ139zk03sPiHN1+qykByqDNsS4YcxpLOUE/kiBUI02K73GbKjTySDF+G3rhl4DXoxIl6MH4y8Y8ZeYRw1z18ber4Lxypysz54tFiS60qI42dJFuOnPuuJ00eLaaM2HJUJtdpTLmHfSLmfc0XxZo1EdGihR+JLe2qjHyeWBd/j7GIzsVKtFTEcr3ZNO5J5HPZvg3soQnT54HjEKXseJ0t7ohV0hgv67j05+1283GDO1WP+x2RZ92Wr2MQE2OcaJSvCXgMJ9BwhI+RI4av3usLsNhfWsni2ecepurMOpFve+Ef8/m+OHoec9XX+LyAUlWREb30n0VqW+NGfq7IF4u+XsM4LhLjn6Upp/H2K+RxtVq+Y/ilMQAAAAAAAAAAAAAAAABAEsNFYwAAAAAAAAAAAAAAAACAJIaLxgAAAAAAAAAAAAAAAAAASWyv5jQmr7ifvt2vy6Tz/LhnnX6CKjL/Zn4/8TNO5vmC2mfLnAJECxe8z2J5dTxm68wb8sNwufTHE7N5zhqZrzhm65uqyzKF+WNYPP8VnneViOhhkXOnuVnf/z8W5Z+nI+5H73L0vexPOvEYFp9+/jmqzMh07kFyXOZ/j3LvtW7LWqTvjE9UfBj/vC+87TssHrtE5oYg+tnPeJ7GV9bo3FZR8dUGxEceMqQxCYhtlkiFEjGkPugV+WdjrjxVxpPLt51z/hdZrPN+E/31b0+zONzN89p5vVmqjj+Pv44rpjPJVJTwMuMKRN5Jt84/O/kYfj6hvGmqDCTu1Fv+i/z+LXlS+vSxW7+J51t47/3VLHY5PBcXEZFLJHt8+j8rVZmRsjLoZyX69zzeT1x9Os/fUpavG1JHI89PHHHpPGWFsyayOBjh7zli6Zy1xaU810q+aGrj9KFLtmhaZZW5qozsBgI5PK9Nfq7MnkxkuwPiOfj3aEijofpYx5B3Lz1/2/OmewzjgVEqLy+dUl1DPUUsovPCX37RKSwuHbOOxY1tOn/e+hU1LH7n/fdVmaLS8Sw+/lQ+Jhuw9HjFtvgB09rJj1V7QOfQaljJ88+s3bBMlbFcfGznF7kIW2WnRkSHTD6IxcUBvr+d7Xr/n33yMRYfeNAYVeaAUv5alsibGg3rviUsjmeXl3e6HrfOzQo7tmZRlLYODWKGFN5ypFHgM+Wq4r54+mQWP/yozhE0k6c0UjmMOw2pKruCPI6J6UPUMN5qFXUSzxq0TSikz6HTps80lNy5bNEt5ObJbK5ExcUlLC4rK2ex36fzocVEjjGPaNO2rdu0z8fnfB6PzLtG5LK2bYtGk6ef2BOae0XudTEvrN3coOo0NfI+qbZTz4/lIS6/tTy/Pj4si58Tzzhbz+8v/sn31Lbt/f68k9W2hjf4msBdP/ixKvPw4ndYXOzS45zphTw37JnnnskLhA3HnnibP77zXlXk3wveYPFjf/kdi797x/+oOp90XMvi5UvWqDILNw6tAwyYkoWPUutqasm1ZcA5zpDv94BJfKxdObWCxSWluv/PC/Av0VG5IIn6RE69lhbebpoadd7vts28HYVEkr2o4Xwoc/mZyLzHr7+hX1taX8Nza04Taz211XyMSURUWcHP+b+971eqzIyDdG57SE5r9/cOfIbJYWTAUEb2qTvOPrpjza06R6vHNzTXHIzvzqgzOdS28f41TQ9DKRLZILa8ZHgmPt7q6k1gHuiR+ZSXi1ivmyV2dMhFIhmbsm/Lo5D3YZ2tupVv+EjmdP3A8Lw8Q3d7kL/n5cv0+IbodRGPPOfTM0XTfGH7deA+w+OjVNwZ+o+IyDL8RlSsq5K45mXH9NjEEde4HMP6vrx25og1JXe6vkbkEvtni32TeZKJiCzRJkzXGtwunlx40RK+btYROVHVOVws6R74XT5v2Y10xUY//f0zLP743dWqzPgiPn4NeHm7qqyYoOrUNjSx2PCxUDi47RzjDCY+x8YvjQEAAAAAAAAAAAAAAAAAkhguGgMAAAAAAAAAAAAAAAAAJDFcNAYAAAAAAAAAAAAAAAAASGK4aAwAAAAAAAAAAAAAAAAAkMR0Nuo9KB4KsTglZsg0X5DDwhOOP1oVycnJZvGrr/Bk6Weceoaqs2L5KhYHg8Gd7eoQK4WFjiHxvNudxmKXWyT8NiTrlpfmQ6EeFi98d7GqIpOCm0QjPCG5JeoUF+arOldefjHfkDVuxNch6uVhf68usv37thNJXj86hLf790rD49GySXxDdhELj7vsa6pOaWkpi2ed9E1VpkPEMl+9Y2jZoQiPM0SZAcOhG/NksLiovFKVsaL8xd995GUWf/cXt6g6FdfwTPO3//pB/pzpm1Wd6dMPZvGRh83WzztlotoG+9p25weXrR4tcfHzx6wq3i8sXbxa1Yn28nPK5KmFqsyGFa0sHhxxP4kGRPz6vOUsDhzpVXWmV/Dz6gHTKlSZihn8OAxFwyyOWumqjpPZx1/byxuol/R51eMR5/zyIlXGdvh3EI3yhu4i3u8REZEV489h8zrpqfp7jfUGWez3+PXzxhzzv0e5KZPLyO1OJSKiF557Vj3udvHv+siZB7C43/BRrS3nR29uji5UWDyexWFxrm5qb1R1og4/DksL+PHuWLpNjCvjY71Qf70q88gz/+QbLL6/B5TzfSUiCuSWsbi9gHdi4d5+VccRQydnQLeb999exGJ/Hm8Dlq0/y4gYQ3pibhb7/IbjHXZo5oxMSk8b+tzHlYTV4ytX8DgWC4oSehx62slHsnjRa4tUGZ84fIPtPG7r0vvayQ9visREvJdOZffc+7TadtkV32HxYTOns/jjlbwPIyI67HA+zvcF9Ljf5+NzsUA2b/eyDyAicvMmQLbDPxjL0nUsi1eKRnRf4tluSDCoH4adiIoToDznt2zWB3hHkLcl00fuHiH2Z+jzn9fN5+4XX3uD4Zl3Xdmxx7E4Ut+myhzm4WPEhqgu86XTxfxrkpjfyIkVEZHocl54611VpNfh/dRDz4k1i9+doOosr93A4k36lSm/d+h7sk3rDKPU2wv+stPHswMzWTxbrCGdcLKeJx5+2BdYXFZWqsqMLctk8aRJ/HwYCvH5KBHR0qUfs3hDfQOLHZdsNUQxcYwFAtmqTPEYPl9euHABizMCAVUnNz+PxW5xsl5dvUbV+fu//szi8lI9TwHYKr6/d+AzYkwCZdoN2+SoV67rJaImZtjYiW9mJC01fI3RtGxdmS/7BVO/y8fNDbV67VJat4R/8ycf1SlLGGol0ufLN+ETseEaDI3faZ3NXT0kPbGQz6v+8/TzqszZ5/ExmuPhY6IVG/WefOvaR1j8+z+dL0qY5tiyTzW9x+0/uyRtG3YCEyl53cnW16Hcbr7N69FjhFiMr03JNUhXmr5A4fbw7y0a48dyaW5A1Skt5WOlldUNqgzF+IA9Vs8PvF9/9yFV5dhzj2XxBRdMYHEi53uTD1bxz+Whf7zE4oCHj/OIiCzq5mVy+PFuuWQbJ/Jn87FfljdDlakLrx3+92BK4r8fxi+NAQAAAAAAAAAAAAAAAACSGC4aAwAAAAAAAAAAAAAAAAAkMVw0BgAAAAAAAAAAAAAAAABIYns1p3FQ3As/J89wjbqP5xlKKcpTRS77Ks/De+P3b2PxEYcfpeqccirPGfT0U/qe+1IkxnP5uQyJYeX93GOx2E5jIp1rWObyI0ffa16mD7MN96N3e/j+zZzJc+x8Za7MB0B03IXnqG2c4X7/gyLTRrvO2TDQse17HAhH1OPJoNWw7TfPvM3iwsMOYfEpx+vvtTiX39v/f749S5V5+j88p8R7tfxxU5oTkZZPpevyGM4GLg8v5DEUCsT4/fIXPsxzGk9x6RwTp/+/b7P4t/ffxeLN7To3gdvD7+Wfk4P8xZ9N6TScVyRN589Lq+DHQ0XFGBGbcm/w4+6SUyepEr1BniemZXMzi+sMee8WLuL5k+0o77POOXqKqnP4oWLb2HK9uxXFLDy5iu9vxwfVqsqzz7zCYpeLdwKFeTrfhT+XJ+j0eFNVmTSLf3buDJ4DwzIMA5wY/478/ixewCOzQRN5Mnm+GXtQ9/e1a+qG/90T0c8xWk2bPJE8GWlDQbfOw+tz83NoOMr70N6ozt86bRrPe9zQUqvK2FE+pvGLHPURr86xFxb5aNxekWvGlFtJHkL6aWltw1oWj/PzvKqTK3RbI1ucKwr4MRXs1fmWNoeCLF5fpzOFdUX483jyeP7CQC7PU0Wkc4FHIrxHtQ25WWHHxlRWUEb60PkqaK9Vj8dc/NjtjqaJEpkkzbniGyy++9d/UGUWr+DtL1ekyTKknaTcXB4XyxRBLkNeeJclQp37Kd3Nz98DIs/7hmbdf970XZ7TeObMA1k8bTrPA05E5Banb9vS/YTXxz8IS/xNsduw/27xYYXDvD0WjylSdbL9PA9Vd6hblbEHt+WzSnGhXe2K1nb+eUZ6+GygK8jbFRFR1JCvWpK9ucxs77Vk+ySaecgRfENpoSqzW9L48Tu+QGcZ84h590FZepx2xsk8dxnFxNy1TOeX7flkPYs3OzIfodYa5fnZ/vHUM6pMTVCPC6RwcKjPGYwnW16+redX/b67g0tY/NwzMv4/w/PxE3p55YGqxLSpfD1lohjDFxbq8bhcpwlH+PEUMbSzcDcf28k6RESdnXwMs/CVf4kSeqztss5k8YMP8nyRZ5yk82ojhzHArhtv2CZHLab1QXk2kDmOYe9Zu4KPg0w9alenHCfrcz4Rv2ZRVsnnz6mGyx62miAHRGz6PZ+cZJvKyKNOzh9MY2m5rYSH2abX4f3EjTfMUyXGfYHPJ8ZPqzA8D/fCi3yN7qTX+Prc+ScfrurERf5nR+V1JrK2e49x4+p4MjCN8WUOY/7ZDDq6ju3m4/xwWM/fZC3LJY8h/byOeG1bjJV6wrrOAQfx+WVHl96XYCc/vkuL+GS4seYTVeeu7/Ncw08/yMeH55x9mqpz6GF8DtLSqNed/vq31/mGGF+LjRmuwUQdvrYQ6uPHd2dQv47MX+3yZKki3sJt6yeDduKXgvFLYwAAAAAAAAAAAAAAAACAJIaLxgAAAAAAAAAAAAAAAAAASQwXjQEAAAAAAAAAAAAAAAAAkthezWkcErmqTDlTM+TGDJ1L7itzeU7j+//Gc7P846FHVZ3v3vQtFq+r4fn+1qzh98EnInIM92+XojGed8iO8Xw0ftO9w708+1NU5MKz3PqDmTqV5885ePpBqsxJJ89m8RGnn8QLZOo8T/pe8jKvpCHngbhnfVeDzp0YiWz7HHp6dc6s0cqbXkgpKUN/e+HN1/kuPu7gn92fHn2RxS31q1SdA7L4MXXRqdNVmdkzec7Uu/7Ec0o8uHDk/Fjd4qsuNXz1JSKvqm0HVZlZp36JxVMuOovFh0ww5AHJk/kIebspGmPIb9mv81dKvd38/v6hEP8si/N1jr2UDJ6fY7BH5w+FxGVnn7i/d2GPuXPeRsNWnb8FYGdu+f6LIxfaK3Rfvf8ERSzb1jv7aD/2FmQk2xWNTZ2U7h4aOzmkc4e6fXwcubqOj1U//oDngCciOvjwU1l83OzTVZn+6LMsLq/iYyly8TzaREQ+Hx+fuMWY3uXS+VwdkUs+FtUDLJeLl8nN4fOfojI9hncG+N/6xkQeZE9mqaoj/zy4rUfnzbTcPM+anA5l5+jvqD8yyGKXyF2V4dK5k10xnpMpyzAv7O7dtn9WNFlzkO2eaK/IqxqW37Vpnssz+pkWBmStNFHILXJiExHNPvUU4z7uaeddf63a9sLjz7C4vFS3i8wTdV7XkSxe+oHYYsoDy+dfMyr4+eLgcTrvXiLWRof6mGTKaPyzX/yBPBlDSeRvuuEHhhItu/GsPA91fe27qoRp2+fJI3+5k8Uzj76QxbfefNm+3B2AUcs08g+KOGQoI1dK2/fI3kAirr9u5Dn5kUf/bO+89g3/2mn8eVO7aYPaNn38L3f5eeo3bWLxBac8LErIGHbOlMN6O5aYGKpkxLpKTObLNUwWLPG8+tqanoPExLU1i/jcsbVdz1lXfMyvn7jceocDAT5vDQ3y1y4qH6fqhMScs/rj9Sx+a963VZ3CfBGX6eseMYfnP88NTGBxdk6uft4x/Hl84prpwZP066xbs5bFC1fovM2+om37kjIw8rXPrfBLYwAAAAAAAAAAAAAAAACAJIaLxgAAAAAAAAAAAAAAAAAASQwXjQEAAAAAAAAAAAAAAAAAkhguGgMAAAAAAAAAAAAAAAAAJDFDCus9J+DLZLFOhp2YtIIyFt/501tY/N0bfqzqPPzPx1h8+KxDWdzW1qHqNDU1j7gvjkjg7SY3i12GrOBy2wknHsPiuV+do+ocd+klYku2KrN7UkUsvxOdbJx8GSy0ov2qyNI3Fmx7hv6Yeny0ilkuSkkZ+tsL29Z/g+ErGMPipbWbWZzr0XXGHXcAi5d/XKvKBDs3sPgbl13I4qbOp1WdV6oN3+12Gg3b3MEwi6eNyVdlDj35MBYfcdShokT6Tl+XiKi7u5XFa9Z0qjIb6ttY3B7sVmUOrChl8SHTeKL5BQvfV3Wamnm7tyz8LQ0AAMDekp8/hjzpQ2PjvIIM9XhF1RQW1zV2sfi2W7+r6lx2CR83x2Jdqsz5Fx7PYpc/j8X9hmlRumFcz57D5dYbrTQWmuY/7nRez+/388fd+nMJqnEPH9dZFFV1on38tT1uj95dOe5J4/FApFfXEfOf7Cw+57P7+lSdrigfU0YiPaqMx73tea2B5JlP7AnPbGzZJ6+zxubx20uWqDK/mzVjn+zL7rrs77/ZJ69zz+//IeLdex498x79/uu6Lw+fFy+7/Mvq8X/8/VEW/+2v/2TxyuWv7L2d20dS3RUsPufc01g8voI/TkQ0adJEFl939fl7fscAgIKGbXIVKWQoI0c2ekULAGB3OaSv8Wz/sHgsgbXveGyAb0jgmpdkD9hqmzPI98WVyq8bOIY6La1BFrvdeh7ucfN5eLrHy2KfmHMTEfkC/Hpb1OavHYnoeXl7eCOLx/qKVZmi7CwWWx4+D/d6+fyZiCjSxa9VesRbLMovUHXO/zq/JvPfv9DXNmuat80mBu3E59i4OgIAAAAAAAAAAAAAAAAAkMRw0RgAAAAAAAAAAAAAAAAAIInhojEAAAAAAAAAAAAAAAAAQBLbqzmNg+08B2l+yRhdKKE8x/wm3idewHOz3BnSObxu/t7tLP54+ScsPva4o1Sdj5YsY/HK6tV6T0SessLCQhZPnXqgqvOVyy5m8Zlzr1Rl9h95CMR1EXE/9+oPPlJFNixfNfzvvoHBPbFjnwt2XysRpQwFEZ0zuD/M743f7eJ/p/G6pe9h78vjbaJ12SZVJtrOj+dvXMOPu/POPkLVqe16g8cJpD5r5+nnaPLUg1WZyqpSsWXX/xYlEuJ1Xl2s2977y9axuK1lsypzWGUO35PYDBavrefPQUS09KMVLHa58Lc0AAAAe8ukqYdQZsbQ2N4x5FJye3m+n0NFvtxov547tDStZ/Fhx+vxuE/kPQrF+GtHDel9bJHzx7L4uNk0jQnk8lxD8jmIiMJhnvNX5hWOGPIIx2L8eVxpfF9M+aT6e3m2vOzSgCpjR/g8qmszn79lG/ItUTp/Ldvm++9O0/viOPx7HTMmV5XJL9yW+ykS0fM7AEg+Bdlpatt3r79sp3G1YaI7/423Wfz6a2+pMosX83WOpkb+PIMxPd9PcfFz29iyEhZPnjxJ1Tn0sC+w+NjjjlRlTjiR56hLVyUAPp0UEXsMZWRvbirTZtg22hUatpk+G0kOG5tEbFiNpSwR9yTwOgAAmpiTyhOSTiNMJObPtm0qxMl5rYkngz+vI85+jiE3c1S8tGN4mZjN59g5RXy+GQzrcZwtrmG4RB7kcVX6OkhLIy/T3q7zK0fD/PP25fDX7g416n3p4j1qZQG/prS0k695EBGddfxsFv+/b5ymylzxzSeH/z3oIKcxAAAAAAAAAAAAAAAAAAAkABeNAQAAAAAAAAAAAAAAAACSGC4aAwAAAAAAAAAAAAAAAAAkMVw0BgAAAAAAAAAAAAAAAABIYq69+eT/fmY+iw+eXK7KTJ9cwOIxleP0E5XPEBuyWHT2lV9VVRybJ81++KEnWOwyXC//yiXns7i6RieYjkTCLJ594nEsvuSKL6s6lJmvt31mpIg4UxdpWsnC9xd8qIpMLC4a/nckNrAnduxzIp22fYaG5hTu4LHlYWFdbYOqMj+/jMVjcsaqMt2Rehb//uWXWbymLqh3Jap3byRfPPccFs+98uuqjNeXx+L6Rp64PdytX3jqlAksDokiG+v15xIOd7HYscOqzOJ3P2FxVSE/DxSWFpFkx/pY3NLUocoAAADAnnHhN/6yv3cBAABGscmVxYZtF7P4m1+/WJUBGC2yDNvkqqTHUEaSq6ZeQ5kJIv5ExL0JvM5nXbqICwxlHBHHDGXkypj8POVzEBG5RZxhKNNn2AYAySZGRINb/p3Ib0RtERuuaTj8rBR34oYivIxljfzasoxl8WtTjmH/Y7ao45JnRyKfj58h+23+HkORflXHsfj7tuVrOz5Vx59zIN8XR5+9o+KSRdTu5hvcuo5PfC4tXXx/m0IhVWfBe+tYfOG3rlRlqsa+NPzvATuVVtepIkb4pTEAAAAAAAAAAAAAAAAAQBLDRWMAAAAAAAAAAAAAAAAAgCSW0O2p43H98/NERGP8Z9S9ffo2tT29ERZn9hhuXqJ+fi33R9+MI9LHtw2In6T3x/TNQvqi0RHLxAb4rZcj4j2FQj2qDg3qn8x/dhlubCK+k+iAvIUBvyV1JDb0+O4eN58H297b9u/RdDMZ8RnEnZ3HRDRo82PMHtC3+7YHeb0Bm7/O4CAphrtIjEge7z09+vi23WmiDL8HQ6/hvtgh0abDYf68sX59HNoD/HwyaOv26QzyYzMq2rQ8LxARxcTt1AcMx/eekhxtAiBxo/m4Gc3vDfae0XzcjOb3BnvPaD5uRvN7g71nNB83W9+bnCsC7EwytIldrmfYJleeDEtGIz6PqY583tH4bcj3ZFoxSuTzldvk8ybyvSXy+aJNAHCj+bjZ9t62P8Mk8n5FutK44cymTlIyxSlRPL7zW1ibb2ktfsMqvh95y2sifS50BvW+DA6KW03b/JpA6oDpGgwvMzjIrz2kyH0lIkeUiRv2V53wZWzrz2VQ3KbbFl+AM6ivg+jrknoMPWD3b/fvoedIpE2kxBMotWnTJiorKxupGADT0NBAY8fqfLyjAdoE7A60CQAObQKAQ5sA4NAmADi0CQAObQKAQ5sA4NAmALhE2kRCF40dx6GmpibKysqilBR9JR9ge/F4nHp6eqikpCShBOifR2gTsCvQJgA4tAkADm0CgEObAODQJgA4tAkADm0CgEObAOB2pU0kdNEYAAAAAAAAAAAAAAAAAABGp9H5ZxYAAAAAAAAAAAAAAAAAAJAQXDQGAAAAAAAAAAAAAAAAAEhiuGgMAAAAAAAAAAAAAAAAAJDEcNEYAAAAAAAAAAAAAAAAACCJ4aIxAAAAAAAAAAAAAAAAAEASw0VjAAAAAAAAAAAAAAAAAIAkhovGAAAAAAAAAAAAAAAAAABJ7P8D0lIl3WbkQc8AAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 2500x400 with 20 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "print(classes)\n",
        "%matplotlib inline\n",
        "\n",
        "# helper function to un-normalize and display an image\n",
        "def imshow(img):\n",
        "  img = img / 2 + 0.5  # unnormalize\n",
        "  plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image\n",
        "\n",
        "# obtain one batch of training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "images = images.numpy() # convert images to numpy for display\n",
        "print(images[0].shape)\n",
        "# plot the images in the batch, along with the corresponding labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "# display 20 images\n",
        "for idx in np.arange(20):\n",
        "  ax = fig.add_subplot(2, 20//2, idx+1, xticks=[], yticks=[])\n",
        "  imshow(images[idx])\n",
        "  ax.set_title(classes[labels[idx]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQBRSenSvD7P"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDAlhqzyyYaG"
      },
      "source": [
        "## AdaOrth Optimizer\n",
        "**Specifying the Loss Function and Optimizer**\n",
        "We use CrossEntropyLoss as Loss function and\n",
        "\n",
        "[Stochastic Gradient Descent](https://leon.bottou.org/publications/pdf/compstat-2010.pdf)\n",
        "Adam\n",
        "MuonMVR2\n",
        "AdaOrth\n",
        " as Optimizer with momentum and weight decay specified by the research paper of **ON THE CONVERGENCE OF MUON AND BEYOND**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXwhO0FpIcm5"
      },
      "source": [
        "#**AdaOrthL method:** \n",
        "\n",
        "\n",
        "![alt text](https://github.com/RayCyder/Intro/blob/main/adaorth.jpg?raw=1)\n",
        "\n",
        "#**AdaOrthL+ method:** similar like MuonMVR2 use two random batches to decrease noise\n",
        "\n",
        "![alt text](https://github.com/RayCyder/Intro/blob/main/adaorth+.jpeg?raw=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "AluijoKUIcm5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# ---------- compile helper (MPS uses aot_eager; CUDA/CPU uses inductor) ----------\n",
        "def _compile_for_device(fn):\n",
        "    if torch.backends.mps.is_available():\n",
        "        return torch.compile(fn, backend=\"aot_eager\", dynamic=True)\n",
        "    return torch.compile(fn, backend=\"inductor\", dynamic=True)\n",
        "\n",
        "\n",
        "@_compile_for_device\n",
        "def zeropower_via_newtonschulz5(\n",
        "    G: torch.Tensor,\n",
        "    steps: int = 3,\n",
        "    eps: float = 1e-7,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Approximate Orth(G) via a Newton–Schulz-style iteration (polar-factor-like).\n",
        "\n",
        "    - Uses bf16 for matmul speed; returns to original dtype.\n",
        "    - Normalizes by Frobenius norm as a cheap stabilizer (keeps scale small).\n",
        "    - If m>n, works on G^T to reduce cost of A = X X^T.\n",
        "    \"\"\"\n",
        "    assert G.ndim == 2\n",
        "    a, b, c = 3.4445, -4.7750, 2.0315\n",
        "\n",
        "    out_dtype = G.dtype\n",
        "    bf16 = torch.bfloat16\n",
        "\n",
        "    # Work in bf16 for speed; keep on same device.\n",
        "    X = G.to(dtype=bf16)\n",
        "\n",
        "    # Stabilize scaling: avoid division by zero and keep values bounded.\n",
        "    # (Frobenius norm is cheap; if you prefer spectral norm, it's too expensive.)\n",
        "    X = X / (X.norm() + eps)\n",
        "\n",
        "    m, n = X.shape\n",
        "    transposed = m > n\n",
        "    if transposed:\n",
        "        X = X.transpose(0, 1)  # .T is fine too; transpose is explicit\n",
        "\n",
        "    # Newton–Schulz iterations\n",
        "    for _ in range(steps):\n",
        "        A = X @ X.transpose(0, 1)      # A = X X^T\n",
        "        A2 = A @ A                    # reuse A^2\n",
        "        B = b * A + c * A2\n",
        "        X = a * X + (B @ X)\n",
        "\n",
        "    if transposed:\n",
        "        X = X.transpose(0, 1)\n",
        "\n",
        "    return X.to(dtype=out_dtype)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Base optimizer\n",
        "# =========================\n",
        "class AdaOrthBasic(torch.optim.Optimizer):\n",
        "    \"\"\"\n",
        "    Base class:\n",
        "      - 2D params (matrices): delegate to step_detail(...) implemented by subclasses\n",
        "      - non-2D params: use a simple AdamW-like update with a corrected input c_t\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        params,\n",
        "        lr: float = 1e-3,\n",
        "        weight_decay: float = 0.0,\n",
        "        adamw_betas=(0.95, 0.99),\n",
        "        gamma: float = 0.025,\n",
        "        eps: float = 1e-8,\n",
        "    ):\n",
        "        defaults = dict(lr=lr, weight_decay=weight_decay, adamw_betas=adamw_betas, gamma=gamma, eps=eps)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update_last_grad(self):\n",
        "        \"\"\"Cache current p.grad into state['last_grad'] for all params.\"\"\"\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                state = self.state[p]\n",
        "                if \"last_grad\" not in state:\n",
        "                    state[\"last_grad\"] = torch.zeros_like(p)\n",
        "                state[\"last_grad\"].copy_(p.grad)\n",
        "\n",
        "    # ---- hooks for subclasses ----\n",
        "    def step_detail(self, grad: torch.Tensor, state: dict, *, eps: float, lr: float) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Return (eta_t, update_matrix) for 2D parameters. Must be overridden.\"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # ---- internal helpers ----\n",
        "    @staticmethod\n",
        "    def _init_scalar_state_like(p: torch.Tensor, grad: torch.Tensor, value: float) -> torch.Tensor:\n",
        "        \"\"\"Create a 0-dim tensor on the same device/dtype as grad.\"\"\"\n",
        "        return torch.tensor(value, device=p.device, dtype=grad.dtype)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _init_state(self, p: torch.Tensor, grad: torch.Tensor):\n",
        "        state = self.state[p]\n",
        "        if len(state) != 0:\n",
        "            return\n",
        "\n",
        "        state[\"step\"] = 0\n",
        "        state[\"last_grad\"] = torch.zeros_like(p)\n",
        "\n",
        "        if p.ndim == 2:\n",
        "            # 2D (matrix) states (subclasses may use a subset, but it's fine to pre-create)\n",
        "            state[\"m_t\"] = torch.zeros_like(p)\n",
        "\n",
        "            # scalar accumulators as 0-dim tensors (device-safe)\n",
        "            state[\"g_sum\"] = self._init_scalar_state_like(p, grad, 0.0)   # Σ g_i^2\n",
        "            state[\"g_max\"] = self._init_scalar_state_like(p, grad, 0.0)   # max g_i^2\n",
        "\n",
        "            # common accumulator for Σ ||M_i||^2 / α_i\n",
        "            state[\"H_sum\"] = self._init_scalar_state_like(p, grad, 0.0)\n",
        "\n",
        "            # for AdaOrth-L+ prefix-min alpha\n",
        "            state[\"alpha\"] = self._init_scalar_state_like(p, grad, 1.0)   # alpha_0 = 1\n",
        "        else:\n",
        "            # non-2D: AdamW-like moments\n",
        "            state[\"exp_avg\"] = torch.zeros_like(p)\n",
        "            state[\"exp_avg_sq\"] = torch.zeros_like(p)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _step_non2d(self, p: torch.Tensor, grad: torch.Tensor, state: dict, *, lr: float, eps: float,\n",
        "                    weight_decay: float, beta1: float, beta2: float, gamma: float):\n",
        "        \"\"\"Non-2D parameters: corrected-input AdamW-like update (engineering choice).\"\"\"\n",
        "        step = state[\"step\"]\n",
        "        last_grad = state[\"last_grad\"]\n",
        "\n",
        "        # c_t = g_t + gamma * (beta1/(1-beta1)) * (g_t - g_{t-1})\n",
        "        c_t = (grad - last_grad).mul(gamma * (beta1 / (1.0 - beta1))).add(grad)\n",
        "\n",
        "        # unit-norm clipping to avoid rare spikes\n",
        "        c_norm = torch.norm(c_t)\n",
        "        if c_norm > 1.0:\n",
        "            c_t = c_t / c_norm\n",
        "\n",
        "        exp_avg = state[\"exp_avg\"]\n",
        "        exp_avg_sq = state[\"exp_avg_sq\"]\n",
        "\n",
        "        # EMA updates\n",
        "        exp_avg.lerp_(c_t, 1.0 - beta1)               # m_t\n",
        "        exp_avg_sq.lerp_(c_t.square(), 1.0 - beta2)   # v_t\n",
        "\n",
        "\n",
        "        g_hat = exp_avg / (eps + exp_avg_sq.sqrt())\n",
        "\n",
        "        # bias correction\n",
        "        bias_correction1 = 1.0 - beta1 ** step\n",
        "        bias_correction2 = 1.0 - beta2 ** step\n",
        "        scale = bias_correction1 / (bias_correction2 ** 0.5)\n",
        "\n",
        "        # decoupled weight decay + update\n",
        "        p.data.mul_(1.0 - lr * weight_decay)\n",
        "        p.data.add_(g_hat, alpha=-lr / scale)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self):\n",
        "        \"\"\"\n",
        "        One optimizer step. n.   \n",
        "\n",
        "        For 2D params: delegates to step_detail(...) implemented in subclasses (AdaOrth-L / AdaOrth-L+).\n",
        "        For non-2D params: uses a simple AdamW-like rule with corrected input c_t.\n",
        "\n",
        "        IMPORTANT (AdaOrth-L+):\n",
        "          state[\"last_grad\"] must store ∇f(X_{t-1}; ξ_t) computed on the SAME ξ_t as current grad ∇f(X_t; ξ_t).\n",
        "          This must be ensured by the training loop via update_last_grad().\n",
        "        \"\"\"\n",
        "        for group in self.param_groups:\n",
        "            lr = group[\"lr\"]\n",
        "            beta1, beta2 = group[\"adamw_betas\"]\n",
        "            eps = group[\"eps\"]\n",
        "            weight_decay = group[\"weight_decay\"]\n",
        "            gamma = group[\"gamma\"]\n",
        "\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                grad = p.grad\n",
        "                self._init_state(p, grad)\n",
        "\n",
        "                state = self.state[p]\n",
        "                state[\"step\"] += 1\n",
        "\n",
        "                if p.ndim == 2:\n",
        "                    eta_t, update = self.step_detail(grad, state, eps=eps, lr=lr)\n",
        "                    # engineering choice: keep decoupled weight decay for matrices as well\n",
        "                    p.data.mul_(1.0 - lr * weight_decay)\n",
        "                    p.data.add_(update, alpha=-eta_t)\n",
        "                else:\n",
        "                    self._step_non2d(\n",
        "                        p, grad, state,\n",
        "                        lr=lr, eps=eps, weight_decay=weight_decay,\n",
        "                        beta1=beta1, beta2=beta2, gamma=gamma\n",
        "                    )\n",
        "\n",
        "\n",
        "# =========================\n",
        "# AdaOrth-L (Algorithm 1)\n",
        "# =========================\n",
        "class AdaOrthL(AdaOrthBasic):\n",
        "    @torch.no_grad()\n",
        "    def step_detail(self, grad: torch.Tensor, state: dict, *, eps: float, lr: float):\n",
        "        # Paper mapping:\n",
        "        #   G_t=grad,  g_t^2=||G_t||_F^2,  α_t=(1+Σ g_i^2)^(-1/2),\n",
        "        #   M_t=(1-α_t)M_{t-1}+α_t G_t,\n",
        "        #   H_t=Σ ||M_i||^2/α_i,\n",
        "        #   γ_t=min( α_t/(eps+max g^2), (eps+H_t)^(-1/2) ),\n",
        "        #   η_t=θ γ_t ||M_t||_F (θ=lr),\n",
        "        #   update=Orth(M_t)\n",
        "        g_sq = grad.norm().pow(2)\n",
        "        state[\"g_sum\"] = state[\"g_sum\"] + g_sq\n",
        "        state[\"g_max\"] = torch.maximum(state[\"g_max\"], g_sq)\n",
        "\n",
        "        alpha = (1.0 + state[\"g_sum\"]).pow(-0.5)\n",
        "\n",
        "        M = state[\"m_t\"]\n",
        "        M.mul_(1.0 - alpha).add_(grad, alpha=alpha)\n",
        "        m_norm = M.norm()\n",
        "\n",
        "        H = state[\"H_sum\"] + (m_norm.pow(2) / (alpha + eps))\n",
        "        state[\"H_sum\"] = H\n",
        "\n",
        "        gamma1 = alpha / (eps + state[\"g_max\"])\n",
        "        gamma2 = (eps + H).pow(-0.5)\n",
        "        gamma_t = torch.minimum(gamma1, gamma2)\n",
        "\n",
        "        eta_t = lr * gamma_t * m_norm\n",
        "        update = zeropower_via_newtonschulz5(M)\n",
        "        return eta_t, update\n",
        "\n",
        "\n",
        "# =========================\n",
        "# AdaOrth-L+ (Algorithm 2)\n",
        "# =========================\n",
        "class AdaOrthLPlus(AdaOrthBasic):\n",
        "    @torch.no_grad()\n",
        "    def step_detail(self, grad: torch.Tensor, state: dict, *, eps: float, lr: float):\n",
        "        # Paper mapping:\n",
        "        #   G_t=grad,  G_{t-1}(same ξ_t)=state[\"last_grad\"],\n",
        "        #   α_t = min_{k<=t} ((eps+max g_i^2)/(eps+Σ g_i^2))^(2/3) (prefix-min recursion),\n",
        "        #   M_t=(1-α_t)(M_{t-1}-G_{t-1})+G_t,\n",
        "        #   H_t=Σ ||M_i||^2/α_i,\n",
        "        #   γ_t=min( sqrt(α_t)/(eps+max g^2), (eps+H_t)^(-1/2) ),\n",
        "        #   η_t=θ γ_t ||M_t||_F (θ=lr),\n",
        "        #   update=Orth(M_t)\n",
        "        g_sq = grad.norm().pow(2)\n",
        "        state[\"g_sum\"] = state[\"g_sum\"] + g_sq\n",
        "        state[\"g_max\"] = torch.maximum(state[\"g_max\"], g_sq)\n",
        "\n",
        "        r_t = ((eps + state[\"g_max\"]) / (eps + state[\"g_sum\"])).pow(2.0 / 3.0)\n",
        "        alpha = torch.minimum(state[\"alpha\"], r_t)   # prefix-min\n",
        "        state[\"alpha\"] = alpha\n",
        "\n",
        "        M = state[\"m_t\"]\n",
        "        M.sub_(state[\"last_grad\"]).mul_(1.0 - alpha).add_(grad)\n",
        "        m_norm = M.norm()\n",
        "\n",
        "        H = state[\"H_sum\"] + (m_norm.pow(2) / (alpha + eps))\n",
        "        state[\"H_sum\"] = H\n",
        "\n",
        "        gamma1 = alpha.sqrt() / (eps + state[\"g_max\"])\n",
        "        gamma2 = (eps + H).pow(-0.5)\n",
        "        gamma_t = torch.minimum(gamma1, gamma2)\n",
        "\n",
        "        eta_t = lr * gamma_t * m_norm\n",
        "        update = zeropower_via_newtonschulz5(M)\n",
        "        return eta_t, update"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AdaGrad-Norm Optimizer\n",
        "\n",
        "\n",
        "![alt text](https://github.com/RayCyder/Intro/blob/main/AdaGrad-Norm.jpg?raw=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class AdaGradNorm(torch.optim.Optimizer):\n",
        "    \"\"\"\n",
        "    AdaGrad-Norm (Ward et al.): \n",
        "        b_{t+1}^2 = b_t^2 + ||g_t||^2\n",
        "        x_{t+1} = x_t - (eta / b_{t+1}) * g_t\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=1.0, b0=1.0, eps=1e-12, weight_decay=0.0):\n",
        "        if b0 <= 0:\n",
        "            raise ValueError(\"b0 must be > 0\")\n",
        "        defaults = dict(lr=lr, b0=b0, eps=eps, weight_decay=weight_decay)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "        for group in self.param_groups:\n",
        "            lr = group[\"lr\"]          # eta\n",
        "            eps = group[\"eps\"]\n",
        "            weight_decay = group[\"weight_decay\"]\n",
        "            b0 = group[\"b0\"]\n",
        "\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad\n",
        "                state = p.state\n",
        "                g2 = grad.norm().pow(2)\n",
        "                # init b2 = b0^2\n",
        "                if state.get(\"b2\") is None:\n",
        "                    state[\"b2\"] = torch.tensor(b0 * b0, device=g2.device, dtype=g2.dtype)\n",
        "                b2 = g2 + state['b2']\n",
        "                state['b2'] = b2\n",
        "                eta = lr / (b2 + eps)\n",
        "                p.data.mul_(1.0 - lr * weight_decay)\n",
        "                p.data.add_(grad, alpha=-eta)\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STORM Optimizer\n",
        "\n",
        "\n",
        "![alt text](https://github.com/RayCyder/Intro/blob/main/storm+.jpg?raw=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class STORMPlus(torch.optim.Optimizer):\n",
        "    \"\"\"\n",
        "    STORM+ optimizer (AdaOrth-style usage with update_last_grad).\n",
        "\n",
        "    REQUIREMENT (strict STORM correction):\n",
        "      state[p][\"last_grad\"] must store ∇f(x_{t-1}; ξ_t) for the SAME mini-batch ξ_t\n",
        "      used to compute current grad ∇f(x_t; ξ_t) before calling step().\n",
        "\n",
        "    Core recursion (per step):\n",
        "      d_t = g_t + (1 - a_t) (d_{t-1} - last_grad)\n",
        "      x_{t+1} = x_t - eta_t * d_t\n",
        "\n",
        "    where a_t, eta_t are global adaptive scalars (per param_group):\n",
        "      a_t   = (1 + Σ ||g||^2)^(-2/3)\n",
        "      eta_t = (eps + Σ (||d||^2 / a_t))^(-1/3)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=1.0, eps=1e-8, weight_decay=0.0):\n",
        "        defaults = dict(lr=lr, eps=eps, weight_decay=weight_decay)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "        # group-level scalars (initialized lazily on first step to get device/dtype)\n",
        "        for group in self.param_groups:\n",
        "            group[\"step\"] = 0\n",
        "            group[\"g2_sum\"] = None             # Σ ||g_t||^2\n",
        "            group[\"d2_over_a_sum\"] = None      # Σ (||d_t||^2 / a_t)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update_last_grad(self):\n",
        "        \"\"\"\n",
        "        Cache current gradients into state[p][\"last_grad\"].\n",
        "\n",
        "        You should call this right after computing grad at OLD parameters x_{t-1}\n",
        "        on the SAME batch ξ_t that will later be used at x_t.\n",
        "        \"\"\"\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                st = self.state[p]\n",
        "                if \"last_grad\" not in st:\n",
        "                    st[\"last_grad\"] = torch.zeros_like(p)\n",
        "                st[\"last_grad\"].copy_(p.grad)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            lr = group[\"lr\"]\n",
        "            eps = group[\"eps\"]\n",
        "            weight_decay = group[\"weight_decay\"]\n",
        "            params = group[\"params\"]\n",
        "\n",
        "            # init per-parameter state\n",
        "            for p in params:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                state = self.state[p]\n",
        "                g_t = p.grad\n",
        "                #comput d_t use last grad and a_t-1\n",
        "                # d_t = g_t + (1-a_t)(d_{t-1} - last_grad)\n",
        "                if 'a_t' not in state: #first\n",
        "                    d_t = p.grad\n",
        "                else:\n",
        "                    #a_t actually is a_t-1\n",
        "                    d_t = g_t + (1.0 - state['a_t']) * (state['d'] - state['last_grad'])\n",
        "                state['d'] = d_t\n",
        "                # compute ||g_t||^2\n",
        "                g2 = g_t.norm().pow(2)\n",
        "                # lazy init group scalars on correct device/dtype\n",
        "                if state[\"g2_sum\"] is None:\n",
        "                    state[\"g2_sum\"] = torch.zeros_like(g2)\n",
        "                if state[\"d2_over_a_sum\"] is None:\n",
        "                    state[\"d2_over_a_sum\"] = torch.zeros_like(g2)\n",
        "                # a_t = (1 + Σ ||g||^2)^(-2/3)\n",
        "                state[\"g2_sum\"] = state[\"g2_sum\"] + g2\n",
        "                a_t = (1.0 + state[\"g2_sum\"]).pow(-2.0 / 3.0)\n",
        "                state['a_t'] = a_t #save for next step\n",
        "                #d2 = Σ (||d_t||^2 / a_t)\n",
        "                d2_over_a = state['d'].norm().pow(2) / a_t\n",
        "                state[\"d2_over_a_sum\"] = state[\"d2_over_a_sum\"] + d2_over_a\n",
        "                # eta_t = (eps + Σ (||d_t||^2 / a_t))^(-1/3)\n",
        "                eta_t  = (state[\"d2_over_a_sum\"] + eps).pow(-1.0 / 3.0)\n",
        "                # parameter update: x <- x - lr*eta_t*d_t  (lr is treated as θ-like multiplier)\n",
        "                step_size = lr * eta_t\n",
        "                # (optional) decoupled weight decay\n",
        "                p.data.mul_(1.0 - step_size * weight_decay)\n",
        "                p.data.add_(d_t, alpha=-step_size)\n",
        "\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lw1tuJ5UIcm5"
      },
      "source": [
        "## Muon-MVR2 Optimzier\n",
        "**Muon-MVR2 method from ON THE CONVERGENCE OF MUON AND BEYOND**\n",
        "\n",
        "fixed some error for mac M1\n",
        "\n",
        "mvr2 only works with 2 dim weights, other dims use method like adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "QEAqv4_vfqlD"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Muon-MVR For LLM Pretraining.\n",
        "'''\n",
        "import torch\n",
        "import math\n",
        "\n",
        "#fix for mps\n",
        "def _compile_for_device(fn):\n",
        "    # Inductor doesn't support MPS; use AOTAutograd there.\n",
        "    if torch.backends.mps.is_available():\n",
        "        return torch.compile(fn, backend=\"aot_eager\", dynamic=True)\n",
        "    # Inductor is fine on CUDA/CPU\n",
        "    return torch.compile(fn, backend=\"inductor\", dynamic=True)\n",
        "\n",
        "\n",
        "@_compile_for_device\n",
        "def zeropower_via_newtonschulz5(G, steps=3, eps=1e-7):\n",
        "    \"\"\"\n",
        "    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G.\n",
        "    \"\"\"\n",
        "    assert len(G.shape) == 2\n",
        "    a, b, c = (3.4445, -4.7750,  2.0315)\n",
        "    d_type = G.dtype\n",
        "    X = G.bfloat16()\n",
        "    #x_flat = X.reshape(-1)      # 把矩阵展平成一个 1D 向量\n",
        "    X /= (X.norm() + eps) # ensure top singular value <= 1\n",
        "    if G.size(0) > G.size(1):\n",
        "        X = X.T\n",
        "    for _ in range(steps):\n",
        "        A = X @ X.T\n",
        "        B = b * A + c * A @ A\n",
        "        X = a * X + B @ X\n",
        "    if G.size(0) > G.size(1):\n",
        "        X = X.T\n",
        "    return X.to(d_type)\n",
        "\n",
        "class MuonMVR(torch.optim.Optimizer):\n",
        "    r'''\n",
        "    Standard MVR:\\\\(nabla f(X_t;\\xi_t) - \\\\nabla f(X_{t-1};\\xi_{t})\n",
        "    Approximate MVR:\n",
        "        1.\\\\(nabla f(X_t;\\xi_t) - \\\\nabla f(X_{t-1};\\xi_{t-1})\n",
        "        2.\\\\(nabla f(X_t;\\xi_t) - \\\\nabla f(X_{t};\\xi_{t-1}), It has low computational complexity\n",
        "         and is more convenient in practice\n",
        "    '''\n",
        "    def __init__(self, params, lr=3e-3, momentum = 0.95 ,adamw_betas=(0.95, 0.99), eps=1e-8,\n",
        "                 weight_decay=0.0, gamma=0.025, is_approx=False):\n",
        "        if lr < 0.0:\n",
        "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
        "        if not 0.0 <= momentum < 1.0:\n",
        "            raise ValueError(f\"Invalid momentum parameter: {momentum}\")\n",
        "        if not 0.0 <= adamw_betas[0] < 1.0:\n",
        "            raise ValueError(f\"Invalid beta1 parameter: {adamw_betas[0]}\")\n",
        "        if not 0.0 <= adamw_betas[1] < 1.0:\n",
        "            raise ValueError(f\"Invalid beta2 parameter: {adamw_betas[1]}\")\n",
        "\n",
        "        defaults = dict(lr=lr, momentum=momentum,adamw_betas=adamw_betas, eps=eps,\n",
        "                       weight_decay=weight_decay, gamma=gamma)\n",
        "        super().__init__(params, defaults)\n",
        "        self.is_approx = is_approx\n",
        "\n",
        "    def adjust_lr_for_muon(self, lr, param_shape):\n",
        "        A, B = param_shape[:2]\n",
        "        # We adjust the learning rate and weight decay based on the size of the parameter matrix\n",
        "        # as describted in the paper\n",
        "        adjusted_ratio = 0.2 * math.sqrt(max(A, B))\n",
        "        # adjusted_ratio = math.sqrt(A*B)\n",
        "        adjusted_lr = lr * adjusted_ratio\n",
        "        return adjusted_lr\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update_last_grad(self):\n",
        "        if not self.is_approx:\n",
        "            for group in self.param_groups:\n",
        "                for p in group['params']:\n",
        "                    state = self.state[p]\n",
        "                    if \"last_grad\" not in state:\n",
        "                        state[\"last_grad\"] = torch.zeros_like(p)\n",
        "                    state[\"last_grad\"].zero_().add_(p.grad, alpha=1.0)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self):\n",
        "        for group in self.param_groups:\n",
        "            lr = group['lr']\n",
        "            beta1, beta2 = group['adamw_betas']\n",
        "            eps = group['eps']\n",
        "            weight_decay = group['weight_decay']\n",
        "            momentum = group['momentum']\n",
        "            gamma = group['gamma']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                grad = p.grad\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p)\n",
        "                    state['last_grad'] = torch.zeros_like(p)\n",
        "                    # state['previous_grad'] = torch.zeros_like(p)\n",
        "                    if len(p.shape) != 2:  # Only for 2D tensors\n",
        "                        state['exp_avg_sq'] = torch.zeros_like(p)\n",
        "\n",
        "                state['step'] += 1\n",
        "                last_grad = state['last_grad']\n",
        "                if len(p.shape) == 2:\n",
        "                    exp_avg = state['exp_avg']\n",
        "\n",
        "                    # Compute momentum-like term with correction\n",
        "                    c_t = (grad - last_grad).mul(gamma * (momentum / (1. - momentum))).add(grad)\n",
        "                    c_t_norm = torch.norm(c_t)\n",
        "                    if c_t_norm > 1.:\n",
        "                        c_t = c_t / c_t_norm\n",
        "                    # Update moving averages\n",
        "                    exp_avg.mul_(momentum).add_(c_t, alpha=1 - momentum)\n",
        "                    update = zeropower_via_newtonschulz5(exp_avg.mul(1./(1.- momentum))) # whiten the update\n",
        "                    adjusted_lr = self.adjust_lr_for_muon(lr, p.shape)\n",
        "                    p.data.mul_(1 - lr * weight_decay)\n",
        "                    p.data.add_(update, alpha=-adjusted_lr)\n",
        "                else:\n",
        "                    # For bias vectors - use simple update\n",
        "                    step = state['step']\n",
        "                    # Compute momentum-like term with correction\n",
        "                    c_t = (grad - last_grad).mul(gamma * (beta1 / (1. - beta1))).add(grad)\n",
        "                    c_t_norm = torch.norm(c_t)\n",
        "                    # avoid inductor lowering bug: compute norm explicitly and detach\n",
        "                    # c_t_norm = torch.sqrt(torch.sum((c_t.detach() * c_t.detach()), dim=None))\n",
        "                    if c_t_norm > 1.:\n",
        "                        c_t = c_t / c_t_norm\n",
        "                    exp_avg = state['exp_avg']\n",
        "                    exp_avg_sq = state['exp_avg_sq']\n",
        "                    exp_avg.lerp_(c_t, 1 - beta1)\n",
        "                    exp_avg_sq.lerp_(c_t.square(), 1 - beta2)\n",
        "                    g = exp_avg / (eps + exp_avg_sq.sqrt())\n",
        "                    bias_correction1 = 1 - beta1**step\n",
        "                    bias_correction2 = 1 - beta2**step\n",
        "                    scale = bias_correction1 / bias_correction2**0.5\n",
        "                    p.data.mul_(1 - lr * weight_decay)\n",
        "                    p.data.add_(g, alpha=-lr / scale)\n",
        "\n",
        "                if self.is_approx:\n",
        "                    state['last_grad'].copy_(grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dMWiYJf1Ou-"
      },
      "source": [
        "#**MuonMVR Training Loop**\n",
        "Here we train the architecture on training data and check its validation loss by using the validation set and saving the model only if there is an improvement ie decrease in the validation loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bFAzI_3Vwhj"
      },
      "source": [
        "### MVR2 Usage\n",
        "3.  **Optimizer Modes**\n",
        "    MuonMVR can be initialized in different modes to trade off between precision and computational cost.\n",
        "\n",
        "    **Exact Variance Reduction (`is_approx=False`)**\n",
        "    To achieve the most precise variance reduction, you must manually manage the model state. Before calculating the gradient for the previous batch, you need to load the model state from the previous iteration. This ensures that the gradient is computed with the correct model weights.\n",
        "    ```python\n",
        "    optimizer = MuonMVR(model.parameters(), lr=1e-3, is_approx=False)\n",
        "    old_state_dict = {}\n",
        "    \n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        # Store the current model state\n",
        "        cur_state_dict = {k: v.data.clone() for k, v in net.state_dict().items()}\n",
        "    \n",
        "        if old_state_dict:\n",
        "            # Load the previous model state to compute the old gradient\n",
        "            net.load_state_dict(old_state_dict)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.update_last_grad()\n",
        "    \n",
        "        # Restore the current model state to compute the new gradient\n",
        "        net.load_state_dict(cur_state_dict)\n",
        "        old_state_dict = {k: v.data.clone() for k, v in cur_state_dict.items()}\n",
        "        \n",
        "        # Standard forward/backward pass and step\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimizer with Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TwoStepOptimizer(BaseOptimizer):\n",
        "  def __init__(self,model,criterion, epochs, learning_rate):\n",
        "    super().__init__(model,criterion, epochs, learning_rate)\n",
        "    self.old_state_dict = {}\n",
        "  def initialize_optimizer(self):\n",
        "    learning_rate = self.learning_rate\n",
        "    optimizer = self.get_optimizer()\n",
        "    # Cosine annealing: lr → eta_min by the end\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer,\n",
        "        T_max=self.epochs,   # period (usually = total epochs)\n",
        "        eta_min=0.1*learning_rate      # minimum LR (here 0.1× of 0.01)\n",
        "    )\n",
        "    self.optimizer,self.scheduler =  optimizer, scheduler\n",
        "    \n",
        "\n",
        "  def zero_grad(self,inputs=None,labels=None):\n",
        "    model, old_state_dict = self.model, self.old_state_dict\n",
        "    optimizer = self.optimizer\n",
        "    # Store the current model state\n",
        "    cur_state_dict = {k: v.data.clone() for k, v in model.state_dict().items()}\n",
        "    if old_state_dict:\n",
        "        # Load the previous model state to compute the old gradient\n",
        "        model.load_state_dict(old_state_dict)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = self.criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.update_last_grad()\n",
        "    #restore state\n",
        "    model.load_state_dict(cur_state_dict)\n",
        "    self.old_state_dict = {k: v.data.clone() for k, v in cur_state_dict.items()}\n",
        "    self.optimizer.zero_grad()\n",
        "\n",
        "  def get_optimizer(self):\n",
        "    raise NotImplementedError\n",
        "  \n",
        "class OneStepOptimizer(BaseOptimizer):\n",
        "  def __init__(self,model,criterion,epochs,learning_rate):\n",
        "    super().__init__(model,criterion,epochs, learning_rate)\n",
        "  def initialize_optimizer(self):\n",
        "    learning_rate = self.learning_rate\n",
        "    optimizer = self.get_optimizer()\n",
        "    # Cosine annealing: lr → eta_min by the end\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer,\n",
        "        T_max=self.epochs,   # period (usually = total epochs)\n",
        "        eta_min=0.1*learning_rate      # minimum LR (here 0.1× of 0.01)\n",
        "    )\n",
        "    self.optimizer,self.scheduler =  optimizer, scheduler\n",
        "\n",
        "  def get_optimizer(self):\n",
        "    raise NotImplementedError\n",
        "\n",
        "class AdaGradNormOptimizer(OneStepOptimizer):\n",
        "  def get_optimizer(self):\n",
        "    learning_rate = self.learning_rate\n",
        "    optimizer = AdaGradNorm(self.model.parameters(), lr=learning_rate, b0=1.0, eps=1e-12, weight_decay=global_config.weight_decay)\n",
        "    return optimizer\n",
        "class STORMPlusOptimizer(TwoStepOptimizer):\n",
        "  def get_optimizer(self):\n",
        "    learning_rate = self.learning_rate\n",
        "    optimizer = STORMPlus(self.model.parameters(), lr=learning_rate, eps=1e-8, weight_decay=global_config.weight_decay)\n",
        "    return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "XiicKSbbint0"
      },
      "outputs": [],
      "source": [
        "class MuonMVR2Optimizer(BaseOptimizer):\n",
        "  def __init__(self,model,criterion, epochs, learning_rate):\n",
        "    super().__init__(model,criterion, epochs, learning_rate)\n",
        "    self.old_state_dict = {}\n",
        "  def initialize_optimizer(self):\n",
        "    learning_rate = self.learning_rate\n",
        "    optimizer = MuonMVR(self.model.parameters(), lr=learning_rate,weight_decay=global_config.weight_decay,gamma=0.1,is_approx=False)\n",
        "    # Cosine annealing: lr → eta_min by the end\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer,\n",
        "        T_max=self.epochs,   # period (usually = total epochs)\n",
        "        eta_min=0.1*learning_rate      # minimum LR (here 0.1× of 0.01)\n",
        "    )\n",
        "    self.optimizer,self.scheduler =  optimizer, scheduler\n",
        "\n",
        "  def zero_grad(self,inputs=None,labels=None):\n",
        "    model, old_state_dict = self.model, self.old_state_dict\n",
        "    optimizer = self.optimizer\n",
        "    # Store the current model state\n",
        "    cur_state_dict = {k: v.data.clone() for k, v in model.state_dict().items()}\n",
        "    if old_state_dict:\n",
        "        # Load the previous model state to compute the old gradient\n",
        "        model.load_state_dict(old_state_dict)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = self.criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.update_last_grad()\n",
        "    #restore state\n",
        "    model.load_state_dict(cur_state_dict)\n",
        "    self.old_state_dict = {k: v.data.clone() for k, v in cur_state_dict.items()}\n",
        "    self.optimizer.zero_grad()\n",
        "\n",
        "\n",
        "class SGDOptimizer(BaseOptimizer):\n",
        "  def __init__(self,model,criterion,epochs,learning_rate):\n",
        "    super().__init__(model,criterion,epochs, learning_rate)\n",
        "  def initialize_optimizer(self):\n",
        "    learning_rate = self.learning_rate\n",
        "    optimizer = optim.SGD(self.model.parameters(), lr=learning_rate, weight_decay=global_config.weight_decay,momentum=0.9)\n",
        "    # Cosine annealing: lr → eta_min by the end\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer,\n",
        "        T_max=self.epochs,   # period (usually = total epochs)\n",
        "        eta_min=0.1*learning_rate      # minimum LR (here 0.1× of 0.01)\n",
        "    )\n",
        "    self.optimizer,self.scheduler =  optimizer, scheduler\n",
        "\n",
        "\n",
        "class AdamOptimizer(BaseOptimizer):\n",
        "  def __init__(self,model,criterion,epochs,learning_rate):\n",
        "    super().__init__(model,criterion,epochs, learning_rate)\n",
        "  def initialize_optimizer(self):\n",
        "    learning_rate = self.learning_rate\n",
        "    optimizer = optim.Adam(self.model.parameters(), lr=learning_rate, weight_decay=global_config.weight_decay)\n",
        "    # Cosine annealing: lr → eta_min by the end\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer,\n",
        "        T_max=self.epochs,   # period (usually = total epochs)\n",
        "        eta_min=0.1*learning_rate      # minimum LR (here 0.1× of 0.01)\n",
        "    )\n",
        "    self.optimizer,self.scheduler =  optimizer, scheduler\n",
        "\n",
        "\n",
        "class AdaOrthLOptimizer(BaseOptimizer):\n",
        "  def __init__(self,model,criterion,epochs,learning_rate):\n",
        "    super().__init__(model,criterion,epochs, learning_rate)\n",
        "    self.old_state_dict = {}\n",
        "  def initialize_optimizer(self):\n",
        "    learning_rate = self.learning_rate\n",
        "    optimizer = AdaOrthL(self.model.parameters(), lr=learning_rate, weight_decay=global_config.weight_decay, gamma=0.1)\n",
        "    # Cosine annealing: lr → eta_min by the end\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer,\n",
        "        T_max=self.epochs,   # period (usually = total epochs)\n",
        "        eta_min=0.1*learning_rate      # minimum LR (here 0.1× of 0.01)\n",
        "    )\n",
        "    self.optimizer,self.scheduler =  optimizer, scheduler\n",
        "  def zero_grad(self,inputs=None,labels=None):\n",
        "    model, old_state_dict = self.model, self.old_state_dict\n",
        "    optimizer = self.optimizer\n",
        "    # Store the current model state\n",
        "    cur_state_dict = {k: v.data.clone() for k, v in model.state_dict().items()}\n",
        "    if self.old_state_dict:\n",
        "        # Load the previous model state to compute the old gradient\n",
        "        model.load_state_dict(old_state_dict)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = self.criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.update_last_grad()\n",
        "    #restore state\n",
        "    model.load_state_dict(cur_state_dict)\n",
        "    self.old_state_dict = {k: v.data.clone() for k, v in cur_state_dict.items()}\n",
        "    self.optimizer.zero_grad()\n",
        "\n",
        "\n",
        "class AdaOrthLPLusOptimizer(BaseOptimizer):\n",
        "  def __init__(self,model,criterion,epochs,learning_rate):\n",
        "    super().__init__(model,criterion,epochs, learning_rate)\n",
        "  def initialize_optimizer(self):\n",
        "    #weight_decay=0.01,gamma=0.1\n",
        "    learning_rate = self.learning_rate\n",
        "    optimizer = AdaOrthLPlus(self.model.parameters(), lr=learning_rate, weight_decay=global_config.weight_decay,gamma=0.1)\n",
        "    # Cosine annealing: lr → eta_min by the end\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer,\n",
        "        T_max=self.epochs,   # period (usually = total epochs)\n",
        "        eta_min=0.1*learning_rate      # minimum LR (here 0.1× of 0.01)\n",
        "    )\n",
        "    self.optimizer,self.scheduler =  optimizer, scheduler\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvhesdndign7"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGej2WdOVHEs"
      },
      "source": [
        "## **grid search learning rate**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "7HvRH5HvVE_X"
      },
      "outputs": [],
      "source": [
        "# learning_rates = [1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2, 1e-1]\n",
        "# def grids_search_val(get_optimizer):\n",
        "#   result = []\n",
        "#   for ii, learning_rate in enumerate(learning_rates):\n",
        "#     print(f\".........Learning rate {ii+1}/{len(learning_rates)}: {learning_rate}............\")\n",
        "#     model = initialize_model(device)\n",
        "#     optimizer = get_optimizer(model, learning_rate)\n",
        "#     train_losses, train_accs, val_losses, val_accs, time_cost = train_once(model,criterion,optimizer,train_loader,val_loader,learning_rate,epochs)\n",
        "#     result.append(min(val_losses))\n",
        "#   return result\n",
        "\n",
        "# import copy\n",
        "# import numpy as np\n",
        "import copy\n",
        "def grid_search_val_no_earlystop(optimizer_class, learning_rates, train_object_val):\n",
        "    # 固定初始模型权重，保证不同lr可比（不动seed也行）\n",
        "    all_results = {\n",
        "        \"best_val_loss\": [],# 每个lr的 final val loss\n",
        "        \"best_train_loss\": [],\n",
        "        \"best_val_acc\": [],\n",
        "        \"best_train_acc\": [],\n",
        "        \"time_cost\": []\n",
        "    }\n",
        "    for ii, lr in enumerate(learning_rates):\n",
        "        print(f\".........Learning rate {ii+1}/{len(learning_rates)}: {lr}............\")\n",
        "        train_object_val.initialize_model()\n",
        "        optimizer = optimizer_class(train_object_val.model,train_object_val.criterion,train_object_val.epochs, lr)\n",
        "        optimizer.initialize_optimizer()\n",
        "        optimizer.scheduler = None #search using no scheduler\n",
        "        train_losses, train_accs, val_losses, val_accs, time_cost  = train_object_val.train(optimizer, need_save=False)\n",
        "        #final_val_loss = float(val_losses[-1])   # ✅ 不用early stop\n",
        "        best_val_loss = min(val_losses)\n",
        "        best_train_loss = min(train_losses)\n",
        "        best_val_acc = max(val_accs)\n",
        "        best_train_acc = max(train_accs)\n",
        "        # results.append(best_val_loss)\n",
        "        all_results[\"best_val_loss\"].append(best_val_loss)\n",
        "        all_results[\"best_train_loss\"].append(best_train_loss)\n",
        "        all_results[\"best_val_acc\"].append(best_val_acc)\n",
        "        all_results[\"best_train_acc\"].append(best_train_acc)\n",
        "        all_results[\"time_cost\"].append(time_cost)\n",
        "\n",
        "    best_i = int(np.argmin(all_results[\"best_val_loss\"]))\n",
        "    best_lr = learning_rates[best_i]\n",
        "    print(f\"\\n✅ Best lr by final val loss: {best_lr}, final val loss={all_results['best_val_loss'][best_i]:.6f}\")\n",
        "\n",
        "    return best_lr, all_results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "def _to_float_list(xs):\n",
        "    return [float(x) for x in xs]\n",
        "\n",
        "def save_grid_result(path: Path, payload: dict):\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with path.open(\"w\") as f:\n",
        "        json.dump(payload, f, indent=2, sort_keys=True)\n",
        "\n",
        "def load_grid_result(path: Path):\n",
        "    with path.open(\"r\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def grid_cache_path(cache_dir: str, optimizer_name: str, random_seed: int, tag: str = \"\") -> Path:\n",
        "    # tag can include model/dataset info, e.g., \"resnet18_cifar10\"\n",
        "    tag_part = f\"_{tag}\" if tag else \"\"\n",
        "    return Path(cache_dir) / f\"grid_{optimizer_name}{tag_part}_seed{random_seed}.json\"\n",
        "\n",
        "def get_or_run_grid_search(\n",
        "    optimizer_name: str,\n",
        "    opt_fn,\n",
        "    random_seed: int,\n",
        "    learning_rates,\n",
        "    train_object: TrainModel,\n",
        "    epochs: int,\n",
        "    cache_dir: str = \"cache_grid\",\n",
        "    tag: str = \"resnet18_cifar10\",\n",
        "    force_rerun: bool = False,\n",
        "):\n",
        "    \"\"\"\n",
        "    Returns: best_lr (float), all_results (dict)\n",
        "    Assumes grid_search_val_no_earlystop(opt_fn) returns (best_lr, losses, all_results).\n",
        "    \"\"\"\n",
        "    path = grid_cache_path(cache_dir, optimizer_name, random_seed, tag)\n",
        "\n",
        "    # metadata to ensure \"same run\"\n",
        "    meta = {\n",
        "        \"optimizer\": optimizer_name,\n",
        "        \"random_seed\": int(random_seed),\n",
        "        \"learning_rates\": _to_float_list(learning_rates),\n",
        "        \"epochs\": int(epochs),\n",
        "        \"tag\": tag,\n",
        "    }\n",
        "\n",
        "    if (not force_rerun) and path.exists():\n",
        "        cached = load_grid_result(path)\n",
        "\n",
        "        # basic compatibility check\n",
        "        if cached.get(\"meta\") == meta:\n",
        "            print(f\"[CACHE HIT] {optimizer_name} seed={random_seed} -> {path}\")\n",
        "            return float(cached[\"best_lr\"]) , cached[\"all_results\"]\n",
        "        else:\n",
        "            print(f\"[CACHE MISMATCH] {optimizer_name} seed={random_seed} -> rerun\\n\"\n",
        "                  f\"  cached meta: {cached.get('meta')}\\n\"\n",
        "                  f\"  current meta: {meta}\")\n",
        "\n",
        "    # ---- run grid search ----\n",
        "    print(f\"[CACHE MISS] running grid search for {optimizer_name}, seed={random_seed}\")\n",
        "    # IMPORTANT: you must set seeds BEFORE calling grid_search_val_no_earlystop\n",
        "    # set_seed(random_seed)  # <- your seed function here\n",
        "\n",
        "    best_lr, all_results = grid_search_val_no_earlystop(opt_fn,learning_rates,train_object)\n",
        "\n",
        "    payload = {\n",
        "        \"meta\": meta,\n",
        "        \"best_lr\": float(best_lr),\n",
        "        \"all_results\": all_results,\n",
        "        \"save_at\": datetime.now().isoformat(),\n",
        "    }\n",
        "    save_grid_result(path, payload)\n",
        "    print(f\"[SAVED] {optimizer_name} seed={random_seed} -> {path}\")\n",
        "\n",
        "    return float(best_lr), all_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "OptimizerConfig.get_optimizers() missing 1 required positional argument: 'self'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# optimizers = {\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m#     # \"Muon-MVR2\": MuonMVR2Optimizer,\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m#     \"SGD\": SGDOptimizer,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m \u001b[38;5;66;03m#     \"STORM+\": STORMPlusOptimizer,\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# }\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m optimizers = \u001b[43mOptimizerConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_optimizers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# 运行 grid search，保存结果\u001b[39;00m\n\u001b[32m     15\u001b[39m results = {}\n",
            "\u001b[31mTypeError\u001b[39m: OptimizerConfig.get_optimizers() missing 1 required positional argument: 'self'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# optimizers = {\n",
        "#     # \"Muon-MVR2\": MuonMVR2Optimizer,\n",
        "#     \"SGD\": SGDOptimizer,\n",
        "#     \"ADAM\": AdamOptimizer,\n",
        "#     \"AdaOrthL\": AdaOrthLOptimizer,\n",
        "#     \"AdaOrthL+\": AdaOrthLPLusOptimizer,\n",
        "#     \"AdaGradNorm\": AdaGradNormOptimizer,\n",
        "#     \"STORM+\": STORMPlusOptimizer,\n",
        "# }\n",
        "optimizers = OptimizerConfig.get_optimizers()\n",
        "# 运行 grid search，保存结果\n",
        "results = {}\n",
        "best = {}\n",
        "train_object_val = TrainModel(train_loader, val_loader, classes, epochs=global_config.val_epochs)#5 for test\n",
        "learning_rates = [1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2, 1e-1]\n",
        "\n",
        "for name, opt_fn in optimizers.items():\n",
        "    best_lr, all_results = get_or_run_grid_search(\n",
        "    optimizer_name=name,\n",
        "    opt_fn=opt_fn,\n",
        "    random_seed=global_config.random_seed,\n",
        "    learning_rates=learning_rates,\n",
        "    epochs=train_object_val.epochs,\n",
        "    cache_dir=\"cache_grid\",\n",
        "    tag=\"resnet18_cifar10\",\n",
        "    train_object = train_object_val\n",
        ")\n",
        "    results[name] = all_results\n",
        "    best[name] = (best_lr, float(np.min(all_results[\"best_val_loss\"]))) #use val loss to choose best lr\n",
        "    print(f\"[{name}] best lr = {best_lr}, min val loss = {best[name][1]:.6f}\")\n",
        "\n",
        "# 画一张总图\n",
        "# plt.figure(figsize=(7, 4.5))\n",
        "# plt.xscale(\"log\")\n",
        "# plt.xlabel(\"Learning rate\")\n",
        "# plt.ylabel(\"Min Validation Loss\")\n",
        "# plt.title(\"Learning Rate Grid Search (All Optimizers)\")\n",
        "# plt.xticks(learning_rates, labels=[str(lr) for lr in learning_rates], rotation=45)\n",
        "\n",
        "# for name, losses in results.items():\n",
        "#     plt.plot(learning_rates, losses, marker=\"o\", linewidth=2, label=name)\n",
        "\n",
        "# plt.legend()\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_lr_sweep_2x2_by_method(all_result, learning_rates, colors=None,\n",
        "                                save_path=\"lr_sweep_2x2.png\",\n",
        "                                x_log=True, sort_by_lr=True):\n",
        "    \"\"\"\n",
        "    all_result: dict(method -> dict(metric -> list over learning_rates))\n",
        "      required metrics:\n",
        "        best_train_loss, best_val_loss, best_train_acc, best_val_acc\n",
        "    learning_rates: list of floats, same length as each metric list\n",
        "    colors: dict(method -> color) optional\n",
        "    \"\"\"\n",
        "    required_metrics = [\"best_train_loss\", \"best_val_loss\", \"best_train_acc\", \"best_val_acc\"]\n",
        "\n",
        "    methods = list(all_result.keys())\n",
        "    if not methods:\n",
        "        raise ValueError(\"all_result is empty.\")\n",
        "\n",
        "    # Validate lengths\n",
        "    lrs = np.asarray(learning_rates, dtype=float)\n",
        "    n = len(lrs)\n",
        "\n",
        "    for m in methods:\n",
        "        if not isinstance(all_result[m], dict):\n",
        "            raise TypeError(f\"all_result[{m}] must be a dict of metrics.\")\n",
        "        for k in required_metrics:\n",
        "            if k not in all_result[m]:\n",
        "                raise KeyError(f\"Missing metric '{k}' for method '{m}'.\")\n",
        "            if len(all_result[m][k]) != n:\n",
        "                raise ValueError(f\"Length mismatch: method '{m}' metric '{k}' has {len(all_result[m][k])}, expected {n}.\")\n",
        "\n",
        "    # Sort by lr for nicer curves\n",
        "    if sort_by_lr:\n",
        "        order = np.argsort(lrs)\n",
        "        lrs_sorted = lrs[order]\n",
        "    else:\n",
        "        order = None\n",
        "        lrs_sorted = lrs\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(12, 8), sharex=True)\n",
        "    ax_tl, ax_vl = axes[0, 0], axes[0, 1]\n",
        "    ax_ta, ax_va = axes[1, 0], axes[1, 1]\n",
        "\n",
        "    panels = [\n",
        "        (ax_tl, \"best_train_loss\", \"Base Learning Rate vs Minimum Train Loss\", \"Min Train Loss\"),\n",
        "        (ax_vl, \"best_val_loss\",   \"Base Learning Rate vs Minimum Test Loss\",  \"Min Test Loss\"),\n",
        "        (ax_ta, \"best_train_acc\",  \"Base Learning Rate vs Maximum Train Accuracy\", \"Max Train Accuracy\"),\n",
        "        (ax_va, \"best_val_acc\",    \"Base Learning Rate vs Maximum Test Accuracy\",  \"Max Test Accuracy\"),\n",
        "    ]\n",
        "\n",
        "    for ax, metric, title, ylabel in panels:\n",
        "        for m in methods:\n",
        "            y = np.asarray(all_result[m][metric], dtype=float)\n",
        "            if order is not None:\n",
        "                y = y[order]\n",
        "            c = colors.get(m, None) if colors else None\n",
        "            ax.plot(lrs_sorted, y, marker=\"o\", linewidth=2, color=c, label=m)\n",
        "\n",
        "        ax.set_title(title)\n",
        "        ax.set_ylabel(ylabel)\n",
        "        ax.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5, alpha=0.4)\n",
        "        if x_log:\n",
        "            ax.set_xscale(\"log\")\n",
        "\n",
        "        # legend per subplot (compact)\n",
        "        ax.legend(loc=\"best\", fontsize=8, frameon=True, ncol=1,\n",
        "                  borderaxespad=0.25, handlelength=1.8, handletextpad=0.5, labelspacing=0.25)\n",
        "\n",
        "    for ax in axes[1, :]:\n",
        "        ax.set_xlabel(\"Base Learning Rate\")\n",
        "\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(save_path, dpi=300)\n",
        "    plt.show()\n",
        "    plt.close(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_lr_sweep_2x2_by_method(\n",
        "    all_result=results,\n",
        "    learning_rates=learning_rates,\n",
        "    colors=OptimizerConfig.colors,\n",
        "    save_path=\"lr_sweep_2x2.png\",\n",
        "    x_log=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vMhznmaIcm6"
      },
      "source": [
        "## **training Model  using best learning rate**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRLLuJ7xIcm6"
      },
      "outputs": [],
      "source": [
        "train_object = TrainModel(train_loader, test_loader, classes, epochs=50)#10 for test should be 100\n",
        "run_results = {}\n",
        "# for name, (best_lr, _) in best.items():\n",
        "for name in optimizers.keys():\n",
        "    best_lr = best[name][0]\n",
        "    print(f\"Training with best lr for {name}: {best_lr}\")\n",
        "    model = train_object.initialize_model()\n",
        "    optimizer = optimizers[name](model,train_object.criterion,train_object.epochs, best_lr)\n",
        "    optimizer.initialize_optimizer()\n",
        "    train_losses, train_accs, test_losses, test_accs, time_costs = train_object.train(optimizer, need_save=True)\n",
        "    run_results[name] = {\n",
        "        \"train_losses\": train_losses,\n",
        "        \"train_accs\": train_accs,\n",
        "        \"test_losses\": test_losses,\n",
        "        \"test_accs\": test_accs,\n",
        "        \"time_curve\": time_costs,\n",
        "    }\n",
        "    print(f\"{name} Test Accuracy: {test_accs[-1]:.2f}%, Time Cost: {time_costs[-1]:.2f}s\")\n",
        "# optimizer = muonmvr2_optimizer(model_mvr2, learning_rate)\n",
        "# losses_mvr2, mvr2_acc, losses_mvr2_val, mvr2_acc_val, mvr2_time_cost = train_once(model_mvr2,criterion,optimizer,train_loader,test_loader,learning_rate,epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4j1G8AgDIcnE"
      },
      "source": [
        "#**Plot Training Loss and Val Loss**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "def to_jsonable(x):\n",
        "    \"\"\"Convert common ML objects to JSON-serializable Python types.\"\"\"\n",
        "    try:\n",
        "        import numpy as np\n",
        "        if isinstance(x, np.ndarray):\n",
        "            return x.astype(float).tolist()\n",
        "        if isinstance(x, (np.floating,)):\n",
        "            return float(x)\n",
        "        if isinstance(x, (np.integer,)):\n",
        "            return int(x)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        import torch\n",
        "        if torch.is_tensor(x):\n",
        "            x = x.detach().cpu()\n",
        "            return x.tolist() if x.ndim > 0 else float(x.item())\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    if isinstance(x, (float, int, str, bool)) or x is None:\n",
        "        return x\n",
        "    if isinstance(x, (list, tuple)):\n",
        "        return [to_jsonable(v) for v in x]\n",
        "    if isinstance(x, dict):\n",
        "        return {str(k): to_jsonable(v) for k, v in x.items()}\n",
        "\n",
        "    # last resort: stringify\n",
        "    return str(x)\n",
        "\n",
        "def save_run_results(run_results: dict, filepath: str, meta: dict | None = None):\n",
        "    payload = {\n",
        "        \"meta\": to_jsonable(meta or {}),\n",
        "        \"run_results\": to_jsonable(run_results),\n",
        "    }\n",
        "    path = Path(filepath)\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(payload, f, indent=2, sort_keys=True)\n",
        "    print(f\"[SAVED] {path}\")\n",
        "\n",
        "def load_run_results(filepath: str):\n",
        "    path = Path(filepath)\n",
        "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        payload = json.load(f)\n",
        "    return payload[\"run_results\"], payload.get(\"meta\", {})\n",
        "meta = {\n",
        "    \"dataset\": \"CIFAR10\",\n",
        "    \"model\": \"ResNet18\",\n",
        "    \"epochs\": train_object.epochs,\n",
        "    \"learning_rates\": learning_rates,\n",
        "    \"best_lrs\": {k: v[0] for k, v in best.items()},  # if best[name] = (best_lr, best_loss)\n",
        "    \"random_seed\": global_config.random_seed,\n",
        "    \"save_time\": datetime.now().isoformat(),\n",
        "}\n",
        "\n",
        "save_run_results(run_results, f\"run_results_seed{global_config.random_seed}.json\", meta=meta)\n",
        "# run_results2, meta2 = load_run_results(f\"run_results_seed{global_config.random_seed}.json\")\n",
        "# print(meta2[\"best_lrs\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# optimizers = {\n",
        "#     \"Muon-MVR2\": MuonMVR2Optimizer,\n",
        "#     \"SGD\": SGDOptimizer,\n",
        "#     \"Adam\": AdamOptimizer,\n",
        "#     \"AdaOrth\": AdaOrthOptimizer,\n",
        "#     \"AdaOrthLMinus\": AdaOrthLMinusOptimizer,\n",
        "# }\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_metrics_2x2(run_results, epochs, colors, save_path=\"metrics.png\",\n",
        "                     x_log=False, y_log_loss=False, y_log_acc=False,\n",
        "                     figsize=(12, 8), lw=2):\n",
        "    epochs_range = list(range(1, epochs + 1))\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=figsize, sharex=True, sharey=False)\n",
        "    ax_tr_loss, ax_te_loss = axes[0, 0], axes[0, 1]\n",
        "    ax_tr_acc,  ax_te_acc  = axes[1, 0], axes[1, 1]\n",
        "\n",
        "    # ---- plot curves ----\n",
        "    for name, res in run_results.items():\n",
        "        c = colors.get(name, None)\n",
        "\n",
        "        ax_tr_loss.plot(epochs_range, res[\"train_losses\"], linewidth=lw, color=c, label=name)\n",
        "        ax_te_loss.plot(epochs_range, res[\"test_losses\"],  linewidth=lw, color=c, linestyle=\"--\", label=name)\n",
        "\n",
        "        ax_tr_acc.plot(epochs_range, res[\"train_accs\"], linewidth=lw, color=c, label=name)\n",
        "        ax_te_acc.plot(epochs_range, res[\"test_accs\"],  linewidth=lw, color=c, linestyle=\"--\", label=name)\n",
        "\n",
        "    # ---- titles ----\n",
        "    ax_tr_loss.set_title(\"Train Loss vs Epochs\")\n",
        "    ax_te_loss.set_title(\"Test Loss vs Epochs\")\n",
        "    ax_tr_acc.set_title(\"Train Accuracy vs Epochs\")\n",
        "    ax_te_acc.set_title(\"Test Accuracy vs Epochs\")\n",
        "\n",
        "    # ---- axes labels ----\n",
        "    for ax in axes[0, :]:\n",
        "        ax.set_ylabel(\"Loss\")\n",
        "    for ax in axes[1, :]:\n",
        "        ax.set_ylabel(\"Accuracy\")\n",
        "    for ax in axes[1, :]:\n",
        "        ax.set_xlabel(\"Epochs\")  # only bottom row needs x label\n",
        "\n",
        "    # ---- shared styling ----\n",
        "    for ax in axes.ravel():\n",
        "        ax.set_xlim(1, epochs)\n",
        "        ax.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5, alpha=0.35)\n",
        "        if x_log:\n",
        "            ax.set_xscale(\"log\")\n",
        "\n",
        "    if y_log_loss:\n",
        "        ax_tr_loss.set_yscale(\"log\")\n",
        "        ax_te_loss.set_yscale(\"log\")\n",
        "    if y_log_acc:\n",
        "        ax_tr_acc.set_yscale(\"log\")\n",
        "        ax_te_acc.set_yscale(\"log\")\n",
        "\n",
        "    # ---- legends (each subplot) ----\n",
        "    legend_kwargs = dict(\n",
        "        fontsize=8, frameon=True, ncol=1,\n",
        "        borderaxespad=0.25, handlelength=1.8, handletextpad=0.5, labelspacing=0.25\n",
        "    )\n",
        "    ax_tr_loss.legend(loc=\"upper right\", **legend_kwargs)\n",
        "    ax_te_loss.legend(loc=\"upper right\", **legend_kwargs)\n",
        "    ax_tr_acc.legend(loc=\"lower right\", **legend_kwargs)\n",
        "    ax_te_acc.legend(loc=\"lower right\", **legend_kwargs)\n",
        "\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(save_path, dpi=300)\n",
        "    plt.show()\n",
        "    plt.close(fig)\n",
        "\n",
        "def plot_acc_vs_time(run_results, colors, save_path=\"acc_time.png\"):\n",
        "    \"\"\"\n",
        "    Requires res[\"time_curve\"] as list[float] length=epochs.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(7, 4.5))\n",
        "\n",
        "    for name, res in run_results.items():\n",
        "        c = colors.get(name, None)\n",
        "        if isinstance(res.get(\"time_curve\", None), (list, tuple)) and len(res[\"time_curve\"]) == len(res[\"test_accs\"]):\n",
        "            plt.plot(res[\"time_curve\"], res[\"test_accs\"], label=name, linewidth=2, color=c)\n",
        "        else:\n",
        "            # fallback: total time -> single point (final acc)\n",
        "            t = float(res[\"time_cost\"])\n",
        "            a = float(res[\"test_accs\"][-1])\n",
        "            plt.scatter([t], [a], label=name, color=c)\n",
        "\n",
        "    plt.xlabel(\"Wall Clock Time (s)\")\n",
        "    plt.ylabel(\"Validation/Test Accuracy (%)\")\n",
        "    plt.title(\"Validation Accuracy vs Time\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300)\n",
        "    plt.show()\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loss  and Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_metrics_2x2(run_results, epochs=train_object.epochs, colors=OptimizerConfig.colors, save_path=f\"loss_acc_{global_config.random_seed}.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Accuracy with Time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_acc_vs_time(run_results, colors=OptimizerConfig.colors, save_path=f\"acc_time_{global_config.random_seed}.png\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "name": "ResNet Implementation on CIFAR10.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "312",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
